<!DOCTYPE html>
<html lang='zh-CN'>

<head>
  <meta name="generator" content="Hexo 6.3.0">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.19.0">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://gcore.jsdelivr.net'>
  <link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  
  <title>视觉transformer - S of S of S</title>

  
    <meta name="description" content="环境12345678import torchfrom torch import nn, einsumimport torch.nn.functional as Fimport torchvisionfrom torchvision import transformsfrom einops import rearrange, repeatfrom einops.layers.torch import">
<meta property="og:type" content="article">
<meta property="og:title" content="视觉transformer">
<meta property="og:url" content="https://dfj9401.github.io/2023/10/16/%E8%A7%86%E8%A7%89transformer/index.html">
<meta property="og:site_name" content="S of S of S">
<meta property="og:description" content="环境12345678import torchfrom torch import nn, einsumimport torch.nn.functional as Fimport torchvisionfrom torchvision import transformsfrom einops import rearrange, repeatfrom einops.layers.torch import">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-10-16T05:53:12.000Z">
<meta property="article:modified_time" content="2023-11-04T13:12:29.987Z">
<meta property="article:author" content="lub">
<meta name="twitter:card" content="summary">
  
  
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  

  

  


  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    

  

<header class="header"><div class="logo-wrap"><a class="title" href="/"><div class="main" ff="title">S of S of S</div><div class="sub normal cap">学习记录</div><div class="sub hover cap" style="opacity:0"> 碎碎念</div></a></div>

<nav class="menu dis-select"></nav>
</header>


<div class="widgets">
<widget class="widget-wrapper search"><div class="widget-body"><div class="search-wrapper" id="search"><form class="search-form"><input type="text" class="search-input" id="search-input" data-filter="/blog/" placeholder="文章搜索"><svg t="1670596976048" class="icon search-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2676" width="200" height="200"><path d="M938.2 832.6L723.8 618.1c-2.5-2.5-5.3-4.4-7.9-6.4 36.2-55.6 57.3-121.8 57.3-193.1C773.3 222.8 614.6 64 418.7 64S64 222.8 64 418.6c0 195.9 158.8 354.6 354.6 354.6 71.3 0 137.5-21.2 193.2-57.4 2 2.7 3.9 5.4 6.3 7.8L832.5 938c14.6 14.6 33.7 21.9 52.8 21.9 19.1 0 38.2-7.3 52.8-21.8 29.2-29.1 29.2-76.4 0.1-105.5M418.7 661.3C284.9 661.3 176 552.4 176 418.6 176 284.9 284.9 176 418.7 176c133.8 0 242.6 108.9 242.6 242.7 0 133.7-108.9 242.6-242.6 242.6" p-id="2677"></path></svg></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div></div></widget>


<widget class="widget-wrapper toc single" id="data-toc"><div class="widget-header cap dis-select"><span class="name">视觉transformer</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83"><span class="toc-text">环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%A4%84%E7%90%86"><span class="toc-text">数据加载和处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">图像预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD-CIFAR-10-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">加载 CIFAR-10 数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ViT%E6%A8%A1%E5%9E%8B"><span class="toc-text">ViT模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%8F%8A%E8%AF%84%E4%BB%B7"><span class="toc-text">训练及评价</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%AF%B9%E8%B1%A1"><span class="toc-text">创建对象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">损失函数和优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%8F%8A%E8%AF%84%E4%BC%B0"><span class="toc-text">训练及评估</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">自注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A8%A1%E5%9E%8B"><span class="toc-text">自注意力机制模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="toc-text">自注意力机制的网络模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5%E8%B0%83%E7%94%A8ViT"><span class="toc-text">直接调用ViT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9AViT"><span class="toc-text">自定ViT</span></a></li></ol></div></div></widget>




</div>


    </aside>
    <div class='l_main'>
      

      



<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a></div><div id="post-meta">发布于&nbsp;<time datetime="2023-10-16T05:53:12.000Z">2023-10-16</time></div></div>

<article class='md-text content post'>
<h1 class="article-title"><span>视觉transformer</span></h1>
<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, einsum</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, repeat</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><code>torch</code> PyTorch是一个用于构建深度学习模型的开源框架。</li>
<li><code>torch.nn</code> torch.nn模块是PyTorch中用于构建神经网络的模块。</li>
<li><code>torchvision</code> torchvision 是 PyTorch 的一个辅助库，专门用于计算机视觉任务。</li>
<li><code>transforms</code> transforms 模块是 torchvision 库中的一个子模块，提供了一系列用于图像预处理和数据增强的转换操作。</li>
<li><code>einops</code> 一个用于操作和转换张量的Python库。</li>
</ul>
<h2 id="数据加载和处理"><a href="#数据加载和处理" class="headerlink" title="数据加载和处理"></a>数据加载和处理</h2><h3 id="图像预处理"><a href="#图像预处理" class="headerlink" title="图像预处理"></a>图像预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Resize((<span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<ul>
<li><code>transforms.Compose</code>是一个用于组合多个数据转换操作的类。通过将一系列转换操作传递给<code>transforms.Compose</code>，可以按顺序应用这些操作。</li>
<li><code>transforms.ToTensor()</code>将图像数据转换为PyTorch的张量格式。它将图像数据从范围[0, 255]归一化到范围[0, 1]之间，并将其转换为张量形式。</li>
<li><code>transforms.Resize((32, 32))</code>用于调整图像的尺寸大小。</li>
</ul>
<h3 id="加载-CIFAR-10-数据集"><a href="#加载-CIFAR-10-数据集" class="headerlink" title="加载 CIFAR-10 数据集"></a>加载 CIFAR-10 数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./cifar-10-batches-py&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./cifar-10-batches-py&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>root</code>指定了数据集的存储路径。如果该文件夹不存在，数据集将被下载并存储在该位置。</li>
<li><code>train</code>表示加载训练集。设置为True时，将加载训练集，其中包含用于模型训练的图像和标签数据。</li>
<li><code>transform</code>指定了对加载的图像数据进行的转换操作。在上述代码中，我们之前定义的transform对象被传递给transform参数，以便对加载的图像数据进行转换。</li>
<li><code>download</code>表示如果数据集文件不存在，则自动从网络下载数据集。如果数据集文件已经存在，则不会重新下载。</li>
</ul>
<p>加载 CIFAR-10 数据集并进行数据预处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>batch_size</code>指定了每个批次中的样本数量。数据加载器将数据集分成多个批次，并每次返回一个批次的数据进行训练或测试。</li>
<li><code>shuffle</code> True表示在每个训练迭代中对数据进行洗牌操作，以打乱样本的顺序。这有助于增加数据的随机性和训练的多样性，在模型训练中更好地进行参数更新。False表示在测试过程中不对数据进行洗牌操作，以确保测试结果的一致性和可比性。</li>
</ul>
<p>将数据集划分为批次并提供数据的迭代器。</p>
<h2 id="ViT模型"><a href="#ViT模型" class="headerlink" title="ViT模型"></a>ViT模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        *,</span></span><br><span class="line"><span class="params">        image_size,</span></span><br><span class="line"><span class="params">        patch_size,</span></span><br><span class="line"><span class="params">        num_classes,</span></span><br><span class="line"><span class="params">        dim,</span></span><br><span class="line"><span class="params">        depth,</span></span><br><span class="line"><span class="params">        heads,</span></span><br><span class="line"><span class="params">        mlp_dim,</span></span><br><span class="line"><span class="params">        pool=<span class="string">&#x27;cls&#x27;</span>,</span></span><br><span class="line"><span class="params">        channels=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">        dim_head=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">        dropout=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">        emb_dropout=<span class="number">0.</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        image_height, image_width = pair(image_size)</span><br><span class="line">        patch_height, patch_width = pair(patch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> image_height % patch_height == <span class="number">0</span> <span class="keyword">and</span> image_width % patch_width == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        num_patches = (image_height // patch_height) * (image_width // patch_width)</span><br><span class="line">        patch_dim = channels * patch_height * patch_width</span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1=patch_height, p2=patch_width),</span><br><span class="line">            nn.Linear(patch_dim, dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))</span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))</span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"></span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)</span><br><span class="line"></span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.mlp_head = nn.Sequential(</span><br><span class="line">            nn.LayerNorm(dim),</span><br><span class="line">            nn.Linear(dim, num_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.to_patch_embedding(img)</span><br><span class="line">        b, n, _ = x.shape</span><br><span class="line"></span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&#x27;() n d -&gt; b n d&#x27;</span>, b=b)</span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line"></span><br><span class="line">        x = self.transformer(x)</span><br><span class="line"></span><br><span class="line">        x = x.mean(dim=<span class="number">1</span>) <span class="keyword">if</span> self.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)</span><br></pre></td></tr></table></figure>

<p><code>to_patch_embedding</code>用于将输入图像划分为小块并进行嵌入的序列模块。它通过Rearrange操作将输入图像重新排列为patch形状，并通过线性层将每个patch嵌入到维度为dim的向量中。<br><code>pos_embedding</code>位置嵌入参数，用于为每个patch和特殊的分类令牌（cls token）提供位置信息。<br><code>cls_token</code>分类令牌，用于表示整个图像的全局特征。<br><code>dropout</code>用于在输入嵌入向量之前对位置嵌入和分类令牌进行随机丢弃，以增强模型的鲁棒性。<br><code>transformer</code>Transformer模型，由depth个编码器层组成。它对输入进行自注意力机制和前馈神经网络操作，以捕捉输入的空间关系和特征表示。<br><code>pool</code>池化方式，可以是’cls’（只使用分类令牌的输出）或’mean’（对所有patch的输出进行平均）。<br><code>to_latent</code>标识函数，用于在池化后的特征上进行可选的降维操作。<br><code>mlp_head</code>用于分类预测的多层感知机头部。它对池化后的特征进行归一化操作，然后通过线性层将特征映射到num_classes个类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            self.layers.append(nn.ModuleList([</span><br><span class="line">                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),</span><br><span class="line">                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            x = ff(x) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><code>PreNorm</code>是一种用于层归一化（Layer Normalization）的模块，它在模型中起到归一化输入数据的作用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads=<span class="number">8</span>, dim_head=<span class="number">64</span>, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算内部维度和是否需要投影输出</span></span><br><span class="line">        inner_dim = dim_head * heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置模块属性</span></span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义注意力权重计算和查询-键-值的线性变换</span></span><br><span class="line">        self.attend = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义输出投影模块</span></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),  <span class="comment"># 线性变换</span></span><br><span class="line">            nn.Dropout(dropout),  <span class="comment"># dropout操作</span></span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()  <span class="comment"># 如果不需要投影则使用恒等映射</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, n, _, h = *x.shape, self.heads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输入x进行查询-键-值的线性变换并分割</span></span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h=h), qkv)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力得分</span></span><br><span class="line">        dots = einsum(<span class="string">&#x27;b h i d, b h j d -&gt; b h i j&#x27;</span>, q, k) * self.scale</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力权重</span></span><br><span class="line">        attn = self.attend(dots)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加权平均值</span></span><br><span class="line">        out = einsum(<span class="string">&#x27;b h i j, b h j d -&gt; b h i d&#x27;</span>, attn, v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重排输出维度</span></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行输出投影</span></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>初始化方法 <code>__init__</code>：对模块进行初始化。</p>
<ul>
<li><code>dim</code>：输入数据的维度。</li>
<li><code>heads</code>：自注意力中的注意力头数。</li>
<li><code>dim_head</code>：每个注意力头的维度。</li>
<li><code>dropout</code>：注意力计算过程中的 dropout 概率。</li>
<li><code>inner_dim</code>：计算每个注意力头的维度乘以注意力头数，得到内部维度。</li>
<li><code>project_out</code>：根据头数和维度判断是否需要对输出进行投影。</li>
<li><code>self.heads</code>：保存注意力头数。</li>
<li><code>self.scale</code>：用于缩放注意力计算中的点积操作的缩放因子。</li>
<li><code>self.attend</code>：使用 softmax 函数进行注意力权重的计算。</li>
<li><code>self.to_qkv</code>：通过线性变换将输入数据映射到查询（Query）、键（Key）和值（Value）的多头注意力表示。</li>
<li><code>self.to_out</code>：如果需要对输出进行投影，则定义一个包含线性变换和 dropout 操作的序列模块；否则，使用 nn.Identity() 作为输出模块。</li>
</ul>
</li>
<li><p>前向传播方法 <code>forward</code>：对输入进行自注意力计算。</p>
<ul>
<li><code>b, n, _, h = *x.shape, self.heads</code>：通过输入 x 的形状确定批量大小 b、序列长度 n 和注意力头数 h。</li>
<li><code>qkv = self.to_qkv(x).chunk(3, dim=-1)</code>：将输入 x 经过线性变换 self.to_qkv 得到查询、键和值的表示，并使用 chunk 方法将结果分成三部分。</li>
<li><code>q, k, v = map(lambda t: rearrange(t, &#39;b n (h d) -&gt; b h n d&#39;, h=h), qkv)</code>：将查询、键和值的表示按照维度重排，使得每个注意力头的维度成为第三个维度。</li>
<li><code>dots = einsum(&#39;b h i d, b h j d -&gt; b h i j&#39;, q, k) * self.scale</code>：计算查询和键之间的点积，并乘以缩放因子 self.scale，得到注意力得分。</li>
<li><code>attn = self.attend(dots)</code>：使用 softmax 函数计算注意力得分的权重。</li>
<li><code>out = einsum(&#39;b h i j, b h j d -&gt; b h i d&#39;, attn, v)</code>：使用注意力权重对值进行加权平均，得到自注意力的输出。</li>
<li><code>out = rearrange(out, &#39;b h n d -&gt; b n (h d)&#39;)</code>：重新排列输出的维度顺序，将注意力头的维度和序列长度的维度合并。</li>
<li><code>return self.to_out(out)</code>：将输出经过投影模块 self.to_out 进行处理，并返回最终的输出结果。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></table></figure>
<p>定义了一个前馈神经网络模块 <code>FeedForward</code>，通过一个线性层、GELU 激活函数和 dropout 操作构成了一个非线性转换的序列模块。该模块用于在 Transformer 编码器中对位置编码进行非线性转换，以增强模型的表达能力。</p>
<h2 id="训练及评价"><a href="#训练及评价" class="headerlink" title="训练及评价"></a>训练及评价</h2><h3 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = ViT(</span><br><span class="line">    image_size=<span class="number">32</span>,</span><br><span class="line">    patch_size=<span class="number">4</span>,</span><br><span class="line">    num_classes=<span class="number">10</span>,</span><br><span class="line">    dim=<span class="number">256</span>,</span><br><span class="line">    depth=<span class="number">12</span>,</span><br><span class="line">    heads=<span class="number">8</span>,</span><br><span class="line">    mlp_dim=<span class="number">512</span>,</span><br><span class="line">    dropout=<span class="number">0.1</span>,</span><br><span class="line">    emb_dropout=<span class="number">0.1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>image_size</code>输入图像的大小。在这个例子中，输入图像的宽度和高度都是32个像素。</li>
<li><code>patch_size</code>将输入图像分成小块（或称为patches）的大小。每个小块的宽度和高度都是4个像素。</li>
<li><code>num_classes</code>分类任务的类别数量。在这个例子中，有10个类别需要分类。</li>
<li><code>dim</code>嵌入向量的维度。每个小块将被嵌入成一个具有256维的向量。</li>
<li><code>depth</code>编码器层数。在这个例子中，有12个编码器层用于处理输入。</li>
<li><code>heads</code>多头注意力机制中的头数。在这个例子中，每个注意力头有8个。</li>
<li><code>mlp_dim</code>全连接层（多层感知机）的隐藏层维度。在这个例子中，隐藏层的维度为512。</li>
<li><code>dropout</code>模型中的Dropout层的丢弃率。在这个例子中，Dropout层的丢弃率为0.1，用于防止过拟合。</li>
<li><code>emb_dropout</code>嵌入层的Dropout层的丢弃率。在这个例子中，嵌入层的丢弃率为0.1，用于在输入嵌入向量之前对输入图像进行随机丢弃。</li>
</ul>
<h3 id="损失函数和优化器"><a href="#损失函数和优化器" class="headerlink" title="损失函数和优化器"></a>损失函数和优化器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>
<p><code>CrossEntropyLoss()</code>是用于多类别分类问题的交叉熵损失函数。<br><code>optim.Adam</code>是Adam优化算法的实现，用于调整模型参数以最小化损失函数。<code>model.parameters()</code>用于传递模型的可学习参数给优化器，以便进行参数更新。</p>
<h3 id="训练及评估"><a href="#训练及评估" class="headerlink" title="训练及评估"></a>训练及评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_correct = <span class="number">0</span></span><br><span class="line">    total_samples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">            images = images.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            total_samples += labels.size(<span class="number">0</span>)</span><br><span class="line">            total_correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    accuracy = total_correct / total_samples</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, Accuracy: <span class="subst">&#123;accuracy:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><code>model.train()</code>将模型设置为训练模式，启用模型中的训练特定操作，例如批量归一化和Dropout。<br><code>optimizer.zero_grad()</code>梯度清零。<br><code>loss = criterion(outputs, labels)</code>计算模型输出与真实标签之间的损失。<code>criterion</code> 是定义的损失函数，用于衡量模型输出与标签之间的差异。<br><code>loss.backward()</code>根据损失函数计算模型参数的梯度。<br><code>optimizer.step()</code>根据梯度更新模型参数。<code>optimizer</code> 是定义的优化器，用于更新模型的参数。<br><code>with torch.no_grad()</code>这个语句块定义了一个上下文管理器，其中的代码块将不会进行梯度计算。这样可以减少内存消耗并加快代码的执行速度。<br><code>torch.max()</code> 函数返回给定张量中的最大值及其对应的索引。在这里，第一个参数是模型输出的数据部分，而第二个参数 1 表示在每个样本的维度上进行比较。</p>
<h1 id="其他工作"><a href="#其他工作" class="headerlink" title="其他工作"></a>其他工作</h1><h2 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h2><h3 id="自注意力机制模型"><a href="#自注意力机制模型" class="headerlink" title="自注意力机制模型"></a>自注意力机制模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(SelfAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.query = nn.Conv2d(in_dim, in_dim // <span class="number">8</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.key = nn.Conv2d(in_dim, in_dim // <span class="number">8</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.value = nn.Conv2d(in_dim, in_dim, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.gamma = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size, channels, height, width = x.size()</span><br><span class="line"></span><br><span class="line">        query = self.query(x).view(batch_size, -<span class="number">1</span>, width * height).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        key = self.key(x).view(batch_size, -<span class="number">1</span>, width * height)</span><br><span class="line">        energy = torch.bmm(query, key)</span><br><span class="line">        attention = torch.softmax(energy, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        value = self.value(x).view(batch_size, -<span class="number">1</span>, width * height)</span><br><span class="line"></span><br><span class="line">        out = torch.bmm(value, attention.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">        out = out.view(batch_size, channels, height, width)</span><br><span class="line"></span><br><span class="line">        out = self.gamma * out + x</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<h3 id="自注意力机制的网络模型"><a href="#自注意力机制的网络模型" class="headerlink" title="自注意力机制的网络模型"></a>自注意力机制的网络模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttentionNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SelfAttentionNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.attention1 = SelfAttention(<span class="number">64</span>)</span><br><span class="line">        self.pool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.attention2 = SelfAttention(<span class="number">128</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(<span class="number">128</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(self.conv1(x))</span><br><span class="line">        x = self.attention1(x)</span><br><span class="line">        x = self.pool1(x)</span><br><span class="line"></span><br><span class="line">        x = torch.relu(self.conv2(x))</span><br><span class="line">        x = self.attention2(x)</span><br><span class="line">        x = self.pool2(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2 id="直接调用ViT"><a href="#直接调用ViT" class="headerlink" title="直接调用ViT"></a>直接调用ViT</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> CIFAR10</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> StepLR</span><br><span class="line"><span class="keyword">from</span> vit_pytorch <span class="keyword">import</span> ViT</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置设备（GPU或CPU）</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], std=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载CIFAR-10数据集</span></span><br><span class="line">train_dataset = CIFAR10(root=<span class="string">&#x27;./cifar-10-batches-py&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = CIFAR10(root=<span class="string">&#x27;./cifar-10-batches-py&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ViT模型</span></span><br><span class="line">model = ViT(</span><br><span class="line">    image_size=<span class="number">224</span>,</span><br><span class="line">    patch_size=<span class="number">16</span>,</span><br><span class="line">    num_classes=<span class="number">10</span>,</span><br><span class="line">    dim=<span class="number">768</span>,</span><br><span class="line">    depth=<span class="number">6</span>,</span><br><span class="line">    heads=<span class="number">8</span>,</span><br><span class="line">    mlp_dim=<span class="number">3072</span>,</span><br><span class="line">    dropout=<span class="number">0.1</span>,</span><br><span class="line">    emb_dropout=<span class="number">0.1</span></span><br><span class="line">).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">scheduler = StepLR(optimizer, step_size=<span class="number">5</span>, gamma=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    train_correct = <span class="number">0.0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        _, predicted = outputs.<span class="built_in">max</span>(<span class="number">1</span>)</span><br><span class="line">        train_correct += predicted.eq(labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    train_accuracy = <span class="number">100.0</span> * train_correct / total</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Train Loss: <span class="subst">&#123;train_loss:<span class="number">.4</span>f&#125;</span>, Train Accuracy: <span class="subst">&#123;train_accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    scheduler.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">test_correct = <span class="number">0.0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = outputs.<span class="built_in">max</span>(<span class="number">1</span>)</span><br><span class="line">        test_correct += predicted.eq(labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_accuracy = <span class="number">100.0</span> * test_correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;test_accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="自定ViT"><a href="#自定ViT" class="headerlink" title="自定ViT"></a>自定ViT</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_size=<span class="number">32</span>, patch_size=<span class="number">4</span>, num_classes=<span class="number">10</span>, dim=<span class="number">128</span>, num_heads=<span class="number">8</span>, num_layers=<span class="number">6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ViT, self).__init__()</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.num_patches = (image_size // patch_size) ** <span class="number">2</span></span><br><span class="line">        self.embedding = nn.Linear(<span class="number">3</span> * patch_size * patch_size, dim)</span><br><span class="line">        self.patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1=patch_size, p2=patch_size),</span><br><span class="line">            nn.Linear(patch_size * patch_size * <span class="number">3</span>, dim)</span><br><span class="line">        )</span><br><span class="line">        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(dim, num_heads), num_layers)</span><br><span class="line">        self.fc = nn.Linear(dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.patch_embedding(x)</span><br><span class="line">        x = self.transformer(x)</span><br><span class="line">        x = x.mean(dim=<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2023/11/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E9%83%A8%E5%88%86/">机器学习作业代码实现部分</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2023/10/16/%E9%9D%9E%E5%8F%82%E7%89%B9%E5%BE%81%E8%A1%A8%E5%BE%81/">非参特征表征</a></div></section></div>








      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>本站由 <a href="/">@anonymity</a> 使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> 主题创建。<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  const stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')){
        src = stellar.config.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.version = '1.19.0';
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.19.0';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
    root : '/',
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://gcore.jsdelivr.net/npm/jquery@3.6.2/dist/jquery.min.js'
  };

  if ('local_search') {
    stellar.search = {};
    stellar.search.service = 'local_search';
    if (stellar.search.service == 'local_search') {
      let service_obj = Object.assign({}, {"field":"all","path":"/search.json","content":true,"codeblock":true,"sort":"-date"});
      stellar.search[stellar.search.service] = service_obj;
    }
  }

  // stellar js
  stellar.plugins.stellar = Object.assign({"sites":"/js/plugins/sites.js","friends":"/js/plugins/friends.js","ghinfo":"/js/plugins/ghinfo.js","timeline":"/js/plugins/timeline.js","linkcard":"/js/plugins/linkcard.js","fcircle":"/js/plugins/fcircle.js","weibo":"/js/plugins/weibo.js"});

  stellar.plugins.marked = Object.assign("https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js");
  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/vanilla-lazyload@17.8.3/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.css","js":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://gcore.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://gcore.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://gcore.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti@0.9.2/umd/heti.min.css","js":"https://unpkg.com/heti@0.9.2/umd/heti-addon.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.copycode = Object.assign({"enable":true,"js":"/js/plugins/copycode.js","default_text":"Copy","success_text":"Copied"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
</body>
</html>
