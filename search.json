[{"title":"随机森林分析","path":"/2023/08/31/随机森林分析/","content":"准备数据共计1059条，各数据字段含义如下表所示： 字段 含义 case 国家编号，代表特定国家的数字 cc3 国家代码，三个字母的国家&#x2F;地区代码 country 国家名称 year 观测年份 systemic_crisis 系统性危机，“ 0”表示当年未发生系统性危机，“ 1”表示当年有发生系统性危机 exch_usd 该国货币兑美元的汇率 domestic_debt_in_default 国内债务违约，“0”表示当年未发生国内债务违约，“1”表示当年有发生国内债务违约 sovereign_external_debt_default 主权外债违约，“0”表示当年未发生主权外债违约，“1”表示当年有发生主权外债违约 gdp_weighted_default 违约债务总额与GDP之比 inflation_annual_cpi 年度CPI通货膨胀率 independence 独立性，“ 0”表示“无独立性”，“ 1”表示“独立性” currency_crises 货币危机，“ 0”表示当年未发生“货币危机”，“ 1”表示当年有发生“货币危机” inflation_crises 通胀危机，“ 0”表示当年未发生“通胀危机”，“ 1”表示当年有发生“通胀危机” banking_crisis 银行业危机，“ no_crisis”表示当年没有发生银行业危机，而“ crisis”表示当年有发生银行业危机 1 数据读取与预处理1.1 读取数据12345678910111213141516171819202122# 导入相应模块import numpy as np import pandas as pdimport matplotlib.pyplot as pltfrom matplotlib.font_manager import FontProperties# 设置字体font = FontProperties(fname = &quot;./dataset/SimHei.ttf&quot;, size=14)import seaborn as snsimport random# 设置绘图风格%matplotlib inlinesns.set(style=&#x27;whitegrid&#x27;)# 忽略所有警告import warningswarnings.filterwarnings(&#x27;ignore&#x27;)# 读取数据data = pd.read_csv(&#x27;./dataset/african_crises.csv&#x27;)data.sample(5) 1.2 查看数据12unique_countries = data.country.unique()unique_countries 12# 数据集的基本信息data.______() 12# 查看数据统计性指标data.____________(include = &#x27;all&#x27;) 1.3 数据预处理12#查看货币危机currency_crises的取值为2的数据data[data[&#x27;currency_crises&#x27;] == 2] 12data = data[data[&#x27;currency_crises&#x27;] != 2]# 得到生成删除货币危机currency_crises的取值为2的数据集data.______ # 查看新生成的数据集大小 2 经济指标探索性分析2.1 汇率变化情况1234567891011121314151617181920212223plt.figure(figsize=(12,20))for i in range(13): plt.subplot(7,2,i+1) country = unique_countries[i] # 随机生成一种颜色 random.choice():从一个序列中随机的抽取一个元素，抽取6次组成6位代表随机颜色 col=&quot;#&quot;+&#x27;&#x27;.join([random.choice(&#x27;0123456789ABCDEF&#x27;) for j in range(6)]) # 绘制折线图 sns.____________(data[data.country == country][&#x27;year&#x27;],data[data.country == country][&#x27;exch_usd&#x27;],label = country,color = col) # np.logical_and()逻辑与 两个条件均成立时返回True plt.plot([np.min(data[np.logical_and(data.country == country,data.independence == 1)][&#x27;year&#x27;]), np.min(data[np.logical_and(data.country == country,data.independence == 1)][&#x27;year&#x27;])], [0,np.max(data[data.country == country][&#x27;exch_usd&#x27;])],color = &#x27;black&#x27;,linestyle = &#x27;dotted&#x27;,alpha = 0.8) plt.______(country) # 添加图像标题 plt.tight_layout() # 自动调整子图参数以提供指定的填充plt.show() # 输出13个国家的货币兑美元的汇率变化情况的折线图 2.2 通货膨胀率变化情况12345678910111213141516171819202122232425plt.figure(figsize=(12,20))for i in range(13): plt.subplot(7,2,i+1) country = unique_countries[i] # 随机生成一种颜色 col=&quot;#&quot;+&#x27;&#x27;.join([random.choice(&#x27;0123456789ABCDEF&#x27;) for j in range(6)]) # 绘制折线图 sns.lineplot(data[data.country == country][&#x27;year&#x27;],data[data.country == country][&#x27;inflation_annual_cpi&#x27;],label = country,color = col) # 加入散点图 plt.______(data[data.country == country][&#x27;year&#x27;],data[data.country == country][&#x27;inflation_annual_cpi&#x27;],color = col,s = 28) # s指散点的面积 plt.plot([np.min(data[np.logical_and(data.country == country,data.independence==1)][&#x27;year&#x27;]), np.min(data[np.logical_and(data.country == country,data.independence==1)][&#x27;year&#x27;])], [np.min(data[data.country == country][&#x27;inflation_annual_cpi&#x27;]),np.max(data[data.country == country][&#x27;inflation_annual_cpi&#x27;])], color = &#x27;black&#x27;,linestyle = &#x27;dotted&#x27;,alpha = 0.8) # alpha指颜色透明度 plt.title(country) plt.tight_layout() # 自动调整子图参数以提供指定的填充plt.show() # 输出13个国家的通货膨胀率变化情况 2.3 其他危机分布情况123456789101112131415sns.set(style=&#x27;darkgrid&#x27;)columns = [&#x27;systemic_crisis&#x27;,&#x27;domestic_debt_in_default&#x27;,&#x27;sovereign_external_debt_default&#x27;,&#x27;currency_crises&#x27;,&#x27;inflation_crises&#x27;,&#x27;banking_crisis&#x27;]# 绘制其他特征的分布规律图plt.figure(figsize=(16,16))for i in range(6): plt.subplot(&#x27;32&#x27;+str(i+1)) sns.countplot(y = data.country,hue = data[columns[i]],palette = &#x27;rocket&#x27;) # palette为调色板 plt.______(loc = 0) # 选择最优的图例位置 plt.title(columns[i]) plt.tight_layout()plt.show() 2.4 特征间的相关性123456789101112131415161718# 创建包含年份，国家，系统性危机，银行危机的数据集systemic = data[[&#x27;year&#x27;,&#x27;country&#x27;, &#x27;systemic_crisis&#x27;, &#x27;banking_crisis&#x27;]]# 绘制观察系统性危机与银行危机发生的重叠性systemic = systemic[(systemic[&#x27;country&#x27;] == &#x27;Central African Republic&#x27;) | (systemic[&#x27;country&#x27;]==&#x27;Kenya&#x27;) | (systemic[&#x27;country&#x27;]==&#x27;Zimbabwe&#x27;) ]plt.figure(figsize=(12,12))count = 1for country in systemic.country.unique(): plt.subplot(len(systemic.country.unique()),1,count) subset = systemic[(systemic[&#x27;country&#x27;] == country)] sns.lineplot(subset[&#x27;year&#x27;],subset[&#x27;systemic_crisis&#x27;],ci=None) # ci参数可用于指定线段区间的大小 plt.scatter(subset[&#x27;year&#x27;],subset[&quot;banking_crisis&quot;], color=&#x27;coral&#x27;, label=&#x27;Banking Crisis&#x27;) plt.subplots_adjust(hspace=0.6) # hspace用来设置子图上下间的距离 plt.______(&#x27;Years&#x27;) # 给x轴命名 plt.______(&#x27;Systemic Crisis/Banking Crisis&#x27;) # 给y轴命名 plt.title(country) count+=1 12345678910111213141516171819202122# 将银行危机banking_crisis列进行特征编码# 将银行危机banking_crisis中未发生危机的数据标为0，发生危机的数据标为1data[&#x27;banking_crisis&#x27;] = data[&#x27;banking_crisis&#x27;].map(&#123;&quot;no_crisis&quot;:0,&quot;crisis&quot;:1&#125;)# 选出所有特征selected_features = [&#x27;systemic_crisis&#x27;, &#x27;exch_usd&#x27;, &#x27;domestic_debt_in_default&#x27;,&#x27;sovereign_external_debt_default&#x27;, &#x27;gdp_weighted_default&#x27;, &#x27;inflation_annual_cpi&#x27;, &#x27;independence&#x27;, &#x27;currency_crises&#x27;,&#x27;inflation_crises&#x27;,&#x27;banking_crisis&#x27;]corr = data[selected_features].______() # 得到各特征间的相关性大小生成相关性矩阵fig = plt.figure(figsize = (12,8))cmap = sns.diverging_palette(220, 10, as_cmap=True) # 生成蓝-白-红的颜色列表mask = np.zeros_like(corr, dtype=np.bool) # 返回与相关性矩阵具有相同形状和类型的零数组作为掩码mask[np.triu_indices_from(mask)] = True # 给相关性矩阵的上三角阵生成掩码# 绘制热力图sns.______(corr, mask=mask, cmap=cmap,vmin=-0.5,vmax=0.7, center=0,annot = True, square=True, linewidths=.5,cbar_kws=&#123;&quot;shrink&quot;: .5&#125;);plt.title(&quot;特征间的相关性&quot;,fontproperties = font)plt.show() 3 构建银行危机预测模型 特征编码 数据集划分与分层采样 建立随机森林预测模型 模型效果的评估 使用SMOTE进行过采样优化模型 特征重要性排序 3.1 特征编码12data.drop([&#x27;case&#x27;,&#x27;cc3&#x27;],axis = 1,inplace = True) # 在原数据集上删掉case列和cc3列data.head() 12345# 对国家country进行labelencoderfrom sklearn.preprocessing import LabelEncoderle = LabelEncoder()le.______(data[&#x27;country&#x27;].values) # 将country的值塞入空字典data[&#x27;country&#x27;]=le.____________(data[&#x27;country&#x27;].values) # 将字典中的country的值转变为索引值 1print(data[&#x27;country&#x27;]) # 查看特征编码后的country名称 1234567# 绘制未发生银行危机no_crisis与发生银行危机crisis的柱状图fig = plt.figure(figsize = (8,6))data[&#x27;banking_crisis&#x27;].value_counts().plot(kind=&#x27;______&#x27;,rot = 360,color = &#x27;lightseagreen&#x27;)plt.xticks([0,1],[&quot;no_crisis&quot;,&quot;crisis&quot;])plt.show() 3.2 数据集划分与分层采样下面我们开始对数据进行训练集与测试集的划分。在Sklearn中的model_selection模块，存在train_test_split()函数，用作训练集和测试集划分，函数语法为：train_test_split(x,y,test_size = None,random_state = None,stratify = y)，其中： x,y: 分别为预测所需的所有特征，以及需要预测的特征。 test_size: 测试集比例，例如test_size=0.2则表示划分20%的数据作为测试集。 random_state: 随机种子，因为划分过程是随机的，为了进行可重复的训练，需要固定一random_state，结果重现。 stratify: 使用分层采样，保证从发生银行危机样本和未发生银行危机样本中抽取了同样比例的训练集和测试集。 函数最终将返回四个变量，分别为x的训练集和测试集，以及y的训练集和测试集。 1234567# 训练集与测试集的划分from sklearn import model_selectionx = data.drop(&#x27;banking_crisis&#x27;,axis = 1) # 将删除banking_crisis列的数据集作为xy = data[&#x27;banking_crisis&#x27;] # banking_crisis列作为yx_train,x_test,y_train,y_test = model_selection.__________________(x, y,test_size=0.2,random_state = 33,stratify=y) 3.3 建立随机森林预测模型随机森林是一种集成学习方法，通过使用随机的方式从数据中抽取样本和特征，训练多个不同的决策树，形成“森林”。每个树都给出自己的分类意见，称“投票”。在分类问题下，森林选择选票最多的分类；在回归问题下则使用平均值。在Python中使用sklearn.ensemble的RandomForestClassifier构建分类模型，其主要参数包括： n_estimators : 训练分类器的数量(默认为100)； max_depth : 每棵树的最大深度(默认为3)； max_features: 划分的最大特征数(默认为 ‘auto’) random_state : 随机种子。 123456from sklearn.ensemble import RandomForestClassifier# 训练随机森林分类模型rf = RandomForestClassifier(n_estimators = 100, max_depth = 20,max_features = 10, random_state = 20)rf.fit(x_train, y_train) y_pred = rf.______(x_test) # 对y进行预测 3.4 模型评估在评价模型好坏时，我们分别使用函数classification_report()、confusion_matrix()和accuracy_score(),用于输出模型的预测报告、混淆矩阵和分类正确率。 12345678910111213from sklearn.metrics import classification_report,confusion_matrixprint(classification_report(y_test, y_pred)) # 输出模型的预测报告confusion_matrix = __________________(y_test, y_pred) print(confusion_matrix) # 输出混淆矩阵# 绘制混淆矩阵热力图fig,ax = plt.subplots(figsize=(8,6)) sns._________(confusion_matrix,ax=ax,annot=True,annot_kws=&#123;&#x27;size&#x27;:15&#125;, fmt=&#x27;d&#x27;,cmap = &#x27;YlGnBu_r&#x27;)ax.set_ylabel(&#x27;真实值&#x27;,fontproperties = font)ax.set_xlabel(&#x27;预测值&#x27;,fontproperties = font)ax.set_title(&#x27;混淆矩阵热力图&#x27;,fontproperties = font)plt.show() # 输出混淆矩阵热力图 12345678910111213141516171819from sklearn.metrics import roc_auc_scorefrom sklearn.metrics import roc_curveroc_auc = ____________(y_test, rf.predict(x_test)) #计算auc的值fpr, tpr, thresholds = ____________(y_test, rf.predict_proba(x_test)[:,1]) #计算不同阈值下的TPR和FPR# 绘制ROC曲线plt.figure(figsize = (8,6))plt.plot(fpr, tpr, label=&#x27;Random Forest (area = %0.2f)&#x27; % roc_auc)plt.plot([0, 1], [0, 1],&#x27;r--&#x27;)# 绘制随机猜测线plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel(&#x27;False Positive Rate&#x27;)plt.ylabel(&#x27;True Positive Rate&#x27;)plt.title(&#x27;ROC curve&#x27;)plt.legend(loc=&quot;lower right&quot;) # 图例位置位于右下方plt.show() 3.5 使用SMOTE进行采样优化模型在对数据集进行划分，接着对训练集进行过采样，将少数类进行扩充。在Python中使用imblearn.over_sampling的SMOTE类构建SMOTE过采样模型。 12345# 对x_train,y_train进行SMOTE过采样from imblearn.over_sampling import SMOTEx_train_resampled, y_train_resampled = SMOTE(random_state=4).fit_resample(x_train, y_train)print(x_train_resampled.shape, y_train_resampled.shape) #查看采样后的数据集大小 1234567# 通过网格搜索选择最优参数from sklearn.model_selection import GridSearchCVparam_grid = [&#123; &#x27;n_estimators&#x27;:[10,20,30,40,50], &#x27;max_depth&#x27;:[5,8,10,15,20,25]&#125;]grid_search = GridSearchCV(rf, param_grid, scoring = &#x27;recall&#x27;) 123456# 输出最佳参数组合以及分数grid_search.fit(x_train_resampled, y_train_resampled)print(&quot;best params:&quot;, grid_search.best_params_)print(&quot;best score:&quot;, grid_search.best_score_) 3.6 特征重要性排序123456789101112131415fig = plt.figure(figsize=(16,12))# 得到随机森林特征重要性评分rf_importance = rf.__________________index = data.drop([&#x27;banking_crisis&#x27;], axis=1).columns # 删掉银行危机banking_crisis列特征# 对得到的特征重要性评分进行降序排序rf_feature_importance = pd.DataFrame(rf_importance.T, index=index,columns=[&#x27;score&#x27;]).sort_values(by=&#x27;score&#x27;, ascending=True)# 水平条形图绘制rf_feature_importance.plot(kind=&#x27;______&#x27;,legend=False,color = &#x27;deepskyblue&#x27;)plt.title(&#x27;随机森林特征重要性&#x27;,fontproperties = font)plt.show()"},{"title":"商品评论情感判定","path":"/2023/08/31/商品评论情感判定/","content":"准备用户在电商平台上面发表的产品评价中包含着用户的偏好信息，所以通过用户评论，可以得到用户的情感倾向以及对产品属性的偏好。 1234567891011121314# 载入必要库import jiebaimport numpy as npimport pandas as pdimport sklearnimport matplotlibimport matplotlib.pyplot as plt import pyecharts.options as optsfrom pyecharts.charts import WordCloudfrom pyecharts.charts import Barimport re#loggingimport warningswarnings.filterwarnings(&#x27;ignore&#x27;) 1 数据读取1.1 读取数据某款手机的商品评论信息数据集，包含2个字段，共计8186个样本。数据集描述如下： 列名 说明 类型 Comment 对该款手机的评论 String Class 该评论的情感倾向: -1 —— 差评 0 —— 中评 1 —— 好评 Int 使用Pandas库中read_csv()函数读取数据。 123#读入数据集data = pd.read_csv(&#x27;./dataset/data.csv&#x27;)data.head(10) 1.2 查看数据12# 数据集的大小data.shape 12# 数据集的基本信息data.info() 2 数据预处理使用分词库jieba中的cut()函数对文本进行分词。 2.1 去除缺失值12# 移除含有缺失值的行data.dropna(axis=0,inplace=True) 12#查看去除缺失值后的行和列data.shape 2.2 分词去除标点数字字母后再分词。 1234def remove_url(src): # 去除标点符号、数字、字母 vTEXT = re.sub(&#x27;[a-zA-Z0-9’!&quot;#$%&amp;\\&#x27;()*+,-./:;&lt;=&gt;?@，。?★、…【】╮ ￣ ▽ ￣ ╭\\\\～⊙％；①（）：《》？“”‘’！[\\\\]^_`&#123;|&#125;~\\s]+&#x27;, &quot;&quot;, src) return vTEXT 12345678910111213cutted = []for row in data.values: text = remove_url(str(row[0])) #去除文本中的标点符号、数字、字母 raw_words = (&#x27; &#x27;.join(jieba.cut(text)))#分词,并用空格进行分隔 cutted.append(raw_words)cutted_array = np.array(cutted)# 生成新数据文件，Comment字段为分词后的内容data_cutted = pd.DataFrame(&#123; &#x27;Comment&#x27;: cutted_array, &#x27;Class&#x27;: data[&#x27;Class&#x27;]&#125;) 12#查看分词后的数据集data_cutted.head() data_cutted为进行分词之后的数据集。 2.3 查看关键词读取停用词文件，再用jieba.analyse中的set_stop_words函数设置停用词，使用extract_tags函数提取关键词。 12345with open(&#x27;./dataset/stopwords.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:#读停用词表 stopwords = [item.strip() for item in f] #通过列表推导式的方式获取所有停用词 for i in stopwords[:100]:#读前100个停用词 print(i,end=&#x27;&#x27;) 123#设定停用词文件,在统计关键词的时候，过滤停用词import jieba.analysejieba.analyse.set_stop_words(&#x27;./dataset/stopwords.txt&#x27;) 1data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == 1] 1234# 好评关键词keywords_pos = jieba.analyse.extract_tags(&#x27;&#x27;.join(data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == 1]),withWeight = True,topK=30)for item in keywords_pos: print(item[0],end=&#x27; &#x27;) 1234#中评关键词keywords_med = jieba.analyse.extract_tags(&#x27;&#x27;.join(data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == 0]),withWeight = True,topK=30)for item in keywords_med: print(item[0],end=&#x27; &#x27;) 12345#差评关键词keywords_neg = jieba.analyse.extract_tags(&#x27;&#x27;.join(data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == -1]),withWeight = True,topK=30)for item in keywords_neg: print (item[0],end=&#x27; &#x27;) 3 可视化分析使用Pyecharts进行绘图。 3.1 评价柱状图1data_cutted[&#x27;Class&#x27;].value_counts() 123456789101112# 不同类别数据记录的统计x_label = [&#x27;好评&#x27;,&#x27;差评&#x27;,&#x27;中评&#x27;]class_num = ( Bar() #设置x轴的值 .add_xaxis(x_label) #设置y轴数据 .add_yaxis(&quot;&quot;,data_cutted[&#x27;Class&#x27;].value_counts().to_list(),color=[&#x27;#4c8dae&#x27;]) #设置title .set_global_opts(title_opts=opts.TitleOpts(title=&quot;好评、中评、差评数量柱状图&quot;)))class_num.render_notebook() 3.2 好评云图123456789101112wordcloud_pos = ( WordCloud() #data_pair：要绘制词云图的数据 .add(series_name=&quot;&quot;, data_pair=keywords_pos[:], word_size_range=[10, 66]) .set_global_opts( title_opts=opts.TitleOpts( #设置词云图标题和标题字号 title=&quot;好评关键词词云图&quot;, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True)) )wordcloud_pos.render_notebook() 3.3 中评云图123456789101112wordcloud_med = ( WordCloud() #data_pair：要绘制词云图的数据 .add(series_name=&quot;&quot;, data_pair=keywords_med[:], word_size_range=[10, 66]) .set_global_opts( title_opts=opts.TitleOpts( #设置词云图标题和标题字号 title=&quot;中评关键词词云图&quot;, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True)) )wordcloud_med.render_notebook() 3.4 差评云图123456789101112wordcloud_neg = ( WordCloud() #data_pair：要绘制词云图的数据 .add(series_name=&quot;&quot;, data_pair=keywords_neg[:], word_size_range=[10, 66]) .set_global_opts( title_opts=opts.TitleOpts( #设置词云图标题和标题字号 title=&quot;差评关键词词云图&quot;, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True)) )wordcloud_neg.render_notebook() 4 文本向量化经过分词之后的文本数据集要先进行向量化之后才能输入到分类模型中进行运算。TF-IDF算法是常用的文本向量化算法。 TF-IDF是Term Frequency-Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。TF-IDF是一种统计方法，用以评估一个词对于一个文件集或一个语料库中的一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。使用sklearn库中的TfidfVectorizer实现tf-idf文本向量化。 1234567# 实现向量化方法from sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(stop_words = stopwords,max_df=2000,min_df=6)#将文本向量化后的数据赋给data_transformdata_transform = vectorizer.fit_transform(data_cutted[&#x27;Comment&#x27;].values.tolist()) 12#文本的词汇表vectorizer.get_feature_names() 12#调用toarray()方法查看文本向量化后的数据data_transform.toarray() 1data_transform.shape 5 高斯朴素贝叶斯模型5.1 数据集划分使用sklearn.model_selection模块的train_test_split()函数划分训练集和测试集。 123from sklearn.model_selection import train_test_split #数据集划分X_train, X_test, y_train, y_test = train_test_split(data_transform, data_cutted[&#x27;Class&#x27;], random_state=10,test_size=0.2) 5.2 构建高斯朴素贝叶斯模型从sklearn.naive_bayes中导入GaussianNB类，使用GaussianNB类初始化一个模型对象，命名为gnb，对gnb调用fit方法，带入训练集X_train，y_train进行训练。 1from sklearn.naive_bayes import GaussianNB 12gnb = GaussianNB()gnb_model = gnb.fit(X_train.toarray(),y_train) 5.3 高斯朴素贝叶斯模型评估从sklearn.metrics中导入classification_report分类报告用于模型评估. 1from sklearn.metrics import classification_report 123gnb_prelabel = gnb_model.predict(X_test.toarray())print(classification_report(y_true=y_test,y_pred=gnb_prelabel)) 6 SVM模型构建6.1 构建SVM模型从sklearn.svm中导入SVC类，使用SVC类初始化一个模型对象，命名为svc，对svc调用fit方法，带入训练集X_train，y_train进行训练。 12345from sklearn.svm import SVC#设置kernel为‘rbf’高斯核，C=1svc = SVC(kernel=&#x27;rbf&#x27;, C=1)svc_model = svc.fit(X_train,y_train) 训练模型后，可以使用模型在测试集X_test上作出预测。 6.2 模型评估12svc_prelabel = svc_model.predict(X_test)print(classification_report(y_true=y_test,y_pred=svc_prelabel)) 通过将SVM模型与构建的高斯朴素贝叶斯模型分类结果比较，可以看出SVM在分类的精确率、召回率，以及模型的准确率上都优于高斯朴素贝叶斯模型。"},{"title":"汽车产品聚类分析","path":"/2023/08/31/汽车产品聚类分析/","content":"内容根据各种汽车参数利用聚类算法进行聚类，识别出相似的汽车，汽车款式数据集中有392个样本，8个特征。说明如下： 字段名称 字段类型 字段说明 mpg 浮点型 每加仑英里 cylinders 整型 气缸数在4到8之间 displacement 浮点型 发动机排量（立方英寸） horsepower 整型 引擎马力 weight 整型 车重（磅） acceleration 浮点型 从0加速到60 mph（秒）的时间 year 整型 型号年份 origin 整型 汽车来源（1.美国，2.欧洲，3.日本） 1 准备工作导入相关包和模块。 123# 忽略警告信息import warningswarnings.filterwarnings(&quot;ignore&quot;) 1234 #导入相关的包和模块import numpy as npimport pandas as pdfrom sklearn import metrics 12import sklearnsklearn.__version__ 1np.__version__ 123#读入数据集并查看数据集的前五行信息auto = pd.read_csv(&#x27;./dataset/Auto.csv&#x27;) auto.head() 12#查看数据集基本信息auto.info() 2 数据预处理1auto.head() 对mpg,displacement,horsepower,weight,acceleration五个连续型变量用Z-score标准化，对cylinders,year,origin三个类别变量进行哑变量编码。 1234#Z-score标准化from sklearn.preprocessing import StandardScalerscaler = StandardScaler()auto[[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]] =scaler.fit_transform(auto[[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]]) 1StandardScaler.__version__ 1auto.head() 1pd.get_dummies? 12#哑变量编码auto_scaled = pd.get_dummies(data=auto,columns=[&#x27;cylinders&#x27;,&#x27;year&#x27;,&#x27;origin&#x27;]) 12#查看进行标准化处理和哑变量编码后数据集的前两行auto_scaled.head(2) 3 K-Means聚类K-Means聚类基于点与点之间的距离的相似度来计算最佳类别归属。其核心目标是将给定的数据集划分成K个簇，并给出每个样本数据对应的中心点。 123#导入K-Means聚类方法，将初始K值设为3，随机种子数设为0，并训练模型from sklearn.cluster import KMeansmodel = KMeans(n_clusters=3,random_state=0).fit(auto_scaled) 1KMeans? 12345#将样本标签和簇质心保存在auto_lable和auto_cluster中,展示分类结果标签的前十行，以及簇质心的第一行auto_label = model.labels_auto_cluster = model.cluster_centers_print(auto_label[:10])print(auto_cluster[0]) 统计聚类结果，根据我们设置的初始K值，结果将所有样本分为三类，并按照每一类的数量进行降序展示： 1pd.value_counts(auto_label) 1pd.value_counts? 12345678#找出簇质心连续性变量的坐标centroid_cluster = pd.DataFrame(auto_cluster).copy().iloc[:,:5]centroid_cluster.columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]#将数据逆标准化，转换为原始数据,结果保留两位小数：centroid_cluster_inversescale = pd.DataFrame(scaler.inverse_transform(centroid_cluster))centroid_cluster_inversescale.columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]centroid_cluster_inversescale.applymap(lambda x:&#x27;%.2f&#x27;%x) 选择轮廓系数作为聚类性能的评估指标，轮廓系数取值范围为[-1,1]，轮廓系数为-1时表示聚类结果不好，为+1时表示簇内实例之间紧凑，为0时表示有簇重叠。轮廓系数越大，表示簇内实例之间紧凑，簇间距离大，即聚类的效果越好。 1pd.value_counts(auto_label) 1234#轮廓系数结果保留四位小数labels = model.labels_print(&quot;轮廓系数(Silhouette Coefficient): %0.4f&quot; % metrics.silhouette_score(auto_scaled, labels)) 1metrics.silhouette_score? 1234for i in range(2,12,2): model = KMeans(n_clusters=i,random_state=0).fit(auto_scaled) labels = model.labels_ print(&quot;轮廓系数(Silhouette Coefficient): %0.4f&quot;% metrics.silhouette_score(auto_scaled, labels)) 根据结果可以看到当k值取2时，其轮廓系数最大，即聚类效果最佳。 4 DBSCAN聚类DBSCAN是一种典型的基于密度的聚类算法，在DBSCAN算法中将数据点分为以下三类： 核心点：在半径Eps内含有超过MinPts数目的点 边界点：在半径Eps内点的数量小于MinPts，但是落在核心点的邻域内 噪音点：既不是核心点也不是边界点的点（离群点）12#导入DBSCANfrom sklearn.cluster import DBSCAN 1DBSCAN? 12#模型的参数设置：一个是半径eps，这里设置为1.5，另一个是指定的数目MinPts，这里设置为4：model = DBSCAN(eps=1.5,min_samples=4).fit(auto_scaled) 根据我们设置的参数，DBSCAN将所有样本分为四类（包含噪声点），并按照每一类的数量降序展示:123auto_label = model.labels_df=pd.value_counts(auto_label)df 123import matplotlib.pyplot as pltplt.figure(figsize=(5,5))df.plot.bar(rot=0) 将不同簇的数据使用均值聚合，得到近似类中心12345#先将数据逆标准化，转为原始数据，并取出前五列，再根据聚类标签分类使用均值聚合，结果保留两位小数auto_new_continuous = pd.DataFrame(scaler.inverse_transform(auto_scaled.iloc[:, :5]), columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;])auto_new_continuous[&#x27;label&#x27;] = auto_labelauto_new_continuous.groupby(&#x27;label&#x27;).mean().applymap(lambda x:&#x27;%.2f&#x27;%x) 去除噪声点样本后，计算轮廓系数12labels = model.labels_metrics.silhouette_score(auto_scaled[labels&gt;-1],labels[labels&gt;-1]) 调整密度参数，查看聚类结果1234567891011121314# 不同最小样本数下的簇个数## 设置参数取值范围min_samples_grid = [1,2,3,4]## 训练模型并输出簇个数cluster_number = []slt_score = []noise_count = []for item in min_samples_grid: model = DBSCAN(eps=1.5,min_samples=item).fit(auto_scaled) cluster_number.append(len(np.unique(model.labels_))-1) slt_score.append(metrics.silhouette_score(auto_scaled[model.labels_&gt;-1], model.labels_[model.labels_&gt;-1])) noise_count.append((model.labels_==-1).sum()) 1np.unique? 123456## 绘图import matplotlib.pyplot as pltplt.plot(min_samples_grid, cluster_number, &#x27;r-*&#x27;, linewidth=2)plt.xlabel(&#x27;最小样本数&#x27;)plt.ylabel(&#x27;簇个数&#x27;)plt.title(&#x27;不同最小样本数下聚类的簇个数&#x27;) 1234plt.plot(min_samples_grid, slt_score, &#x27;b-*&#x27;, linewidth=2)plt.xlabel(&#x27;最小样本数&#x27;)plt.ylabel(&#x27;轮廓系数&#x27;)plt.title(&#x27;不同最小样本数下聚类的轮廓系数&#x27;) 1234plt.plot(min_samples_grid, noise_count, &#x27;g-*&#x27;, linewidth=2)plt.xlabel(&#x27;最小样本数&#x27;)plt.ylabel(&#x27;噪声点数量&#x27;)plt.title(&#x27;不同最小样本数下聚类的噪声点数量&#x27;) 5 层次聚类层次聚类有两种常用的形式，自顶向下和自底向上,自底向上的主要做法是，在开始时，将每个样本视为一个簇，重复的合并最近的两个簇，直到簇的个数达到给定值。 12345from sklearn.cluster import AgglomerativeClustering# 训练模型model = AgglomerativeClustering(n_clusters=3,linkage=&#x27;average&#x27;).fit(auto_scaled)# 输出模型结果 auto_label = model.labels_ 1AgglomerativeClustering? 123auto_label = model.labels_df=pd.value_counts(auto_label)df 12plt.figure(figsize=(5,5))df.plot.bar() 12345#先将数据逆标准化，转为原始数据，并取出前五列，再根据聚类标签分类使用均值聚合，结果保留两位小数auto_new_continuous = pd.DataFrame(scaler.inverse_transform(auto_scaled.iloc[:, :5]), columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;])auto_new_continuous[&#x27;label&#x27;] = auto_labelauto_new_continuous.groupby(&#x27;label&#x27;).mean().applymap(lambda x:&#x27;%.2f&#x27;%x)"},{"title":"图书数据可视化","path":"/2023/08/30/图书数据可视化/","content":"1 数据集介绍使用上次清洗过的数据进行数据分析，并利用可视化图形对分析。数据集共9个字段，600个样本，如下： 价格 星级 评论数 作者 出版日期 出版社 书名 简介1 简介2 2 数据读取2.1 导入相关的库12345678#import pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltimport jiebafrom wordcloud import WordCloud,STOPWORDS# 设置中文字体plt.rcParams[&#x27;font.sans-serif&#x27;]=&#x27;SimHei&#x27;%config InlineBackend.figure_format = &#x27;svg&#x27; 2.2 数据读取调用Pandas对象的read_csv()函数。 1234#读数据集data = pd.read_csv(&#x27;./dataset/当当网机器学习图书数据（已清洗）.csv&#x27;,sep=&#x27;,&#x27;,encoding=&#x27;utf-8&#x27;)#查看前5行data.head(5) 2.3 字段基本统计信息使用DataFrame对象的describe()方法可以查看各列基本统计信息，统计并生成数据集哥哥字段的样本数、均值、标准差、最小值、四分位数等。describe()方法主要参数： percentiles：自定义分位数，默认是25%,50%,75% include：指定统计的数据类型，默认只统计数值型，当为all时数值和离散型都统计 exclude：意排除哪些字段，默认统计所有列 12#查看数据基本统计信息 data.describe(include=&#x27;all&#x27;) 3 可视化使用python中的Matplotilb、Seaborn等库进行可视化。 3.1 各出版社的图书数量统计各出版社关于机器学习相关的图书数量，对出版数量有直观了解。Series对象的value_counts()方法可以对某列的取值数量分布进行统计，其主要参数： normallize：默认为False，若为True，则以百分比的形式显示 sort：是否对结果进行排序，默认为True ascending: 默认对值降序排列(False) dropna:是否删除空值，默认删除(True) 12# 各出版社出版的图书数量data[&#x27;出版社&#x27;].value_counts(ascending=True) 从统计结果可以看到出版机器学习相关图书最多的机械工业出版社。 3.2 各星级图书数量统计柱状图使用Seaborn中的barplot函数绘制柱状图，其主要参数有： x：x坐标传入的值 y：y坐标传入的值 data：传入的数据集 12# 查看各星级的数量data[&#x27;星级&#x27;].value_counts() 123456789101112# # 绘制画布，即画布的大小和分辨率plt.figure(figsize=(8,5), dpi=100)x = data[&#x27;星级&#x27;].value_counts().indexy = data[&#x27;星级&#x27;].value_counts().valuessns.barplot(x,y)# 设置标题plt.title(&#x27;各星级图书数量统计柱状图&#x27;,fontsize=13)# 设置x轴标签plt.xlabel(&#x27;星级&#x27;,fontsize=10)# 设置y轴标签plt.ylabel(&#x27;数量&#x27;,fontsize=10)plt.show() 3.3 图书价格直方图使用Seaborn中的histplot函数来绘制直方图，主要参数有： data：传入的数据 x：做直方图所用的数据，必须是一维数组 bins：分组数量 12345678910111213# 绘制画布，即大小和分辨率plt.figure(figsize=(8,5),dpi=100)# 绘图，分别统计不同图书价格的数量sns.histplot(data,x=&#x27;价格&#x27;,bins=20)# 绘制价格均值直线plt.plot([data[&#x27;价格&#x27;].mean(),data[&#x27;价格&#x27;].mean()],[0,200],&#x27;g--&#x27;)# 设置标题plt.title(&#x27;图书价格直方图&#x27;,fontsize=13)# 设置x轴标签plt.xlabel(&#x27;图书价格&#x27;,fontsize=10)# 设置y轴标签plt.ylabel(&#x27;数量&#x27;,fontsize=10)plt.show() 3.4 高价图书分析设置价格大于100元为高价图书，筛选高价图书如下： 1234# 利用DataFrame直接筛选输出高价图书data_price_high = data[data[&#x27;价格&#x27;]&gt;100]# 查看前3行高价图书信息data_price_high.head(3) 3.4.1高价书出版社统计使用Matplotlib中的pie函数绘制饼状图，主要参数如下： x：每一块的数值比例 labels：每一块外侧显示的说明文字 autopct：控制图内显示的百分比 startangle：起始角度，默认图是从x轴正方向画起，逆时针方向 pctdistance：数值标签距离圆心的距离 radius：控制饼图半径 1data_price_high[&#x27;出版社&#x27;].value_counts().index 123456789## 按照出版社进行分段统计x = data_price_high[&#x27;出版社&#x27;].value_counts().values# 设置饼图的标签labels = data_price_high[&#x27;出版社&#x27;].value_counts().index# 绘制画布plt.figure(figsize=(8,5),dpi=100)# 绘制饼状图plt.pie(x,labels=labels,autopct=&#x27;%1.1f%%&#x27;,startangle=30,pctdistance=0.8,radius=1)plt.show() 3.4.2 高价书星级评定使用Matplotlib中的bach()函数绘制柱状图，主要参数如下： y：y坐标 width：柱子的宽度，即统计的数值大小 height：柱子的高度，默认为0.8 1data_price_high.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean() 1234567891011# 取值各个出版社星级的平均值width = data_price_high.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean().values# 按照出版社求各个出版社星级评分的平均值，取出索引，即出版社y = data_price_high.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean().index# 设置画布plt.figure(figsize=(8,5),dpi=100)# 绘图plt.barh(y,width,height=0.8,color=&#x27;orange&#x27;)plt.title(&#x27;高价图书星级评定&#x27;,fontsize=13)plt.show() 3.5 各出版社图书价格均值对各出版社出版图书价格均值进行统计。 12# 按照出版社进行分组聚合，计算每组平均的图书价格，计算结果如下data.groupby(&#x27;出版社&#x27;)[&#x27;价格&#x27;].mean().sort_values(ascending=False) 3.6 各出版社出版图书口碑分析将出版社一列分组，计算各出版社图书平均星级。 1234# 按照出版社分段统计，求各个出版社星级的均值data_star_mean = data.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean()# 将每个出版社的平均星级进行降序排序 默认是升序排序data_star_mean.sort_values(ascending = False) 3.7 图书简介文本分词对图书的简介进行分析，首先数据中简介1这一列是文本类型，因此我们要先进行分词，分词的目的是将文本按一定的规则进行分词处理。在这里我们使用jieba库里面的cut函数进行分词，jieba库是专门使用Python语言开发的分词库,占用资源较少，常识类文档的分词精度较高。cut函数的主要参数如下： sentence:要进行的分词的句子样本 cut_all:分词的模式，有全模式和精准模式，默认false，精准模式 HMM:隐马尔科夫链，即HMM模型，默认开启，这个是在分词的理论模型中用到的 1234567# 对数据集的每个样本的文本进行中文分词#记录分词后的结果cutted = [] for item in data[&#x27;简介1&#x27;].values: raw_words = (&quot; &quot;.join(jieba.cut(str(item)))) cutted.append(raw_words) 1234567# 创建一个新的DataFrame，将没分词和分词后的句子添加到里面data_cutted = pd.DataFrame(&#123; &#x27;简介1&#x27;: data[&#x27;简介1&#x27;], &#x27;简介1_cut&#x27;: cutted&#125;)data_cutted.head() 3.8 词云图分词处理完毕后，再处理停用词，最后形成词云图。利用wordcloud中的WordColoud()函数绘制词云图，其中主要参数为： font_path:字体路径 stopword:将被忽略或者是删除的单词表 width:词云图的宽度，默认400 height：词云图的高度，默认200 max_font_size:最大字体的大小 12345# 读取停用词stopwd=pd.read_csv(&#x27;./dataset/中文停用词表数据集.csv&#x27;)stopwords=set([i for i in stopwd[&#x27;cn_stopwords&#x27;]])print(len(stopwords))# stopwords 1234567# 定义词云图wc = WordCloud(font_path = &quot;./dataset/simsun.ttc&quot;,#设置字体 stopwords = stopwords, #设置停用词 background_color = &#x27;white&#x27;, width = 1000, height = 618, max_font_size = 400) 12345678# 运行统计词频wc.generate(data_cutted[&#x27;简介1_cut&#x27;].sum())# 4、显示图片plt.figure(&quot;词云图&quot;) #指定所绘图名称plt.imshow(wc) # 以图片的形式显示词云plt.axis(&quot;off&quot;) #关闭图像坐标系plt.show() 1# data_cutted[&#x27;简介1_cut&#x27;].sum() 词云图突出显示了简介中出现频率较高的词，出现词频越高的词在词云图中显示越大。"},{"title":"网站数据清洗","path":"/2023/08/30/网站数据清洗/","content":"1 数据集介绍来自某图书网站爬取的机器学习相关图书信息，数据集共600条数据，5个字段，如下 书名 出版信息 当前价格 星级 评论数 2 数据读取2.1 读取数据数据集保存在csv文件中，使用Pandas中的read_csv()读取csv文件，结果保存为DataFrame或Series对象，使用DataFrame或Series对象的head()方法可以查看前n行数据。 1234567#导入相关的库import pandas as pdimport numpy as np#读取数据data = pd.read_csv(&#x27;./dataset/data.csv&#x27;)#查看数据data.head() 2.2 查看数据集的基本信息调用DataFrame对象的info()方法，获取数据的列名，非空值个数，列数据类型，内存占用信息。 1data.info() 数据集索引为0～599，共600条数据。各字段数据类型均为字符型。 3 数据清洗3.1 提取价格数值由于当前价格一列中含有¥符号，想对图书价格进行统计分析，需要从当前价格中取出价格的数值。借助正则表达书来完成上述操作。re库是python中正则表达式的支持库，使用findall()函数将当前价格中的数值提取出来，保存为新一列当前价格_match，findall()函数返回字符串中所有与正则表达式匹配的全部字符串，返回形式为数组。 +将前面的模式匹配一次或多次 ？匹配前一个字符零次或一次 *将前面的模式匹配零次或多次 .匹配除换行符之外的任意字符 \\转译字符 \\d匹配数字0～9 \\d{n}匹配正好n位数的数字 \\d{n,}匹配至少为n位的数字 \\d{n,m}匹配m～n位数的数字 [A-Za-z]+匹配英文字母组成的字符串 [A-Za-z0-9]+匹配由数字和英文字母组成的字符串DataFrame对象中apply方法可以将某个函数应用到由列或行形成的Series对象上，定义一个函数num_func，用于提取价格数值，然后使用apply方法将num_func应用到当前价格一列上。 1data[&#x27;当前价格&#x27;] 12345678import re#使用正则表达式将当前价格一列只读取数字部分并创建新的一列def func(data): result = re.findall(r&#x27;\\d+\\.?\\d*&#x27;,data) return float(result[0])data[&#x27;当前价格_match&#x27;] = data[&#x27;当前价格&#x27;].apply(func)data.head(3) 3.2 提取评论数值由于评论数一列中不仅包含数值，对评论数进行统计分析，需要从评论数一列中提取出评论的数值，保存为新的一列评论数_match。 1data[&#x27;评论数&#x27;] 12345678import re# 定义读取评论数的函数def func_1(data): esult = re.findall(r&#x27;\\d+&#x27;,data) return int(result[0])# 利用apply方法，将每一条数据进行处理data[&#x27;评论数_match&#x27;] = data[&#x27;评论数&#x27;].apply(func_1) 12# 查看是否处理成功data.head(3) 3.3 转换图书星级数值星级一列中同样包含一些其他字符，从星级一列中提取出星级的数值，将星级数值转换到[0,5]区间内，保存新的一列为星级_match。 123#提取星级数data[&#x27;星级_match&#x27;] = data[&#x27;星级&#x27;].apply(func_1)data.head(3) 12#将星级除以20，取值范围转换到[0,5]的区间内data[&#x27;星级_match_cal&#x27;] = data[&#x27;星级_match&#x27;].apply(lambda x:x/20) 1data.head(3) 3.4 提取作者、出版时间和出版社出版信息中包含作者、出版日期、出版社，用/分割为三部分，分三列存放，字符串对象的split方法通过制定分隔符对字符串进行切片。 1data[&#x27;出版信息&#x27;][:5] 1234# 将出版信息分割成三列，分别提取出作者、出版日期和出版社# 提取出作者data[&#x27;作者&#x27;] = data[&#x27;出版信息&#x27;].apply(lambda x:x.split(&#x27;/&#x27;)[0])data.head(3) 提取出的出版时间为字符串对象，我们可以通过datetime库中的strptime函数将字符串转换为datetime时间类型对象。 12345678910111213# 用正则表达式提取日期，并将日期字符串转换成日期格式from datetime import datetimedef func_2(data): result = re.findall(r&#x27;\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;&#x27;,data) if len(result)&lt;1: return None else: return datetime.strptime(result[0],&#x27;%Y-%m-%d&#x27;) #返回日期类型# 提取日期，并添加为新的一列data[&#x27;出版日期&#x27;] = data[&#x27;出版信息&#x27;].apply(func_2)data.head(3) 12345# 提取出版社一列，并添加为新的一列data[&#x27;出版社&#x27;] = data[&#x27;出版信息&#x27;].apply(lambda x:x.split(&#x27;\\&#x27;)[-1])# 查看结果data.head(3) 3.5 提取书名和简介书名字段包含书名和书籍的相关介绍。以空格为分隔符对字段进行分割，提取图书的书名和简介部分。 1data[&#x27;书名&#x27;][:5] 123456#将&#x27;【】&#x27;和&#x27;[]&#x27;以及之间的内容，用空格来代替def func_3(data): data = data.strip()#先去除头和尾的空格 data = re.sub(&quot;【.*?】&quot;,&quot; &quot;,data) data = re.sub(&quot;\\[.*?\\]&quot;,&quot; &quot;,data) return data.split(&quot; &quot;) 12data[&#x27;书名_split&#x27;] = data[&#x27;书名&#x27;].apply(func_3)data[&#x27;书名_split&#x27;][:5] 123# 提取书名data[&#x27;书名_split_1&#x27;] = data[&#x27;书名_split&#x27;].apply(lambda x:x[0])data[&#x27;书名_split_1&#x27;][:5] 1234#因为分割后的字段长度不唯一，所以从第三个开始我们要先判定每个字段分割后的长度，之后再进行提取# 提取简介1data[&#x27;书名_split_2&#x27;] = data[&#x27;书名_split&#x27;].apply(lambda x: None if len(x)&lt;=1 else x[1])data[&#x27;书名_split_2&#x27;][:5] 123# 提取简介2data[&#x27;书名_split_3&#x27;] = data[&#x27;书名_split&#x27;].apply(lambda x: None if len(x)&lt;=2 else x[2])data[&#x27;书名_split_3&#x27;][:5] 1data.head(3) 3.6 删除不需要的列使用DataFrame对象的drop方法删除不需要的列。 123# 删除不需要的列data.drop([&#x27;书名&#x27;,&#x27;出版信息&#x27;,&#x27;当前价格&#x27;,&#x27;星级&#x27;,&#x27;评论数&#x27;,&#x27;星级_match&#x27;,&#x27;书名_split&#x27;],axis=1,implace=True)data.head(3) 3.7 修改列名使用DataFrame对象的rname对列进行重命名。 123# 修改列名data.rename(columns=&#123;&#x27;当前价格_match&#x27;:&#x27;当前价格&#x27;,&#x27;评论数_match&#x27;:&#x27;评论数&#x27;,&#x27;星级_match_cal&#x27;:&#x27;星级&#x27;,&#x27;书名_split_1&#x27;:&#x27;书名&#x27;,&#x27;书名_split_2&#x27;:&#x27;简介1&#x27;,&#x27;书名_split_3&#x27;:&#x27;简介2&#x27;&#125;,implace=True)data.head(3) 4 保存数据使用DataFrame对象的to_csv方法将处理好的数据保存为CSV文件。 1data.to_csv(&#x27;当当网机器学习图书数据(已清洗).csv&#x27;, sep=&#x27;,&#x27;,encoding=&#x27;utf8&#x27;,index=False)"},{"title":"Hello World","path":"/2023/08/06/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"}]