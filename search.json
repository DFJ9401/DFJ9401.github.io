[{"title":"视觉transformer","path":"/2023/10/16/视觉transformer/","content":"环境12345678import torchfrom torch import nn, einsumimport torch.nn.functional as Fimport torchvisionfrom torchvision import transformsfrom einops import rearrange, repeatfrom einops.layers.torch import Rearrange torch PyTorch是一个用于构建深度学习模型的开源框架。 torch.nn torch.nn模块是PyTorch中用于构建神经网络的模块。 torchvision torchvision 是 PyTorch 的一个辅助库，专门用于计算机视觉任务。 transforms transforms 模块是 torchvision 库中的一个子模块，提供了一系列用于图像预处理和数据增强的转换操作。 einops 一个用于操作和转换张量的Python库。 数据加载和处理图像预处理1234transform = transforms.Compose([ transforms.ToTensor(), transforms.Resize((32, 32))]) transforms.Compose是一个用于组合多个数据转换操作的类。通过将一系列转换操作传递给transforms.Compose，可以按顺序应用这些操作。 transforms.ToTensor()将图像数据转换为PyTorch的张量格式。它将图像数据从范围[0, 255]归一化到范围[0, 1]之间，并将其转换为张量形式。 transforms.Resize((32, 32))用于调整图像的尺寸大小。 加载 CIFAR-10 数据集12train_dataset = torchvision.datasets.CIFAR10(root=&#x27;./cifar-10-batches-py&#x27;, train=True, transform=transform, download=True)test_dataset = torchvision.datasets.CIFAR10(root=&#x27;./cifar-10-batches-py&#x27;, train=False, transform=transform, download=True) root指定了数据集的存储路径。如果该文件夹不存在，数据集将被下载并存储在该位置。 train表示加载训练集。设置为True时，将加载训练集，其中包含用于模型训练的图像和标签数据。 transform指定了对加载的图像数据进行的转换操作。在上述代码中，我们之前定义的transform对象被传递给transform参数，以便对加载的图像数据进行转换。 download表示如果数据集文件不存在，则自动从网络下载数据集。如果数据集文件已经存在，则不会重新下载。 加载 CIFAR-10 数据集并进行数据预处理。 12train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False) batch_size指定了每个批次中的样本数量。数据加载器将数据集分成多个批次，并每次返回一个批次的数据进行训练或测试。 shuffle True表示在每个训练迭代中对数据进行洗牌操作，以打乱样本的顺序。这有助于增加数据的随机性和训练的多样性，在模型训练中更好地进行参数更新。False表示在测试过程中不对数据进行洗牌操作，以确保测试结果的一致性和可比性。 将数据集划分为批次并提供数据的迭代器。 ViT模型12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class ViT(nn.Module): def __init__( self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool=&#x27;cls&#x27;, channels=3, dim_head=64, dropout=0., emb_dropout=0. ): super().__init__() image_height, image_width = pair(image_size) patch_height, patch_width = pair(patch_size) assert image_height % patch_height == 0 and image_width % patch_width == 0 num_patches = (image_height // patch_height) * (image_width // patch_width) patch_dim = channels * patch_height * patch_width assert pool in &#123;&#x27;cls&#x27;, &#x27;mean&#x27;&#125; self.to_patch_embedding = nn.Sequential( Rearrange(&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;, p1=patch_height, p2=patch_width), nn.Linear(patch_dim, dim), ) self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim)) self.cls_token = nn.Parameter(torch.randn(1, 1, dim)) self.dropout = nn.Dropout(emb_dropout) self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) self.pool = pool self.to_latent = nn.Identity() self.mlp_head = nn.Sequential( nn.LayerNorm(dim), nn.Linear(dim, num_classes) ) def forward(self, img): x = self.to_patch_embedding(img) b, n, _ = x.shape cls_tokens = repeat(self.cls_token, &#x27;() n d -&gt; b n d&#x27;, b=b) x = torch.cat((cls_tokens, x), dim=1) x += self.pos_embedding[:, :(n + 1)] x = self.dropout(x) x = self.transformer(x) x = x.mean(dim=1) if self.pool == &#x27;mean&#x27; else x[:, 0] x = self.to_latent(x) return self.mlp_head(x) to_patch_embedding用于将输入图像划分为小块并进行嵌入的序列模块。它通过Rearrange操作将输入图像重新排列为patch形状，并通过线性层将每个patch嵌入到维度为dim的向量中。pos_embedding位置嵌入参数，用于为每个patch和特殊的分类令牌（cls token）提供位置信息。cls_token分类令牌，用于表示整个图像的全局特征。dropout用于在输入嵌入向量之前对位置嵌入和分类令牌进行随机丢弃，以增强模型的鲁棒性。transformerTransformer模型，由depth个编码器层组成。它对输入进行自注意力机制和前馈神经网络操作，以捕捉输入的空间关系和特征表示。pool池化方式，可以是’cls’（只使用分类令牌的输出）或’mean’（对所有patch的输出进行平均）。to_latent标识函数，用于在池化后的特征上进行可选的降维操作。mlp_head用于分类预测的多层感知机头部。它对池化后的特征进行归一化操作，然后通过线性层将特征映射到num_classes个类别。 123456789101112131415class Transformer(nn.Module): def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.): super().__init__() self.layers = nn.ModuleList([]) for _ in range(depth): self.layers.append(nn.ModuleList([ PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)), PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)) ])) def forward(self, x): for attn, ff in self.layers: x = attn(x) + x x = ff(x) + x return x PreNorm是一种用于层归一化（Layer Normalization）的模块，它在模型中起到归一化输入数据的作用。 12345678910111213141516171819202122232425262728293031323334353637383940414243class Attention(nn.Module): def __init__(self, dim, heads=8, dim_head=64, dropout=0.): super().__init__() # 计算内部维度和是否需要投影输出 inner_dim = dim_head * heads project_out = not (heads == 1 and dim_head == dim) # 设置模块属性 self.heads = heads self.scale = dim_head ** -0.5 # 定义注意力权重计算和查询-键-值的线性变换 self.attend = nn.Softmax(dim=-1) self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False) # 定义输出投影模块 self.to_out = nn.Sequential( nn.Linear(inner_dim, dim), # 线性变换 nn.Dropout(dropout), # dropout操作 ) if project_out else nn.Identity() # 如果不需要投影则使用恒等映射 def forward(self, x): b, n, _, h = *x.shape, self.heads # 将输入x进行查询-键-值的线性变换并分割 qkv = self.to_qkv(x).chunk(3, dim=-1) q, k, v = map(lambda t: rearrange(t, &#x27;b n (h d) -&gt; b h n d&#x27;, h=h), qkv) # 计算注意力得分 dots = einsum(&#x27;b h i d, b h j d -&gt; b h i j&#x27;, q, k) * self.scale # 计算注意力权重 attn = self.attend(dots) # 加权平均值 out = einsum(&#x27;b h i j, b h j d -&gt; b h i d&#x27;, attn, v) # 重排输出维度 out = rearrange(out, &#x27;b h n d -&gt; b n (h d)&#x27;) # 进行输出投影 return self.to_out(out) 初始化方法 __init__：对模块进行初始化。 dim：输入数据的维度。 heads：自注意力中的注意力头数。 dim_head：每个注意力头的维度。 dropout：注意力计算过程中的 dropout 概率。 inner_dim：计算每个注意力头的维度乘以注意力头数，得到内部维度。 project_out：根据头数和维度判断是否需要对输出进行投影。 self.heads：保存注意力头数。 self.scale：用于缩放注意力计算中的点积操作的缩放因子。 self.attend：使用 softmax 函数进行注意力权重的计算。 self.to_qkv：通过线性变换将输入数据映射到查询（Query）、键（Key）和值（Value）的多头注意力表示。 self.to_out：如果需要对输出进行投影，则定义一个包含线性变换和 dropout 操作的序列模块；否则，使用 nn.Identity() 作为输出模块。 前向传播方法 forward：对输入进行自注意力计算。 b, n, _, h = *x.shape, self.heads：通过输入 x 的形状确定批量大小 b、序列长度 n 和注意力头数 h。 qkv = self.to_qkv(x).chunk(3, dim=-1)：将输入 x 经过线性变换 self.to_qkv 得到查询、键和值的表示，并使用 chunk 方法将结果分成三部分。 q, k, v = map(lambda t: rearrange(t, &#39;b n (h d) -&gt; b h n d&#39;, h=h), qkv)：将查询、键和值的表示按照维度重排，使得每个注意力头的维度成为第三个维度。 dots = einsum(&#39;b h i d, b h j d -&gt; b h i j&#39;, q, k) * self.scale：计算查询和键之间的点积，并乘以缩放因子 self.scale，得到注意力得分。 attn = self.attend(dots)：使用 softmax 函数计算注意力得分的权重。 out = einsum(&#39;b h i j, b h j d -&gt; b h i d&#39;, attn, v)：使用注意力权重对值进行加权平均，得到自注意力的输出。 out = rearrange(out, &#39;b h n d -&gt; b n (h d)&#39;)：重新排列输出的维度顺序，将注意力头的维度和序列长度的维度合并。 return self.to_out(out)：将输出经过投影模块 self.to_out 进行处理，并返回最终的输出结果。 12345678910111213class FeedForward(nn.Module): def __init__(self, dim, hidden_dim, dropout=0.): super().__init__() self.net = nn.Sequential( nn.Linear(dim, hidden_dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden_dim, dim), nn.Dropout(dropout) ) def forward(self, x): return self.net(x) 定义了一个前馈神经网络模块 FeedForward，通过一个线性层、GELU 激活函数和 dropout 操作构成了一个非线性转换的序列模块。该模块用于在 Transformer 编码器中对位置编码进行非线性转换，以增强模型的表达能力。 训练及评价创建对象1234567891011model = ViT( image_size=32, patch_size=4, num_classes=10, dim=256, depth=12, heads=8, mlp_dim=512, dropout=0.1, emb_dropout=0.1) image_size输入图像的大小。在这个例子中，输入图像的宽度和高度都是32个像素。 patch_size将输入图像分成小块（或称为patches）的大小。每个小块的宽度和高度都是4个像素。 num_classes分类任务的类别数量。在这个例子中，有10个类别需要分类。 dim嵌入向量的维度。每个小块将被嵌入成一个具有256维的向量。 depth编码器层数。在这个例子中，有12个编码器层用于处理输入。 heads多头注意力机制中的头数。在这个例子中，每个注意力头有8个。 mlp_dim全连接层（多层感知机）的隐藏层维度。在这个例子中，隐藏层的维度为512。 dropout模型中的Dropout层的丢弃率。在这个例子中，Dropout层的丢弃率为0.1，用于防止过拟合。 emb_dropout嵌入层的Dropout层的丢弃率。在这个例子中，嵌入层的丢弃率为0.1，用于在输入嵌入向量之前对输入图像进行随机丢弃。 损失函数和优化器12criterion = torch.nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) CrossEntropyLoss()是用于多类别分类问题的交叉熵损失函数。optim.Adam是Adam优化算法的实现，用于调整模型参数以最小化损失函数。model.parameters()用于传递模型的可学习参数给优化器，以便进行参数更新。 训练及评估12device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)model.to(device) 12345678910111213141516171819202122232425262728for epoch in range(10): model.train() for images, labels in train_loader: images = images.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() model.eval() total_correct = 0 total_samples = 0 with torch.no_grad(): for images, labels in test_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs, dim=1) total_samples += labels.size(0) total_correct += (predicted == labels).sum().item() accuracy = total_correct / total_samples print(f&quot;Epoch &#123;epoch + 1&#125;, Accuracy: &#123;accuracy:.4f&#125;&quot;) model.train()将模型设置为训练模式，启用模型中的训练特定操作，例如批量归一化和Dropout。optimizer.zero_grad()梯度清零。loss = criterion(outputs, labels)计算模型输出与真实标签之间的损失。criterion 是定义的损失函数，用于衡量模型输出与标签之间的差异。loss.backward()根据损失函数计算模型参数的梯度。optimizer.step()根据梯度更新模型参数。optimizer 是定义的优化器，用于更新模型的参数。with torch.no_grad()这个语句块定义了一个上下文管理器，其中的代码块将不会进行梯度计算。这样可以减少内存消耗并加快代码的执行速度。torch.max() 函数返回给定张量中的最大值及其对应的索引。在这里，第一个参数是模型输出的数据部分，而第二个参数 1 表示在每个样本的维度上进行比较。 其他工作自注意力自注意力机制模型123456789101112131415161718192021222324class SelfAttention(nn.Module): def __init__(self, in_dim): super(SelfAttention, self).__init__() self.query = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1) self.key = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1) self.value = nn.Conv2d(in_dim, in_dim, kernel_size=1) self.gamma = nn.Parameter(torch.zeros(1)) def forward(self, x): batch_size, channels, height, width = x.size() query = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1) key = self.key(x).view(batch_size, -1, width * height) energy = torch.bmm(query, key) attention = torch.softmax(energy, dim=-1) value = self.value(x).view(batch_size, -1, width * height) out = torch.bmm(value, attention.permute(0, 2, 1)) out = out.view(batch_size, channels, height, width) out = self.gamma * out + x return out 自注意力机制的网络模型1234567891011121314151617181920212223242526class SelfAttentionNet(nn.Module): def __init__(self): super(SelfAttentionNet, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1) self.attention1 = SelfAttention(64) self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) self.attention2 = SelfAttention(128) self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) self.fc = nn.Linear(128 * 8 * 8, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = self.attention1(x) x = self.pool1(x) x = torch.relu(self.conv2(x)) x = self.attention2(x) x = self.pool2(x) x = x.view(x.size(0), -1) x = self.fc(x) return x 直接调用ViT123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import torchimport torch.nn as nnimport torch.optim as optimimport torchvision.transforms as transformsfrom torchvision.datasets import CIFAR10from torch.utils.data import DataLoaderfrom torchvision.models import resnet18from torch.optim.lr_scheduler import StepLRfrom vit_pytorch import ViT# 设置随机种子torch.manual_seed(42)# 设置设备（GPU或CPU）device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)# 数据预处理transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])# 加载CIFAR-10数据集train_dataset = CIFAR10(root=&#x27;./cifar-10-batches-py&#x27;, train=True, download=True, transform=transform)test_dataset = CIFAR10(root=&#x27;./cifar-10-batches-py&#x27;, train=False, download=True, transform=transform)# 创建数据加载器batch_size = 64train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)# 定义ViT模型model = ViT( image_size=224, patch_size=16, num_classes=10, dim=768, depth=6, heads=8, mlp_dim=3072, dropout=0.1, emb_dropout=0.1).to(device)# 定义损失函数和优化器criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=0.001)scheduler = StepLR(optimizer, step_size=5, gamma=0.5)# 训练模型num_epochs = 10for epoch in range(num_epochs): model.train() train_loss = 0.0 train_correct = 0.0 total = 0 for images, labels in train_loader: images = images.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() train_loss += loss.item() _, predicted = outputs.max(1) train_correct += predicted.eq(labels).sum().item() total += labels.size(0) train_accuracy = 100.0 * train_correct / total print(f&quot;Epoch [&#123;epoch + 1&#125;/&#123;num_epochs&#125;], Train Loss: &#123;train_loss:.4f&#125;, Train Accuracy: &#123;train_accuracy:.2f&#125;%&quot;) scheduler.step()# 在测试集上评估模型model.eval()test_correct = 0.0total = 0with torch.no_grad(): for images, labels in test_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) _, predicted = outputs.max(1) test_correct += predicted.eq(labels).sum().item() total += labels.size(0)test_accuracy = 100.0 * test_correct / totalprint(f&quot;Test Accuracy: &#123;test_accuracy:.2f&#125;%&quot;) 自定ViT1234567891011121314151617181920class ViT(nn.Module): def __init__(self, image_size=32, patch_size=4, num_classes=10, dim=128, num_heads=8, num_layers=6): super(ViT, self).__init__() self.patch_size = patch_size self.num_patches = (image_size // patch_size) ** 2 self.embedding = nn.Linear(3 * patch_size * patch_size, dim) self.patch_embedding = nn.Sequential( Rearrange(&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;, p1=patch_size, p2=patch_size), nn.Linear(patch_size * patch_size * 3, dim) ) self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(dim, num_heads), num_layers) self.fc = nn.Linear(dim, num_classes) def forward(self, x): x = self.patch_embedding(x) x = self.transformer(x) x = x.mean(dim=1) x = self.fc(x) return x"},{"title":"非参特征表征","path":"/2023/10/16/非参特征表征/","content":"环境1234567891011121314151617181920%matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltimport matplotlib.animation as animationfrom scipy.stats import gaussian_kdefrom scipy.integrate import cumtrapzfrom numpy import linalg as laimport sysimport sparsify_PyTorchimport utilityimport torchimport timefrom scipy.ndimage import zoomimport sysimport importlibimportlib.reload(sys)from importlib import reloadimport torch.linalg as linalgimport scipy.io as sioimport cv2 NumPy 使用NumPy的函数和数据结构。 matplotlib 使用Matplotlib绘图函数。 scipy 估计数据的概率密度函数,计算数据的累积积分,对数组进行插值和重采样。 PyTorch 用于线性代数运算，如矩阵分解和求逆等。 cv2 OpenCV是一个开源计算机视觉库，它提供了丰富的图像处理和计算机视觉算法。 utility 该模块包含了一些实用函数和工具。 sparsify_PyTorch 一个用于稀疏化PyTorch模型的工具或技术。稀疏化是一种优化模型的方法，通过减少模型中参数的数量，从而降低模型的存储需求和计算成本，同时保持模型的性能。 实现方法使用流形学习计算降维后的字典1234567891011121314151617181920def spd(ahat): # 将输入矩阵A转换为PyTorch张量并移到GPU上 A = torch.matmul(ahat, ahat.t()) A=A.cpu().numpy() N = A.shape[0]#1/N inv_sqrt = torch.rsqrt(torch.tensor(N, dtype=torch.float32, device=&#x27;cuda&#x27;)) # 计算 1/n 的平方根 # 特征分解 v, Q = la.eig(A) #min_non_zero = np.min(v[v &gt; 0]) # 将数组中的零值替换为最小非零值 #v[v &lt;= 0] = min_non_zero V = np.diag(v**(-0.5)) T = Q * V * la.inv(Q) V_cuda = torch.from_numpy(T).cuda() V_cuda =V_cuda * inv_sqrt return V_cuda 该函数的作用是对输入矩阵进行特征分解，并返回一个经过处理的CUDA张量。 1234567891011121314151617def compute_smallest_eigenvectors(A, k): # 将数据转换为PyTorch张量 # A_tensor = torch.tensor(A, dtype=torch.float32) # 计算矩阵 A 的特征值和特征向量 eigenvalues, eigenvectors = torch.linalg.eig(A) eigenvalues = eigenvalues.real # 提取实部 eigenvectors = eigenvectors.real # 提取实部 # 对特征值进行排序，找到最小的 k 个特征值的索引 sorted_indices = torch.topk(eigenvalues, k, largest=False).indices # 提取最小的 k 个特征向量 smallest_eigenvectors = eigenvectors[:, sorted_indices] A_transposed = torch.transpose(smallest_eigenvectors, 0, 1) return A_transposed 计算输入矩阵A的特征值和特征向量，并返回最小的k个特征向量的转置。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def Sparse_train(Image, x_in, y_in): vm=Image xdim = x_in # Patch size ydim = y_in # Patch size stride = 6 rows = (vm.shape[0] - xdim) // stride + 1 cols = (vm.shape[1] - ydim) // stride + 1 I = np.zeros([xdim * ydim, rows * cols]).astype(&#x27;float32&#x27;) x = 0 for j in range(cols): if j % 2 == 0: for i in range(rows): I[:, x] = vm[i * stride:i * stride + xdim, j * stride:j * stride + ydim].reshape([xdim * ydim]) x += 1 else: for i in range(rows - 1, -1, -1): I[:, x] = vm[i * stride:i * stride + xdim, j * stride:j * stride + ydim].reshape([xdim * ydim]) x += 1 I_cuda = torch.from_numpy(I).cuda() # 加载 .npz 文件 data = np.load(&#x27;basis1_IMAGES_Vanhateren_12x.npz&#x27;) basis11 = data[&#x27;basis1&#x27;] basis1 = torch.from_numpy(basis11).cuda() ahat, Res = sparsify_PyTorch.FISTA(I_cuda, basis1, 0.08, 500) V = np.zeros([2048, 2048]).astype(&#x27;float32&#x27;) V_cuda = torch.from_numpy(V).cuda() V_cuda = spd(ahat) # v-1/2 n = rows * cols # 创建一个全零矩阵，分配到CUDA设备上 D_cuda = torch.zeros((n, n), device=device) # 设置主对角线为 1 D_cuda.diagonal().fill_(1) # 设置副对角线为 -1 D_cuda.diagonal(offset=1).fill_(-1 / 2) D_cuda.diagonal(offset=-1).fill_(-1 / 2) P_1 = torch.mm(torch.mm(torch.mm(torch.mm(torch.mm(V_cuda, ahat), D_cuda), D_cuda.t()), ahat.t()), V_cuda) U = compute_smallest_eigenvectors(P_1, 21) P = torch.mm(U, V_cuda) return P 训练得到将维后的字典P。 稀疏编码和降维1234567891011121314151617181920212223242526272829303132333435363738394041424344def Sparse1(Image, P, basis1, x_in, y_in): # 字典块大小 vm = Image # 将参数 Image 赋值给 vm 变量 xdim = x_in # 补丁大小 ydim = y_in # 补丁大小 stride = 8 # 步幅 rows = (vm.shape[0] - xdim) // stride + 1 # 行数 cols = (vm.shape[1] - ydim) // stride + 1 # 列数 I = np.zeros([xdim * ydim, rows * cols]).astype(&#x27;float32&#x27;) # 创建大小为 [xdim * ydim, rows * cols] 的零矩阵 x = 0 # 初始化 x 为 0 for j in range(cols): if j % 2 == 0: for i in range(rows): I[:, x] = vm[i * stride:i * stride + xdim, j * stride:j * stride + ydim].reshape([xdim * ydim]) x += 1 else: for i in range(rows - 1, -1, -1): I[:, x] = vm[i * stride:i * stride + xdim, j * stride:j * stride + ydim].reshape([xdim * ydim]) x += 1 I_cuda = torch.from_numpy(I).cuda() # 将 Numpy 数组 I 转换为 PyTorch 的张量，并将其移动到 GPU 上 # 加载 .npz 文件 ahat, Res = sparsify_PyTorch.FISTA(I_cuda, basis1, 0.08, 500) # 调用名为 FISTA 的函数，传递参数 I_cuda、basis1、0.08 和 500，并返回 ahat 和 Res B = torch.mm(P, ahat) # 使用 PyTorch 的矩阵乘法函数 mm，将 P 和 ahat 相乘，结果存储在 B 中 F = np.zeros([rows, cols, 21]).astype(&#x27;float32&#x27;) # 创建大小为 [rows, cols, 21] 的零矩阵 F_cuda = torch.from_numpy(F).cuda() # 将 Numpy 数组 F 转换为 PyTorch 的张量，并将其移动到 GPU 上 x = 0 # 初始化 x 为 0 for j in range(cols): if j % 2 == 0: for i in range(rows): F_cuda[i, j, :] = B[:, x] # 将 B[:, x] 赋值给 F_cuda[i, j, :] x += 1 else: for i in range(rows - 1, -1, -1): F_cuda[i, j, :] = B[:, x] # 将 B[:, x] 赋值给 F_cuda[i, j, :] x += 1 f = F_cuda.cpu().numpy() # 将 GPU 上的张量 F_cuda 转换为 Numpy 数组 f return f, rows, cols # 返回 f、rows 和 cols 将输入的灰度图进行滑窗分块，然后进行稀疏编码，得到稀疏矩阵，再使用流形学习进行降维。 目标检测1234567891011121314151617181920def proto_object(smap, blocksize): #对于T按照f_t大小resizze得到 s = &#123;&#125; # 存储块的起始位置 e = &#123;&#125; # 存储块的结束位置 m, n = smap.shape # 获取smap数组的行数和列数 smap_integral = np.cumsum(np.cumsum(smap, axis=1), axis=0) # 计算smap数组的积分图 x_block = blocksize[0] # 块的宽度 y_block = blocksize[1] # 块的高度 flag = np.zeros((y_block, x_block)) # 存储块的标志 block_width = int(n // x_block) # 每个块的宽度 block_height = int(m // y_block) # 每个块的高度 block = np.zeros((m, n)) # 存储块数据 for i in range(x_block): for j in range(y_block): s[(i, j)] = &#123;&#x27;x&#x27;: i * block_width, &#x27;y&#x27;: j * block_height&#125; # 记录块的起始位置 e[(i, j)] = &#123;&#x27;x&#x27;: (i + 1) * block_width - 1, &#x27;y&#x27;: (j + 1) * block_height - 1&#125; # 记录块的结束位置 flag[i, j] = 1 # 将块的标志设置为1，表示有效块 return block, flag, s, e # 返回块数据、块标志、块起始位置和块结束位置 对输入图像进行分块（16x16）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899def FinalStage3(n, rm, q, t, alpha, s_ind, sc, searchmode, target): #图像编号 多尺度的参数 s1 = f&#x27;&#123;n&#125;:&#x27; rm1 = rm rm2 = np.zeros(rm.shape) overlap=[] if np.max(rm) &gt; 0: f_rho = rm[int(q.shape[0] / 2):rm.shape[0] - int(q.shape[0] / 2), int(q.shape[1] / 2):rm.shape[1] - int(q.shape[1] / 2)] e_pdf, ind = np.histogram(f_rho.flatten(), density=True) e_cdf = np.cumsum(e_pdf / np.sum(e_pdf)) detection = np.where(e_cdf &gt; alpha)[0] plt.figure() plt.plot(ind[:-1], e_pdf / np.sum(e_pdf)) t_n = ind[detection[0]] if t_n &lt; 0: t_n = 0 x = q.shape[0] y = q.shape[1] half_x1 = x // 2 half_y1 = y // 2 cnt = 0 while np.max(rm1) &gt; t_n: x_ind, y_ind = np.where(rm == np.max(rm1)) if len(x_ind) &gt; 1: x_ind = x_ind[0] y_ind = y_ind[0] if searchmode != 0: scaled_x1 = int(np.floor(half_x1 / sc[s_ind[x_ind, y_ind]])) scaled_y1 = int(np.floor(half_y1 / sc[s_ind[x_ind, y_ind]])) else: scaled_x1 = half_x1 scaled_y1 = half_y1 x_b = x_ind - scaled_x1 y_b = y_ind - scaled_y1 x_e = x_ind + scaled_x1 y_e = y_ind + scaled_x1 hengzhou = t.shape[0] # 获取矩阵T的行数，并将结果赋值给变量hengzhou shuzhou = t.shape[1] # 获取矩阵T的列数，并将结果赋值给变量shuzhou target_h = target.shape[0] # 获取矩阵target的行数，并将结果赋值给变量target_h target_s = target.shape[1] # 获取矩阵target的列数，并将结果赋值给变量target_s target_1 = target_h / hengzhou # 计算target_h除以hengzhou的结果，并将结果赋值给变量target_1 target_2 = target_s / shuzhou # 计算target_s除以shuzhou的结果，并将结果赋值给变量target_2 target_3 = np.ceil(y_b * target_1) # 将y_b乘以target_1的结果进行向上取整，并将结果赋值给变量target_3 target_4 = np.ceil(x_b * target_2) # 将x_b乘以target_2的结果进行向上取整，并将结果赋值给变量target_4 x1, x2 = np.meshgrid(np.arange(-scaled_y1, scaled_y1 + 1), np.arange(-scaled_x1, scaled_x1 + 1)) kkk = np.exp(-(0.5 / (1) ** 2) * (x1 ** 2 + x2 ** 2)) if x_b &lt;= 0: x_b = 0 x_e = x_b + kkk.shape[0] - 1 if x_e &gt; np.floor(rm.shape[0]): x_e = np.floor(rm.shape[0]) x_b = x_e - kkk.shape[0] + 1 if y_b &lt;= 0: y_b = 0 y_e = y_b + kkk.shape[1] - 1 if y_e &gt; np.floor(rm.shape[1]): y_e = np.floor(rm.shape[1]) y_b = y_e - kkk.shape[1] + 1 # 初始化一个长度为cnt-1的全0数组 overlap.append(np.zeros_like(rm)) overlap[cnt][max(0, x_b):min(x_e, rm.shape[0]-1), max(0, y_b):min(y_e, rm.shape[1]-1)] = 1 ol = np.zeros(cnt) # 创建一个长度为cnt-1的全0数组 if cnt &gt; 0: for cnt1 in range(0, cnt): intersection = overlap[cnt] * overlap[cnt1] union = overlap[cnt] + overlap[cnt1] union = union &gt; 0 ol[cnt1] = np.sum(intersection) / np.sum(union) if np.max(ol) &lt; 0.01: rm2 = drawbox_bold(rm2, max(0, x_b), max(0, y_b), min(x_e, rm2.shape[0]-1), min(y_e, rm2.shape[1]-1), np.max(rm1)) # print(np.max(RM1)) s1 = f&#x27;&#123;s1&#125; (&#123;target_4&#125;,&#123;target_3&#125;) &#123;np.max(rm1)&#125;&#x27; t[max(0, x_b):min(x_e, t.shape[0]-1), max(0, y_b):min(y_e, t.shape[1]-1)] = np.array(image.fromarray(q).resize((rm2[max(0, x_b):min(x_e, rm2.shape[0]-1), max(0, y_b):min(y_e, rm2.shape[1]-1)]).shape[1],rm2[max(0, x_b):min(x_e, rm2.shape[0]-1), max(0, y_b):min(y_e, rm2.shape[1]-1)]).shape[0]) rm2[max(0, x_b):min(x_e, rm2.shape[0]-1), max(0, y_b):min(y_e, rm2.shape[1]-1)] = rm2[max(0,x_b):min(x_e, rm2.shape[0]-1), max(0, y_b):min(y_e, rm2.shape[1]-1)] rm1[max(0, x_b):min(x_e, rm1.shape[0]-1), max(0, y_b):min(y_e, rm1.shape[1]-1)] = 0 else: rm1[max(0, x_b):min(x_e, rm1.shape[0]-1), max(0, y_b):min(y_e, rm1.shape[1]-1)] = 0 ol = None else: rm2 = drawbox_bold(rm2, max(0, x_b), max(0, y_b), min(x_e, RM2.shape[0]-1), min(y_e, RM2.shape[1]-1),np.max(rm1)) # print(np.max(RM1)) s1 = f&#x27;&#123;s1&#125; (&#123;target_4&#125;,&#123;target_3&#125;) &#123;np.max(rm1)&#125;&#x27; t[max(0, x_b):min(x_e, t.shape[0]-1), max(0, y_b):min(y_e, t.shape[1]-1)] = np.array(image.fromarray(q).resize((rm2[max(0, x_b):min(x_e, rm2.shape[0]-1), max(0, y_b):min(y_e, rm2.shape[1]-1)]).shape[1],rm2[max(0, x_b):min(x_e, rm2.shape[0]-1), max(0, y_b):min(y_e, rm2.shape[1]-1)]).shape[0]) rm2[max(0, x_b):min(x_e, rm2.shape[0]-1), max(0, y_b):min(y_e, rm2.shape[1]-1)] = rm2[max(0,x_b):min(x_e, rm2.shape[0]-1), max(0, y_b):min(y_e, rm2.shape[1]-1)] rm1[max(0, x_b):min(x_e, rm1.shape[0]-1), max(0, y_b):min(y_e, rm1.shape[1]-1)] = 0 del x1, x2, kkk cnt += 1 return rm2, t, s1 该函数的作用是进行目标检测和处理，根据给定的参数和阈值对输入的矩阵进行处理，并返回处理后的矩阵和相关信息。记录该矩形框的边界坐标(x_b, y_b, x_e, y_e)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374def stage3forMultiscale(rm, q, alpha): #每个小块的权重参数 rm1 = rm.copy() #rm2 = np.zeros(rm.shape) rm3 = np.zeros(rm.shape) if np.max(rm) &gt; 0.01: f_rho = rm[int(np.floor(q.shape[0] / 2)):rm.shape[0] - int(np.floor(q.shape[0] / 2)), int(np.floor(q.shape[1] / 2)):rm.shape[1] - int(np.floor(q.shape[1] / 2))] f_rho_vector = f_rho.flatten() # 将f_rho转换为一维向量 kde = gaussian_kde(f_rho_vector) e_pdf = kde.evaluate(f_rho_vector) ind = np.argsort(f_rho_vector) e_pdf_normalized = e_pdf / np.sum(e_pdf) # 将PDF归一化 e_cdf = cumtrapz(e_pdf_normalized, initial=0) # 计算CDF detection = np.where(e_cdf &gt; alpha)[0] t_n = ind[detection[0]] if t_n &lt; 0: t_n = 0 x = q.shape[0] y = q.shape[1] half_x1 = x // 2 half_y1 = y // 2 x_range = np.arange(-half_y1, half_y1 + 1) y_range = np.arange(-half_x1, half_x1 + 1) x1, x2 = np.meshgrid(x_range, y_range) tt = x1 ** 2 + x2 ** 2 kkk = np.exp(-(0.5 / 5 ** 2) * tt) cnt = 0 # 非极大抑制 while np.max(rm1) &gt;= t_n: cnt = cnt + 1 x_ind, y_ind = np.where(rm == np.max(rm1)) if len(x_ind) &gt; 1: x_ind = x_ind[0] y_ind = y_ind[0] np.column_stack((x_ind, y_ind)) indx[cnt - 1] = x_ind indy[cnt - 1] = y_ind # 这里好像没有用到这个，先给## x_b = x_ind - half_x1 y_b = y_ind - half_y1 x_e = x_ind + half_x1 y_e = y_ind + half_y1 if x_b &lt;= 0: x_b = 0 x_e = x_b + kkk.shape[0] - 1 if x_e &gt; np.floor(rm.shape[0]): x_e = np.floor(rm.shape[0]) x_b = x_e - kkk.shape[0] + 1 if y_b &lt;= 0: y_b = 0 y_e = y_b + kkk.shape[1] - 1 if y_e &gt; np.floor(rm.shape[1]): y_e = np.floor(rm.shape[1]) y_b = y_e - kkk.shape[1] + 1 #rm2[x_b - 1:x_e, y_b - 1:y_e] = rm1[x_b - 1:x_e, y_b - 1:y_e] * kkk[:x_e - x_b + 1, :y_e - y_b + 1] rm3[x_b - 1:x_e, y_b - 1:y_e] = rm1[x_b - 1:x_e, y_b - 1:y_e] #rm2 = draw_box(rm, x_b - 1, y_b - 1, x_e - 1, y_e - 1, np.max(rm1)) rm1[x_b - 1:x_e, y_b - 1:y_e] = 0 det_tag = 1 else: det_tag = 0 indx = 0 indy = 0 return rm3 多尺度搜索目标。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114def MultiScaleSearch_CoarseToFine(k1, Q, T, F_Q, F_T, S, E, alpha, target, blocksize): # 图像编号 缩小后的q t 特征矩阵q t 搜索索引开始 结束 置信参数 原始图像 分块数量 M_Q, N_Q = Q.shape M_T, N_T = T.shape stepsize = min(N_Q, M_Q) SC_t = [] # for kkk in range(0, 19): for kkk in range(1, 21): if max(M_T, N_T) &lt; 400: SC_t.append((400 + stepsize * (kkk - 15) * 1.5) / 400) elif max(M_T, N_T) &gt; 600: SC_t.append((600 + stepsize * (kkk - 15) * 1.5) / 600) else: SC_t.append((max(M_T, N_T) + stepsize * (kkk - 15) * 1.5) / max(M_T, N_T)) SC_t = [x for x in SC_t if x &gt; 0.4] SC_t = [x for x in SC_t if x &lt; 1.6] SC = [0.3, 0.5] space = 1 interval = 1 print(&#x27;Multi-scale search.&#x27;) progress = 0 RM3s = np.zeros((F_T.shape[0], F_T.shape[1], len(SC_t))) # 根据实际需求确定数组的形状 Ts = &#123;&#125; RMs = &#123;&#125; for n in range(len(SC_t)): flags = np.copy(flag) for m in range(len(SC)): progress = progress + 1 / len(SC) * len(SC_t) if progress &gt; 0.025: progress = progress - 0.025 print(&#x27;.&#x27;, end=&#x27;&#x27;, flush=True) Qs = zoom(Q, SC[m]) F_Qs = zoom(F_Q, SC[m]) F_Q1 = F_Qs[::space, ::space, :] # norm_FQ = np.linalg.norm(F_Q1.ravel(), ord=&#x27;fro&#x27;) norm_FQ = np.linalg.norm(F_Q1) F_Ts = zoom(F_T, SC_t[n] * SC[m], order=3) Ts1 = zoom(T, SC_t[n] * SC[m], order=3) Ts[(m, n)] = np.zeros_like(Ts1) Ts[(m, n)] = Ts1 RMs[(m, n)] = np.zeros_like(Ts[(m, n)]) f_max = np.zeros(blocksize) for x in range(blocksize[0]): for y in range(blocksize[1]): S1_x = max(int(S[x, y][&#x27;x&#x27;] * SC[m] * SC_t[n]), 0) E1_x = min(max(int(E[x, y][&#x27;x&#x27;] * SC[m] * SC_t[n]), 0), F_Ts.shape[1] - F_Qs.shape[1]) S1_y = max(int(S[x, y][&#x27;y&#x27;] * SC[m] * SC_t[n]), 0) E1_y = min(max(int(E[x, y][&#x27;y&#x27;] * SC[m] * SC_t[n]), 0), F_Ts.shape[0] - F_Qs.shape[0]) if E1_y &lt; S1_y: S1_y, E1_y = E1_y, S1_y if E1_x &lt; S1_x: S1_x, E1_x = E1_x, S1_x if flags[x, y] == 1: for i in range(S1_y, E1_y + 1, interval): for j in range(S1_x, E1_x + 1, interval): if i + F_Qs.shape[0] &lt;= F_Ts.shape[0] and j + F_Qs.shape[1] &lt;= F_Ts.shape[1]: F_T_i = F_Ts[i: i + F_Qs.shape[0]: space, j: j + F_Qs.shape[1]: space, :] norm_FTI = np.linalg.norm(F_T_i) rho = np.dot(F_Q1.ravel(), F_T_i.ravel()) / (norm_FQ * norm_FTI) RMs[(m, n)][int(F_Qs.shape[0] / 2) + i, int(F_Qs.shape[1] / 2) + j] = (rho ** 2) / (1 - rho ** 2) f = RMs[(m, n)][int(np.floor(F_Qs.shape[0] / 2) + S1_y):min(int(np.floor(F_Qs.shape[0] / 2) + E1_y),RMs[(m, n)].shape[0]),int(np.floor(F_Qs.shape[1] / 2) + S1_x):min(int(np.floor(F_Qs.shape[1] / 2) + E1_x),RMs[(m, n)].shape[1])] f_max[x, y] = np.max(f) if f_max[x, y] &lt; 0.2: flags[x, y] = 0 f = None RM3 = stage3forMultiscale(RMs[len(sc)][n], qs, 0.9) RM3s[:, :, n] = cv2.resize(RM3, (f_t.shape[1], f_t.shape[0]), interpolation=cv2.INTER_NEAREST) E_RM1 = np.max(RM3s, axis=2) s_ind = np.argmax(RM3s, axis=2) E_RM2, RM3, dayin1 = FinalStage3(k1, E_RM1, Q[:F_Q.shape[0], :F_Q.shape[1]],T[:F_T.shape[0], :F_T.shape[1]], alpha, s_ind, SC_t, 1, target) dayin1 = str(dayin1) with open(&#x27;save_data.txt&#x27;, &#x27;a&#x27;) as fid: fid.write(dayin1) fid.write(&#x27; &#x27;) # 换行 elapsed_time = time.time() - start_time print(f&quot;Elapsed time: &#123;elapsed_time&#125; seconds&quot;) # 将 E_RM2 调整为与 target 的大小相同 e_rm2_resized = cv2.resize(E_RM2, (target.shape[1], target.shape[0])) # 将 target 的第一通道转换为浮点值 target_channel = target[..., 0].astype(float) # 创建一个 1x2 的子图布局，第一个图为 &#x27;E_RM2_resized&#x27;，第二个图为 &#x27;target_channel&#x27; fig, axs = plt.subplots(1, 2, figsize=(10, 5)) # 显示 E_RM2_resized axs[0].imshow(e_rm2_resized, cmap=&#x27;jet&#x27;) axs[0].set_title(&#x27;E_RM2_resized&#x27;) # 显示 target_channel cax = axs[1].imshow(target_channel, cmap=&#x27;jet&#x27;) axs[1].set_title(&#x27;target_channel&#x27;) # 绘制颜色栏 fig.colorbar(cax) # 显示图像 plt.show() 总而言之，该函数通过多尺度搜索和相似度计算，找到图像中与目标物体最匹配的位置，并在图像中显示结果。 整体流程12query = cv2.imread(&#x27;./31-30_white.png&#x27;,0)target = cv2.imread(&#x27;./3_white.jpg&#x27;,0) query是目标图像，target是被检测图像，即我们寻找的与之比较的图像。 12torch.cuda.set_device(0) #use GPU 1device = torch.device(&quot;cuda&quot;) 使用GPU加速。 12P,F_T,rows_t,cols_t=Sparse_train(query,8,8)np.savez(&quot;P_16.npz&quot;, basis1 = P.cpu().numpy()) 字典训练和降维。 123data1 = np.load(&#x27;P_16.npz&#x27;)P_1 = data1[&#x27;basis1&#x27;]P = torch.from_numpy(P_1).cuda() 加载学习到的字典，将文件中的数据转化为张量。 12F_Q,rows_q,cols_q=Sparse1(query,P,basis1,16,16)F_T,rows_t,cols_t=Sparse1(target,P,basis1,16,16) 进行稀疏编码和特征降维。 1block,flag,S,E = proto_object(T,blocksize) 图像分块。 1MultiScaleSearch_CoarseToFine(1,Q,T,F_Q,F_T,S,E,flag,0.99,target,blocksize) 滑窗检测，对目标画框。"},{"title":"卷积神经网络进行MNIST手写数字分类","path":"/2023/09/26/卷积神经网络进行MNIST手写数字分类/","content":"环境1234import torchimport torch.nn as nnimport torch.optim as optimfrom torchvision import datasets, transforms torch是PyTorch的核心库，提供了张量操作和其他基本功能。 torch.nn模块包含了构建神经网络模型所需的类和函数。 torch.optim模块包含了各种优化算法的实现，用于调整模型的参数以最小化损失函数。 torchvision.datasets模块提供了用于加载常见视觉数据集的功能。 torchvision.transforms模块包含了对图像进行预处理和数据增强的函数。 数据加载及预处理可重复性12# 设置随机种子torch.manual_seed(42) 参数初始化：在创建神经网络模型时，通常会对模型的参数进行随机初始化。设置了随机种子后，每次运行时参数的初始值将是相同的。 数据洗牌：在训练数据集中，常常会对数据进行洗牌操作，以打破数据的顺序性，提高模型的泛化能力。设置了随机种子后，每次运行时数据洗牌的结果将是相同的。 数据集转换数据转换是数据预处理的一个重要步骤，通常在模型训练之前应用。 标准化和归一化：通过标准化和归一化操作，可以将数据转换为具有零均值和单位方差的分布，以便更好地满足模型的假设。这有助于提高模型的稳定性、加快训练速度，并使得不同特征的权重更加平衡。 数据增强：数据增强是通过应用一系列随机变换来扩充训练数据集的大小。这样做可以增加数据的多样性，减轻模型对训练数据的依赖，提高模型的泛化能力。常见的数据增强操作包括随机旋转、平移、缩放、镜像翻转等。 图像预处理：对图像数据进行预处理操作，例如裁剪、调整大小、亮度和对比度增强等，有助于去除图像中的噪声、无关信息或改善图像质量，使得模型更容易学习到有效的特征。 数据类型转换：有时候需要将数据从一种类型转换为另一种类型，以满足模型的输入要求。例如，将图像数据转换为张量形式，将文本数据转换为数值向量表示。 12345# 定义训练和测试数据的转换transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) transforms.Compose是一个用于组合多个数据转换操作的类。通过将一系列转换操作传递给transforms.Compose，可以按顺序应用这些操作。 transforms.ToTensor()将图像数据转换为PyTorch的张量格式。它将图像数据从范围[0, 255]归一化到范围[0, 1]之间，并将其转换为张量形式。 transforms.Normalize((0.1307,), (0.3081,))对图像数据进行标准化操作。这里传递了两个参数，即均值和标准差。这些值是预先计算得到的MNIST数据集的均值和标准差。标准化操作将图像数据减去均值并除以标准差，以使数据分布更接近于零均值和单位方差。 加载使用torchvision.datasets.MNIST类来加载MNIST数据集。 123# 加载MNIST数据集train_dataset = datasets.MNIST(root=&#x27;./data&#x27;, train=True, download=True, transform=transform)test_dataset = datasets.MNIST(root=&#x27;./data&#x27;, train=False, download=True, transform=transform) root指定了数据集的存储路径，这里将数据集存储在名为”data”的文件夹中。如果该文件夹不存在，数据集将被下载并存储在该位置。 train表示加载训练集。设置为True时，将加载MNIST训练集，其中包含用于模型训练的图像和标签数据。 transform指定了对加载的图像数据进行的转换操作。在上述代码中，我们之前定义的transform对象被传递给transform参数，以便对加载的图像数据进行转换。 download表示如果数据集文件不存在，则自动从网络下载MNIST数据集。如果数据集文件已经存在，则不会重新下载。 将MNIST数据集加载到训练集和测试集的train_dataset和test_dataset对象中，这些对象包含了经过转换的图像和标签数据. 123# 定义数据加载器train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False) 数据加载器（DataLoader）用于批量加载和处理数据集。 train_dataset是之前加载的训练集数据对象，其中包含了经过转换的训练图像和标签数据。 test_dataset是之前加载的测试集数据对象，其中包含了经过转换的测试图像和标签数据。 batch_size指定了每个批次中的样本数量。数据加载器将数据集分成多个批次，并每次返回一个批次的数据进行训练或测试。 shuffle True表示在每个训练迭代中对数据进行洗牌操作，以打乱样本的顺序。这有助于增加数据的随机性和训练的多样性，在模型训练中更好地进行参数更新。False表示在测试过程中不对数据进行洗牌操作，以确保测试结果的一致性和可比性。 定义CNN模型123456789101112131415161718# 定义一个使用普通CNN的模型class CNNNet(nn.Module): def __init__(self): super(CNNNet, self).__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) self.fc = nn.Linear(32 * 7 * 7, 10) def forward(self, x): x = self.conv1(x) x = nn.ReLU()(x) x = nn.MaxPool2d(kernel_size=2)(x) x = self.conv2(x) x = nn.ReLU()(x) x = nn.MaxPool2d(kernel_size=2)(x) x = x.view(x.size(0), -1) x = self.fc(x) return x self.conv1是第一个卷积层，它接受单通道的输入（in_channels&#x3D;1），输出通道数为16（out_channels&#x3D;16），卷积核的大小为3x3（kernel_size&#x3D;3），步幅为1（stride&#x3D;1），填充为1（padding&#x3D;1）。 self.conv2是第二个卷积层，它接受16通道的输入（in_channels&#x3D;16），输出通道数为32（out_channels&#x3D;32），卷积核的大小为3x3（kernel_size&#x3D;3），步幅为1（stride&#x3D;1），填充为1（padding&#x3D;1）。 self.fc是全连接层，它接受展平后的特征作为输入，输入特征数量为32x7x7（32 * 7 * 7），输出特征数量为10，用于分类任务。 nn.ReLU()一个 ReLU 激活函数对象，通过调用 nn.ReLU() 将 x 作为输入，对x进行非线性激活。 nn.MaxPool2d减少特征。 x.view(x.size(0), -1)将特征图展平为一维向量，其中 x.size(0) 表示批次大小。 在forward方法中，定义了模型的前向传播过程。输入数据x经过卷积、ReLU激活函数、最大池化等操作，然后通过展平操作将特征展平为一维向量。最后，通过全连接层self.fc将特征映射到10个类别的输出。这个模型的结构包括两个卷积层和一个全连接层，通过卷积层提取图像的特征，然后通过全连接层进行分类。 训练模型创建实例123456# 创建一个CNNNet实例model = CNNNet()# 定义损失函数和优化器criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=0.001) nn.CrossEntropyLoss()是用于多类别分类问题的交叉熵损失函数。optim.Adam是Adam优化算法的实现，用于调整模型参数以最小化损失函数。model.parameters()用于传递模型的可学习参数给优化器，以便进行参数更新。lr&#x3D;0.001表示学习率为0.001，即每次参数更新的步长大小。 训练12345678910111213141516# 训练模型def train(model, train_loader, criterion, optimizer, epochs): model.train() for epoch in range(epochs): running_loss = 0.0 for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() running_loss += loss.item() if batch_idx % 100 == 99: print(&#x27;[Epoch: %d, Batch: %5d] Loss: %.3f&#x27; % (epoch+1, batch_idx+1, running_loss/100)) running_loss = 0.0 参数： model：要训练的CNN模型。 train_loader：用于训练的数据加载器，它会提供训练数据的批次。 criterion：损失函数，用于计算训练过程中的损失。 optimizer：优化器，用于更新模型的参数。 epochs：训练的轮数。 使用一个外部循环来迭代指定的epochs次数。在每个epoch中，通过一个内部循环遍历训练数据加载器的批次。对于每个批次，首先将优化器的梯度缓存清零，然后，通过调用model(data)进行前向传播，得到模型的输出。接着，计算输出与目标标签之间的损失，使用预先定义的损失函数criterion。通过调用loss.backward()进行反向传播，计算损失关于模型参数的梯度。接着，调用optimizer.step()执行一步优化器，即根据梯度更新模型的参数。在训练过程中，累加每个批次的损失值到running_loss变量中。 12# 在训练集上训练模型train(model, train_loader, criterion, optimizer, epochs=5) 模型评估12345678910111213# 在测试集上评估模型性能def test(model, test_loader): model.eval() correct = 0 total = 0 with torch.no_grad(): for data, target in test_loader: output = model(data) _, predicted = torch.max(output.data, 1) total += target.size(0) correct += (predicted == target).sum().item() accuracy = 100 * correct / total print(&#x27;Accuracy on test set: %.2f%%&#x27; % accuracy) 参数： model：要评估的CNN模型。 test_loader：用于测试的数据加载器，它会提供测试数据的批次。 通过遍历测试数据加载器的批次，进行模型的前向传播。对于每个批次，获取模型的输出，并使用torch.max()函数找到每个样本预测的最大值及其索引。接着，累加正确预测的样本数到correct变量中，并累加测试样本的总数到total变量中。最后，计算准确率（Accuracy）作为正确预测的样本数除以测试样本的总数。 12# 在测试集上测试模型性能test(model, test_loader)"},{"title":"图像处理概念","path":"/2023/09/11/图像处理概念/","content":"图像处理图像处理指输入和输出都是图像的处理，包括从图像中提取特征等。 与计算机视觉图像处理水平更低，主要对图像进行处理并且输出，计算机视觉则从输入的图像或视频进行理解，达到模拟人类视觉的目的。图像处理使用更为原始的方法，计算机视觉则是使用高集成度的算法。"},{"title":"集成学习","path":"/2023/09/02/集成学习/","content":"1 数据说明与导入本案例使用的数据集为台湾地区信用卡客户数据，具体数据字段的表格信息记录如下: 列名 含义说明 数据类型 ID 客户ID。 int64 LIMIT_BAL 银行给予客户的信用额度，包括个人信用额度和客户的家庭信用额度。 float64 SEX 客户的性别。男性记为1，女性记为2。 int64 EDUCATION 客户的教育水平。研究生及以上记为1，大学记为2，高中记为3，其它记为0。 int64 MARRIAGE 客户的婚姻状况。未婚记为2，已婚记为1，其它记为0。 int64 AGE 客户的年龄。 int64 PAY_1～PAY_6 这六个变量是2005年4月到9月每月的还款记录，取值为-1～9，代表累计逾期月数。 int64 BILL_AMT1～BILL_AMT6 这六个变量是2005年4月到9月每月的账单记录，即每月用信用卡消费记录。 float64 PAY_AMT1～PAY_AMT6 这六个变量是2005年4月到9月每月的支付记录，包括还账单金额和存入信用卡的金额。 float64 default.payment.next.month 代表客户下个月是否违约，违约记为1，未违约则记为0。 int64 123456789101112131415import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport warnings%matplotlib inlineimport sklearnfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier from sklearn import metrics from sklearn.metrics import precision_recall_curvefrom sklearn.metrics import confusion_matrixfrom sklearn import preprocessingwarnings.filterwarnings(&quot;ignore&quot;) 1df = pd.read_csv(&#x27;./dataset/data.csv&#x27;) 2 数据初步探索2.1 数据信息检查1df.____ 1df.shape() 1df.info() 1df.describe() 2.2 数据清洗首先对教育程度的字段用value_counts进行查看：它可以查看字段中有多少个不同值，并计算每个不同值有在该列中有多少重复值。 1df.describe() 1df[&#x27;EDUCATION&#x27;].________ 我们使用replace完成上述替换，inplace参数设置为True表示替换原数据。 12df[&#x27;EDUCATION&#x27;].______([1,3,4,5,6],[3,1,0,0,0],inplace=True)df[&#x27;EDUCATION&#x27;].______ 12df[&#x27;SEX&#x27;].replace([1,2],______,inplace=True)df[&#x27;SEX&#x27;].value_counts() 1df[&#x27;MARRIAGE&#x27;].value_counts() 12df[&#x27;MARRIAGE&#x27;].replace(3,0,inplace=True)df[&#x27;MARRIAGE&#x27;].value_counts() 1df[&#x27;PAY_1&#x27;].value_counts() 12df.iloc[:,[6,7,8,9,10,11]] = df.iloc[:,[6,7,8,9,10,11]].replace(______)df.iloc[:,[6,7,8,9,10,11]].describe() 1df[&#x27;default.payment.next.month&#x27;].value_counts()___/df[&#x27;default.payment.next.month&#x27;].value_counts()____ 2.3 数据可视化与分析2.3.1 数据分布情况首先对于对于连续型变量LIMIT_BAL，使用直方图查看数据分布状况。 12345678910sns.set(rc=&#123;&#x27;figure.figsize&#x27;:(10,5),&quot;font.size&quot;:15,&#x27;font.sans-serif&#x27;:[&#x27;SimHei&#x27;],&quot;axes.titlesize&quot;:15,&quot;axes.labelsize&quot;:15&#125;)plt.title(&#x27;信用额度分布图&#x27;)# 改变 matplotlib 颜色缩写词的解释方式sns.set_color_codes(&quot;pastel&quot;)# kde：是否绘制高斯核密度估计图 bins：直方图bins(柱)的数目sns._____(df[&#x27;LIMIT_BAL&#x27;],_____,bins=200, color=&quot;blue&quot;)plt.xlabel(&quot;信用额度&quot;)plt.rcParams[&#x27;figure.dpi&#x27;] = 1200 #为了使图片更加清晰plt.show() 1234567sns.set(rc=&#123;&#x27;figure.figsize&#x27;:(10,5),&quot;font.size&quot;:15,&#x27;font.sans-serif&#x27;:[&#x27;SimHei&#x27;],&quot;axes.titlesize&quot;:15,&quot;axes.labelsize&quot;:15&#125;)plt.title(&#x27;客户年龄分布图&#x27;)sns.set_color_codes(&quot;pastel&quot;)sns._____(_______,kde=True,bins=59, color=&quot;green&quot;)plt.xlabel(&quot;年龄&quot;)plt.show() 12345678910111213141516171819202122232425# 绘制多个子图，1行3列fig = plt._______(______,figsize=(16,12))sns.set(rc=&#123;&#x27;figure.figsize&#x27;:(10,5),&quot;font.size&quot;:15,&#x27;font.sans-serif&#x27;:[&#x27;SimHei&#x27;],&quot;axes.titlesize&quot;:15,&quot;axes.labelsize&quot;:15&#125;)# 绘制第一个子图plt._______sex_count = df[&#x27;SEX&#x27;].value_counts()# 画饼图，设置扇形标签，设置百分比显示格式，一位小数百分比sex_count.plot(kind=_____, labels=________, autopct=&#x27;%1.1f%%&#x27;)plt.ylabel(&quot;&quot;)plt.title(&#x27;性别分布图&#x27;)plt.subplot(1,3,2)edu_count = df[&#x27;EDUCATION&#x27;].value_counts()edu_count.plot(kind=&#x27;pie&#x27;, labels=[&#x27;本科&#x27;, &#x27;研究生或以上&#x27;,&#x27;高中&#x27;,&#x27;其他&#x27;], autopct=&#x27;%1.1f%%&#x27;)plt.title(&#x27;教育情况分布图&#x27;)plt.ylabel(&quot;&quot;)plt.subplot(1,3,3)mar_count = df[&#x27;MARRIAGE&#x27;].value_counts()mar_count.plot(kind=&#x27;pie&#x27;, labels=[&#x27;未婚&#x27;, &#x27;已婚&#x27;,&#x27;其他&#x27;], autopct=&#x27;%1.1f%%&#x27;)plt.title(&#x27;婚姻状况分布图&#x27;)plt.ylabel(&quot;&quot;)plt.show() 2.3.2 结合违约情况查看数据分布1234567891011121314151617181920212223242526272829303132sns.set(rc=&#123;&#x27;figure.figsize&#x27;:(13,5),&quot;font.size&quot;:15,&#x27;font.sans-serif&#x27;:[&#x27;SimHei&#x27;],&quot;axes.titlesize&quot;:15,&quot;axes.labelsize&quot;:15&#125;)plt.figure()# fig代表绘图窗口(Figure)；ax代表这个绘图窗口上的坐标系(axis)fig, ax = plt.subplots(1,3,figsize=(12,5))plt.subplot(1,3,1)# 条形图，以x轴标签划分统计个数，再以hue标签统计违约与未违约的个数ax=sns._______(x=_______, ______=&#x27;default.payment.next.month&#x27;,data = df)# 设置x轴不同标签名称ax._________([&#x27;男&#x27;, &#x27;女&#x27;],fontsize=12)# 设置x轴名称plt.xlabel(&quot;性别&quot;)plt.ylabel(&quot;&quot;)# 设置图的标题plt.title(&#x27;不同性别违约情况&#x27;)plt.subplot(1,3,2)ax=sns.countplot(x=&#x27;EDUCATION&#x27;,hue=&#x27;default.payment.next.month&#x27;,data = df)ax.set_xticklabels([&#x27;其他&#x27;, &#x27;高中&#x27;,&#x27;本科&#x27;,&#x27;研究生及以上&#x27;],fontsize=12)plt.xlabel(&quot;教育程度&quot;)plt.ylabel(&quot;&quot;)plt.title(&#x27;不同教育程度违约情况&#x27;)plt.subplot(1,3,3)ax=sns.countplot(x=&#x27;MARRIAGE&#x27;,hue=&#x27;default.payment.next.month&#x27;,data = df)ax.set_xticklabels([&#x27;其他&#x27;, &#x27;已婚&#x27;,&#x27;未婚&#x27;], fontsize=12)plt.xlabel(&quot;婚姻状况&quot;)plt.ylabel(&quot;&quot;)plt.title(&#x27;不同婚姻状况违约情况&#x27;)plt.show() 1234567891011121314151617181920212223242526t0 = df[df[&#x27;default.payment.next.month&#x27;] == 0] # 未违约的样本数据t1 = df[df[&#x27;default.payment.next.month&#x27;] == 1] # 违约的样本数据plt.figure()fig, ax = plt.subplots(1,2,figsize=(12,6))# 画第一个子图plt.subplot(1,2,1)# 画直方图，设置高斯核密度估计图参数，以字典传入核密度图的属性信息sns._______(______[&#x27;AGE&#x27;],_______=&#123;&quot;color&quot;:&quot;blue&quot;,&quot;label&quot;:&quot;未违约&quot;&#125;)sns._______(______[&#x27;AGE&#x27;],_______=&#123;&quot;color&quot;:&quot;orange&quot;,&quot;label&quot;:&quot;违约&quot;&#125;)# 设置x轴名称、字符大小plt.xlabel(&#x27;年龄&#x27;, fontsize=12)locs, labels = plt.xticks()plt.title(&#x27;不同年龄违约情况&#x27;)# 刻度线参数设置plt.tick_params(axis=&#x27;both&#x27;, which=&#x27;major&#x27;, labelsize=12)plt.subplot(1,2,2)sns.distplot(t0[&quot;LIMIT_BAL&quot;],kde_kws=&#123;&quot;color&quot;:&quot;blue&quot;,&quot;label&quot;:&quot;未违约&quot;&#125;)sns.distplot(t1[&quot;LIMIT_BAL&quot;],kde_kws=&#123;&quot;color&quot;:&quot;orange&quot;,&quot;label&quot;:&quot;违约&quot;&#125;)plt.xlabel(&#x27;信用卡额度&#x27;, fontsize=12)locs, labels = plt.xticks()plt.title(&#x27;不同信用卡额度违约情况&#x27;)plt.tick_params(axis=&#x27;both&#x27;, which=&#x27;major&#x27;, labelsize=12)plt.show() 3 分类建模3.1 KNN模型在KNN模型中，由于要计算欧氏距离，因此应该先对离散变量做哑变量编码处理，使其变为数值型特征我们对无序离散变量SEX,MARRIAGE进行哑变量编码。 123str_columns = [&#x27;SEX&#x27;,&#x27;MARRIAGE&#x27;]df_new = pd._______(df, columns=str_columns)df_new 12y = df_new[&#x27;default.payment.next.month&#x27;] #因变量X = df_new.______(columns=[&#x27;ID&#x27;,&#x27;default.payment.next.month&#x27;]) #去除因变量与ID，剩余为特征 对特征列进行标准化处理，使得处理过后的每一列数据都是均值为0，标准差为1的数据。 12X.iloc[:,0:20] = ___________(X.iloc[:,0:20])X.sample(5) 12print (&#x27;均值为：&#123;a&#125;&#x27;.format(a = X.LIMIT_BAL.mean()))print (&#x27;标准差为：&#123;a&#125;&#x27;.format(a = X.LIMIT_BAL.std())) 对数据进行训练集和测试集的划分。选取测试集大小为0.3，random_state随机种子。 123X_train, X_test,\\y_train, y_test \\= _________(X, y, test_size=______, random_state=0) 使用sklearn中的KNeighborsClassifier对训练集进行分类训练。当不输入参数时，默认KNN所选取的n_neighbors参数取值为5。 1234# 创建knn类knn_model = ___________# 对数据进行拟合学习knn_model._____(X_train, y_train) 得到模型knn_model后，我们在测试集上验证其分类效果用predict方法获取模型对每一个测试集样本标签的预测结果knn_pred用predict_proba方法获取模型把每一个样本划分为正类的概率knn_score 1234# 预测结果knn_pred = knn_model.______(X_test)# 属于正类的概率knn_score = knn_model._______(X_test)[:,1] 为了展示分类模型的效果，我们需要获取模型的准确率得分，分类报告以及混淆矩阵。sklearn中的accuracy_score用于计算准确率sklearn中的classification_report函数用于显示主要分类指标的文本报告．在报告中显示每个类的精确度，召回率，F1值等信息sklearn中的confusion_matrix总结模型真实值与预测值的情形分析表，以矩阵形式显示 为了方便后续使用，我们定义一个函数Get_report来获取这些信息。 12345678910111213def Get_report(testers , predictors): print (&#x27;模型的准确率为：&#123;a&#125;&#x27;.format(a=metrics._________(testers, predictors))) print (&#x27;模型的分类报告展示如下:&#x27;) print (metrics._________(testers, predictors)) print (&#x27;模型的混淆矩阵展示如下:&#x27;) plt.figure(figsize=(8,4)) ConfMatrix = __________(testers, predictors) sns.heatmap(ConfMatrix,annot=True, cmap=&quot;Blues&quot;, fmt=&quot;d&quot;, xticklabels = [&#x27;未违约&#x27;, &#x27;违约&#x27;], yticklabels = [&#x27;未违约&#x27;, &#x27;违约&#x27;]) plt.ylabel(&#x27;真实标签&#x27;) plt.xlabel(&#x27;预测标签&#x27;) plt.title(&quot;混淆矩阵&quot;) 1Get_report(y_test , knn_pred) 总体预测的精准度尚可，违约样本的实际召回率只有0.27，能够综合展示正类样本的准确率和召回率的f1-score只有0.36。我们再尝试其他模型进行对比。 3.2 决策树1234from sklearn import treefrom sklearn.tree import DecisionTreeClassifiertree_model = __________(random_state=1)tree_model.______(X_train, y_train) 123tree_score = tree_model.______(X_test)[:,1]tree_pred = tree_model._______(X_test)Get_report(_____,_______) 决策树相比KNN，在召回率和f1-score上有所提升。 4 数据过采样1df[&#x27;default.payment.next.month&#x27;].value_counts()[0]/df[&#x27;default.payment.next.month&#x27;].value_counts()[1] 负类和正类的比例在3.5左右，样本数据不平衡问题会导致二分类问题的难度较高。为了解决这个问题，引入SMOTE过采样算法，改变训练集中正负类别样本的分布。 123456789from imblearn.over_sampling import SMOTE# 划分数据集X_train, X_test,\\y_train, y_test \\= train_test_split(X, y, test_size=0.3, random_state=0)# 定义一个SMOTE模型smo = ________(random_state=42)# fit_resample拟合数据X_smo, y_smo = smo.__________(X_train, y_train) 5 引入新评价指标同时，针对不平衡样本的评价指标也需要进行相应的更新，我们可以引入Precision_Recall_curve、AP值、ROC_curve、G-mean，我们定一个Get_curve函数，展示这些曲线，并给出一系列对应的数值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def Get_curve (tester , scorer , predictionvalues): # PR曲线 precision, recall, thresholds = ___________(tester , scorer) # roc曲线 fpr,tpr,thresholds1 = metrics._________(tester , scorer) print(precision) print(recall) f1 = [] gmean = [] # 计算F1：2PR/P+R for i in range(len(precision)): f1.append(2*(precision[i]*recall[i])/(precision[i]+recall[i])) # 计算gmean:(tpr*(1-fpr))^1/2 for i in range(len(fpr)): gmean.append(np._____(tpr[i]*(1-fpr[i]))) plt.figure() fig, ax = plt.subplots(2,2,figsize=(12,3)) # 子图1 PR曲线 plt.subplot(1,3,1) # 横轴召回率，纵轴准确率 plt.plot(_______,_______) plt.title(&#x27;精准率-召回率曲线&#x27;) plt.xlabel(&#x27;召回率&#x27;) plt.ylabel(&#x27;精准率&#x27;) # 设置x轴作图范围 plt.xlim(0,1) # 子图2，roc曲线 plt.subplot(1,3,2) # 横轴假正率，纵轴真正率 plt.plot(_______,_______) plt.title(&#x27;ROC曲线&#x27;) plt.xlabel(&#x27;FPR&#x27;) plt.ylabel(&#x27;TPR&#x27;) # 子图3，gmean曲线 plt.subplot(1,3,3) # gmean根据阈值变化的曲线 plt.plot(________,_______) plt.title(&#x27;Gmean曲线&#x27;) plt.xlabel(&#x27;阈值&#x27;) plt.ylabel(&#x27;gmean值&#x27;) plt.xlim(0,1) plt.show() # 计算AP值 apscore = sklearn.metrics.__________(tester, scorer, average=&#x27;macro&#x27;, sample_weight=None) print(&quot;AP值:&#123;name&#125;&quot;.format(name=apscore)) # Gmean最大时的阈值 best_threshold = thresholds1[_______] # F1最大时的阈值 best_thresholds = thresholds[_______] print(&#x27;F1最大对应阈值 = &#123;a&#125;&#x27;.format(a = best_thresholds)) print(&#x27;Gmean最大对应阈值 = &#123;a&#125;&#x27;.format(a = best_threshold)) print(&#x27;准确率得分= &#123;a&#125;&#x27;.format(a = metrics.accuracy_score(tester, predictionvalues))) print(&#x27;最大Gmean = &#123;a&#125;&#x27;.format(a = max(gmean))) print(&#x27;最大F1 = &#123;a&#125;&#x27;.format(a = max(f1))) 我们传入上文中利用决策树模型得到的参数y_test,tree_score，tree_pred来测试这个函数的效果。 1Get_curve(y_test,tree_score,tree_pred) 6 集成学习6.1 XGBoost12345678# 划分数据集X_train, X_test,\\y_train, y_test \\= train_test_split(X, y, test_size=0.3, random_state=0)# 过采样处理smo = SMOTE(random_state=42)X_smo, y_smo = smo.fit_resample(X_train, y_train) 12345from xgboost import XGBClassifiermodel = _________(booster=&#x27;gbtree&#x27;,learning_rate=0.1,gamma=1,scale_pos_weight=1,n_estimators=1000,max_depth=6,alpha=5,reg_lambda=1)# 设置训练数据eval_set = [(X_test, y_test)]model._____(X_smo, y_smo, early_stopping_rounds=10, eval_metric=&quot;auc&quot;, eval_set=eval_set, verbose=False) learning_rate参数代表学习率，决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。alpha和reg_lambda分别对应L1，L2正则项系数。 12345from xgboost import XGBClassifiermodel = _________(booster=&#x27;gbtree&#x27;,learning_rate=0.1,gamma=1,scale_pos_weight=1,n_estimators=1000,max_depth=6,alpha=5,reg_lambda=1)# 设置训练数据eval_set = [(X_test, y_test)]model._____(X_smo, y_smo, early_stopping_rounds=10, eval_metric=&quot;auc&quot;, eval_set=eval_set, verbose=False) 我们调用Get_curve函数考察最终的结果。 1Get_curve(y_test,y_socre,y_pred) 6.2 CATBoost12345y = df[&#x27;default.payment.next.month&#x27;] # 因变量X = df.drop(columns=[&#x27;ID&#x27;,&#x27;default.payment.next.month&#x27;]) # 去除ID与因变量，剩余为特征X_train, X_test,\\y_train, y_test \\= train_test_split(X, y, test_size=0.3, random_state=0) 123456from sklearn.model_selection import GridSearchCVfrom catboost import CatBoostClassifiercategorical_features_indices = [&#x27;SEX&#x27;,&#x27;MARRIAGE&#x27;]model = ___________(iterations=100, depth=6, cat_features = categorical_features_indices, loss_function=&#x27;Logloss&#x27;, logging_level=&#x27;Silent&#x27;)model.fit(X_train,y_train,eval_set=(X_test, y_test)) 获取CatBoost模型的预测值以及预测为正类的概率。 123cb_pred = model.predict(X_test)cb_socre = model.predict_proba(X_test)[:,1]# predictions = [round(value) for value in cb_pred] 我们再获取使用CatBoost得到的评价指标，与之前的情况做对比。 1Get_curve(y_test, cb_socre, cb_pred) 1234THRESHOLD = __________# 大于阈值输出1，小于则输出0y_pred = _______(model.predict_proba(X_test)[:,1] &gt;= THRESHOLD, 1, 0)Get_report(y_test,y_pred) 123THRESHOLD = _________y_pred = np.where(model.predict_proba(X_test)[:,1] &gt;= THRESHOLD, 1, 0)Get_report(y_test,y_pred)"},{"title":"基于健康学习的机器诊断","path":"/2023/09/02/基于健康学习的机器诊断/","content":"1 数据读取与预处理1.1 数据集简介肝病患数据集包含416名肝病患者记录和167非肝病患者记录，数据集信息如下： 列名 数据类型 含义说明 Age Integer 患者的年龄 Gender String 患者的性别 TB Float 总胆红素 DB Float 直接胆红素 Alkphos Integer 碱性磷酸酶 Sgpt Integer 谷丙转氨酶 Sgot Integer 天冬氨酸氨基转移酶 TP Float 总蛋白 ALB Float 白蛋白 A&#x2F;G Ratio Float 白蛋白和球蛋白比率 label Integer 是否患病(1为患病，2为健康) 1.2 导入数据123456import pandas as pdimport numpy as np# 不显示警告import warningswarnings.filterwarnings(&#x27;ignore&#x27;) 查看数据集 12data = pd.read_csv(&quot;./dataset/Indian Liver Patient Dataset (ILPD).csv&quot;)data.head() 12# 查看数据信息data.info() Gender为唯一的object特征，我们需要提前处理为数值型，方便建立模型。使用.describe()对总体数据进行统计，查看常见的数据的统计学特征。 1data.describe(include=&#x27;all&#x27;) 1.3 数字编码对Gender进行数字编码。 12345678from sklearn.preprocessing import LabelEncoder# 数字编码le = LabelEncoder()# 数字编码data[&quot;Gender&quot;] = le.fit_transform(data[&quot;Gender&quot;][:])data 女性(Female)被编码为0，男性(Male)被编码为1。 1.4 缺失值填补A&#x2F;G Ratio存在缺失值，我们需要对其进行填补。在这里我们采用均值填补的方法进行填充。 1234567from sklearn.impute import SimpleImputer# 采用均值填补法imp = SimpleImputer(strategy=&quot;mean&quot;)data[&quot;A/G Ratio&quot;] = imp.fit_transform(data[&quot;A/G Ratio&quot;].to_frame())data.describe() 1.5 数据列数值转化本数据集的标签列label的值为 {1,2}，将其转化为{−1,1}或者{0,1}。 1234567891011# 定义label列calculate_col = &quot;label&quot;calculate_value = 2# 将label值从&#123;1，2&#125;，转化为&#123;1，0&#125;并更名为label_caldata[calculate_col + &#x27;_cal&#x27;] = calculate_value - data[calculate_col]# 删除label列data = data.drop(labels=&#x27;label&#x27;, axis=1)#查看转化后的数据集data 2 数据相关性探索2.1 年龄分布的直方图、性别数量的柱状图12345678910111213141516171819import matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline%config InlineBackend.figure_format = &#x27;svg&#x27;plt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;] # 用来正常显示中文标签plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False # 用来正常显示负号# 定义genders列genders = [&#x27;女性&#x27;, &#x27;男性&#x27;]fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 5))# 画出年龄分布直方图sns.histplot(data[&#x27;Age&#x27;], kde=False, palette=&quot;Set2&quot;, ax=ax1)# 画出性别分布柱状图sns.countplot(x=&#x27;Gender&#x27;, data=data, palette=&quot;Set2&quot;, ax=ax2)ax1.set(xlabel=&#x27;年龄&#x27;, ylabel=&#x27;数量&#x27;)ax2.set(xlabel=&#x27;性别&#x27;, ylabel=&#x27;数量&#x27;)ax2.set_xticklabels(genders) 2.2 年龄、性别与患病的分布情况12345678910111213141516171819202122# 定义两个图像fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 5))# 绘制年龄箱线图sns.boxplot(x=&#x27;label_cal&#x27;, y=&#x27;Age&#x27;, hue=&#x27;label_cal&#x27;, data=data, palette=&quot;Set2&quot;, ax=ax1)# 绘制性别柱状图sns.countplot(x=&#x27;Gender&#x27;, hue=&#x27;label_cal&#x27;, data=data, palette=&quot;Set2&quot;, ax=ax2)# 给图像加标题ax1.set_title(&#x27;年龄与患病情况的分布&#x27;, fontsize=13)ax2.set_title(&#x27;性别与患病情况的分布&#x27;, fontsize=13)# 给图像添加标签labels = [&#x27;不患病&#x27;, &#x27;患病&#x27;]ax1.set(xlabel=&#x27;是否患病&#x27;, ylabel=&#x27;年龄&#x27;)ax1.set_xticklabels(labels)ax1.legend_.remove()ax2.set(xlabel=&#x27;性别&#x27;, ylabel=&#x27;数量&#x27;)ax2.set_xticklabels(genders)ax2.legend([&#x27;不患病&#x27;, &#x27;患病&#x27;])plt.show() 2.3 各指标间相关性1234567891011121314151617181920def correlation_heatmap(df): hm, ax = plt.subplots(figsize=(12, 8))# 绘制热力图 hm = sns.heatmap( df.corr(), cmap=&#x27;Blues&#x27;, square=True, cbar_kws=&#123;&#x27;shrink&#x27;: .9&#125;, ax=ax, annot=True, linewidths=0.1, vmax=1.0, linecolor=&#x27;white&#x27;, annot_kws=&#123;&#x27;fontsize&#x27;: 12&#125; ) plt.title(&#x27;连续型对象间的皮尔森相关系数&#x27;, y=1.05, size=15)# 删掉Gender与label_cal列df = data.drop([&#x27;Gender&#x27;, &#x27;label_cal&#x27;], axis=1)correlation_heatmap(df) 针对两个离散型变量Gender和label_cal，我们使用卡方检验来观测其相关性，使用的方法是sklearn.feature_selection中的chi2方法： 1234567891011121314from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2# 定义两个新的数据框gender = pd.DataFrame(data[&#x27;Gender&#x27;])label_cal = pd.DataFrame(data[&#x27;label_cal&#x27;])# 使用卡方检验model1 = SelectKBest(chi2, k=1) # 选择k个最佳特征model1.fit_transform(gender, label_cal)# 打印得分print(&#x27;性别变量与是否得病之间的得分为：%.4f&#x27; % model1.scores_)print(&#x27;性别变量与是否得病之间的pvalue为：%.4f&#x27; % model1.pvalues_) 计算label_cal与其他连续型变量之间的关系，使用的方法为sklearn.feature_selection中的f_classif方法： 123456789101112131415from sklearn.feature_selection import f_classiffdata = pd.DataFrame(data.drop([&#x27;Gender&#x27;, &#x27;label_cal&#x27;], axis=1))# 删除gender列与label_cal列label_cal = pd.DataFrame(data[&#x27;label_cal&#x27;])# 使用f_classif计算label_cal与其他连续型变量间的关系F, p_val = f_classif(fdata, label_cal)# f分布的0.05分位数print(&#x27;各连续型变量的名称：&#x27;)print(fdata.columns.tolist())print(&#x27;各连续型变量与是否得病之间的F值为：&#x27;)print(F)print(&#x27;各连续型变量与是否得病之间的pvalue为：&#x27;)print(p_val) 从计算得出的P值能看出，除了TP的值为0.398，大于0.05以外，各个连续型变量与label_cal之间具有很高的的相关性。 3 构建分类模型3.1 数据集划分我们根据计算出的label_cal来对数据集进行划分。划分比例设置为测试集:训练集 = 20%:80%。 123456from sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_reportfrom sklearn.metrics import confusion_matrixfrom sklearn.metrics import accuracy_scorex = data.drop([&#x27;label_cal&#x27;], axis=1) # x为删除label_cal的数据集y = data[&#x27;label_cal&#x27;] # y为label_cal列 12# 划分数据集x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2, stratify=data[&#x27;label_cal&#x27;]) 3.2 逻辑回归逻辑回归是一种广义线性回归模型，在回归的基础上，使用Logistic函数将连续型的输出映射到（0，1）之间，用来解决分类问题。 在Python中，使用sklearn_model的LogisticRegression进行分类建模，使用的主要参数有： penalty ——可设为l1或者l2，代表L1和L2正则化，默认为l2。 class_weight ——class-weight用于指定样本各个类别的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。 random_state ——随机种子，设定为一个常数，保证每次运行的结果都是一样的。 我们使用这几个参数进行模型的构建，其中，class_weight为我们自己赋予的权重： 123456789101112from sklearn.linear_model import LogisticRegression# 为权重赋值weights = &#123;0: 1, 1: 1.3&#125;# 进行logistic回归lr = LogisticRegression(penalty=&#x27;l2&#x27;, random_state=8, class_weight=weights)lr.fit(x_train, y_train)# 对y进行预测y_predprb = lr.predict_proba(x_test)[:, 1]y_pred = lr.predict(x_test) 1234567891011121314from sklearn import metricsfrom sklearn.metrics import auc# 计算fpr，tpr及thresholds的值fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predprb)# 计算gmean的值gmean = np.sqrt(tpr*(1-fpr))# 计算最大的gmean值对应的thresholds值dictionary = dict(zip(thresholds,gmean))max_thresholds = max(dictionary, key=dictionary.get)print(&quot;最大的GMean值为：%.4f&quot;%(max(gmean)))print(&quot;最大的GMean对应的thresholds为：%.4f&quot;%(max_thresholds)) 1234from sklearn.metrics import roc_auc_score# 计算AUC值test_roc_auc = roc_auc_score(y_test, y_predprb)print(test_roc_auc) 12# 打印模型分类预测报告print(classification_report(y_test, y_pred)) 12345678# 画出混淆矩阵热力图cm1 = confusion_matrix(y_test, y_pred)plt.figure(figsize=(8, 6))sns.heatmap(cm1, annot=True, linewidths=.5, square=True, cmap=&#x27;Blues&#x27;)plt.ylabel(&#x27;Actual label&#x27;)plt.xlabel(&#x27;Predicted label&#x27;)all_sample_title = &#x27;ROC AUC Score: &#123;0&#125;&#x27;.format(round(test_roc_auc,2))plt.title(all_sample_title, size=15) 患病类别(label_cal=1)的召回率(Recall)达到0.93，且精确度(Precision)达到0.71，总体的平均F1_score达到0.45，是一个分类水平一般的模型。 3.3 决策树使用sklearn中的DecisionTreeClassifier算法来训练决策树模型。使用的主要参数有： max_depth：设置决策树的最大深度。为多次试验后设置的较好值。 class_weight：用于指定样本各个类别的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。设置为balanced时，能够保证样本量少的类别权重较高。 random_state ——随机种子，设定为一个常数，保证每次运行的结果都是一样的。 12345678910from sklearn.tree import DecisionTreeClassifierfrom sklearn import tree# 建立决策树模型model = DecisionTreeClassifier(random_state=5, class_weight=weights)model = model.fit(x_train, y_train)# 对y进行预测y_predict = model.predict(x_test)y_predprb = model.predict_proba(x_test)[:, 1] 123# 计算AUC值test_roc_auc = roc_auc_score(y_test, y_predprb)print(test_roc_auc) 12# 打印模型分类预测报告print(classification_report(y_test, y_predict)) 12345678# 绘制混淆矩阵热力图cm2 = confusion_matrix(y_test, y_predict)plt.figure(figsize=(9, 9))sns.heatmap(cm2, annot=True, linewidths=.5, square=True, cmap=&#x27;Blues&#x27;)plt.ylabel(&#x27;Actual label&#x27;)plt.xlabel(&#x27;Predicted label&#x27;)all_sample_title = &#x27;ROC AUC score: &#123;0&#125;&#x27;.format(round(test_roc_auc,2))plt.title(all_sample_title, size=15) 患病类别(label_cal=1)的召回率(Recall)达到0.71，且精确度(Precision)达到0.77，总体的平均F1_score达到0.58，分类水平一般。 3.4 随机森林在Python中，使用sklearn.ensemble的RandomForestClassifier进行分类建模，使用的主要参数有： n_estimator：训练分类器的数量，默认值为100。 max_depth：每棵树的最大深度，默认为3。 random_state：随机种子，设定为一个常数，保证每次运行的结果都是一样的。 class_weight：用于指定样本各个类别的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。 12345678910from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import cross_val_score# 建立随机森林模型ran_for = RandomForestClassifier(n_estimators=80, random_state=0, class_weight=weights)ran_for.fit(x_train, y_train)# 对y进行预测y_pred_ran = ran_for.predict(x_test)y_predprb = ran_for.predict_proba(x_test)[:, 1] 123# 计算AUC值test_roc_auc = roc_auc_score(y_test, y_predprb)print(test_roc_auc) 12# 打印模型分类预测报告print(classification_report(y_test, y_pred_ran, digits=2)) 12345678# 绘制混淆矩阵热力图cm3 = confusion_matrix(y_test, y_pred_ran)plt.figure(figsize=(9, 9))sns.heatmap(cm3, annot=True, linewidths=.5, square=True, cmap=&#x27;Blues&#x27;)plt.ylabel(&#x27;Actual label&#x27;)plt.xlabel(&#x27;Predicted label&#x27;)all_sample_title = &#x27;ROC AUC score: &#123;0&#125;&#x27;.format(round(test_roc_auc,2))plt.title(all_sample_title, size=15) 患病类别(label_cal=1)的召回率(Recall)达到0.80，且精确度(Precision)达到0.77，总体的平均F1_score达到0.61。 可以得到随机森林模型的分类效果高于逻辑回归与决策树。 3.5 主成分分析PCA降维是一种常见的数据降维方法，其目的是在“信息”损失较小的前提下，将高维的数据转换到低维，从而减小计算量。PCA通常用于高维数据集的探索与可视化，还可以用于数据压缩，数据预处理等。主成分分析必须从相同量纲的变量表格开始。由于需要将变量总方差分配给特征根，因此变量必须有相同的物理单位，方差才有意义（方差的单位是变量单位的平方）。主成分分析的变量也可以是无量纲的数据，例如标准化或对数转化后的数据。因此在构建模型之前，我们需要进行数据标准化。常用的标准化方法有 min-max 标准化和 z-score 标准化等。在本例中，我们直接采用 z-score 标准化方法。 对数据进行标准化处理： 123456789from sklearn import preprocessingX = data.iloc[:, :-1] # 除了label_cal列y = data[&#x27;label_cal&#x27;]np.random.seed(123)perm = np.random.permutation(len(X)) # 将数组随机生成一个新的序列X = X.loc[perm]y = y[perm]X = preprocessing.scale(X)# 进行标准化处理 1234567891011from sklearn.decomposition import PCA# 使用PCA进行降维pca = PCA(copy=True, n_components=6, whiten=False, random_state=1)X_new = pca.fit_transform(X)print(u&#x27;所保留的6个主成分的方差贡献率为：&#x27;)print(pca.explained_variance_ratio_)print(u&#x27;排名前2的主成分特征向量为：&#x27;)print(pca.components_[0:1])print(u&#x27;累计方差贡献率为：&#x27;)print(sum(pca.explained_variance_ratio_)) 12# 对数据集进行划分x_train, x_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=2, stratify=data[&#x27;label_cal&#x27;]) 12345678# 建立随机森林模型ran_for = RandomForestClassifier(n_estimators=80, random_state=0, class_weight=weights)# 训练模型ran_for.fit(x_train, y_train)# 对y进行预测y_pred_ran = ran_for.predict(x_test)y_predprb = ran_for.predict_proba(x_test)[:, 1] 123# 计算AUC值test_roc_auc = roc_auc_score(y_test, y_predprb)print(test_roc_auc) 12# 打印模型分类预测报告print(classification_report(y_test, y_pred_ran, digits=2)) 12345678# 绘制混淆矩阵热力图cm4 = confusion_matrix(y_test, y_pred_ran)plt.figure(figsize=(9, 9))sns.heatmap(cm4, annot=True, linewidths=.5, square=True, cmap=&#x27;Blues&#x27;)plt.ylabel(&#x27;Actual label&#x27;)plt.xlabel(&#x27;Predicted label&#x27;)all_sample_title = &#x27;ROC AUC score: &#123;0&#125;&#x27;.format(round(test_roc_auc,2))plt.title(all_sample_title, size=15) 降维之后的随机森林模型实现了较为显著的提升，能够实现测试集上患病类别(label_cal=1)的召回率(Recall)达到0.85，且精确度(Precision)达到0.83，总体的平均F1_score达到0.65。"},{"title":"随机森林分析","path":"/2023/08/31/随机森林分析/","content":"准备数据共计1059条，各数据字段含义如下表所示： 字段 含义 case 国家编号，代表特定国家的数字 cc3 国家代码，三个字母的国家&#x2F;地区代码 country 国家名称 year 观测年份 systemic_crisis 系统性危机，“ 0”表示当年未发生系统性危机，“ 1”表示当年有发生系统性危机 exch_usd 该国货币兑美元的汇率 domestic_debt_in_default 国内债务违约，“0”表示当年未发生国内债务违约，“1”表示当年有发生国内债务违约 sovereign_external_debt_default 主权外债违约，“0”表示当年未发生主权外债违约，“1”表示当年有发生主权外债违约 gdp_weighted_default 违约债务总额与GDP之比 inflation_annual_cpi 年度CPI通货膨胀率 independence 独立性，“ 0”表示“无独立性”，“ 1”表示“独立性” currency_crises 货币危机，“ 0”表示当年未发生“货币危机”，“ 1”表示当年有发生“货币危机” inflation_crises 通胀危机，“ 0”表示当年未发生“通胀危机”，“ 1”表示当年有发生“通胀危机” banking_crisis 银行业危机，“ no_crisis”表示当年没有发生银行业危机，而“ crisis”表示当年有发生银行业危机 1 数据读取与预处理1.1 读取数据12345678910111213141516171819202122# 导入相应模块import numpy as np import pandas as pdimport matplotlib.pyplot as pltfrom matplotlib.font_manager import FontProperties# 设置字体font = FontProperties(fname = &quot;./dataset/SimHei.ttf&quot;, size=14)import seaborn as snsimport random# 设置绘图风格%matplotlib inlinesns.set(style=&#x27;whitegrid&#x27;)# 忽略所有警告import warningswarnings.filterwarnings(&#x27;ignore&#x27;)# 读取数据data = pd.read_csv(&#x27;./dataset/african_crises.csv&#x27;)data.sample(5) 1.2 查看数据12unique_countries = data.country.unique()unique_countries 12# 数据集的基本信息data.______() 12# 查看数据统计性指标data.____________(include = &#x27;all&#x27;) 1.3 数据预处理12#查看货币危机currency_crises的取值为2的数据data[data[&#x27;currency_crises&#x27;] == 2] 12data = data[data[&#x27;currency_crises&#x27;] != 2]# 得到生成删除货币危机currency_crises的取值为2的数据集data.______ # 查看新生成的数据集大小 2 经济指标探索性分析2.1 汇率变化情况1234567891011121314151617181920212223plt.figure(figsize=(12,20))for i in range(13): plt.subplot(7,2,i+1) country = unique_countries[i] # 随机生成一种颜色 random.choice():从一个序列中随机的抽取一个元素，抽取6次组成6位代表随机颜色 col=&quot;#&quot;+&#x27;&#x27;.join([random.choice(&#x27;0123456789ABCDEF&#x27;) for j in range(6)]) # 绘制折线图 sns.____________(data[data.country == country][&#x27;year&#x27;],data[data.country == country][&#x27;exch_usd&#x27;],label = country,color = col) # np.logical_and()逻辑与 两个条件均成立时返回True plt.plot([np.min(data[np.logical_and(data.country == country,data.independence == 1)][&#x27;year&#x27;]), np.min(data[np.logical_and(data.country == country,data.independence == 1)][&#x27;year&#x27;])], [0,np.max(data[data.country == country][&#x27;exch_usd&#x27;])],color = &#x27;black&#x27;,linestyle = &#x27;dotted&#x27;,alpha = 0.8) plt.______(country) # 添加图像标题 plt.tight_layout() # 自动调整子图参数以提供指定的填充plt.show() # 输出13个国家的货币兑美元的汇率变化情况的折线图 2.2 通货膨胀率变化情况12345678910111213141516171819202122232425plt.figure(figsize=(12,20))for i in range(13): plt.subplot(7,2,i+1) country = unique_countries[i] # 随机生成一种颜色 col=&quot;#&quot;+&#x27;&#x27;.join([random.choice(&#x27;0123456789ABCDEF&#x27;) for j in range(6)]) # 绘制折线图 sns.lineplot(data[data.country == country][&#x27;year&#x27;],data[data.country == country][&#x27;inflation_annual_cpi&#x27;],label = country,color = col) # 加入散点图 plt.______(data[data.country == country][&#x27;year&#x27;],data[data.country == country][&#x27;inflation_annual_cpi&#x27;],color = col,s = 28) # s指散点的面积 plt.plot([np.min(data[np.logical_and(data.country == country,data.independence==1)][&#x27;year&#x27;]), np.min(data[np.logical_and(data.country == country,data.independence==1)][&#x27;year&#x27;])], [np.min(data[data.country == country][&#x27;inflation_annual_cpi&#x27;]),np.max(data[data.country == country][&#x27;inflation_annual_cpi&#x27;])], color = &#x27;black&#x27;,linestyle = &#x27;dotted&#x27;,alpha = 0.8) # alpha指颜色透明度 plt.title(country) plt.tight_layout() # 自动调整子图参数以提供指定的填充plt.show() # 输出13个国家的通货膨胀率变化情况 2.3 其他危机分布情况123456789101112131415sns.set(style=&#x27;darkgrid&#x27;)columns = [&#x27;systemic_crisis&#x27;,&#x27;domestic_debt_in_default&#x27;,&#x27;sovereign_external_debt_default&#x27;,&#x27;currency_crises&#x27;,&#x27;inflation_crises&#x27;,&#x27;banking_crisis&#x27;]# 绘制其他特征的分布规律图plt.figure(figsize=(16,16))for i in range(6): plt.subplot(&#x27;32&#x27;+str(i+1)) sns.countplot(y = data.country,hue = data[columns[i]],palette = &#x27;rocket&#x27;) # palette为调色板 plt.______(loc = 0) # 选择最优的图例位置 plt.title(columns[i]) plt.tight_layout()plt.show() 2.4 特征间的相关性123456789101112131415161718# 创建包含年份，国家，系统性危机，银行危机的数据集systemic = data[[&#x27;year&#x27;,&#x27;country&#x27;, &#x27;systemic_crisis&#x27;, &#x27;banking_crisis&#x27;]]# 绘制观察系统性危机与银行危机发生的重叠性systemic = systemic[(systemic[&#x27;country&#x27;] == &#x27;Central African Republic&#x27;) | (systemic[&#x27;country&#x27;]==&#x27;Kenya&#x27;) | (systemic[&#x27;country&#x27;]==&#x27;Zimbabwe&#x27;) ]plt.figure(figsize=(12,12))count = 1for country in systemic.country.unique(): plt.subplot(len(systemic.country.unique()),1,count) subset = systemic[(systemic[&#x27;country&#x27;] == country)] sns.lineplot(subset[&#x27;year&#x27;],subset[&#x27;systemic_crisis&#x27;],ci=None) # ci参数可用于指定线段区间的大小 plt.scatter(subset[&#x27;year&#x27;],subset[&quot;banking_crisis&quot;], color=&#x27;coral&#x27;, label=&#x27;Banking Crisis&#x27;) plt.subplots_adjust(hspace=0.6) # hspace用来设置子图上下间的距离 plt.______(&#x27;Years&#x27;) # 给x轴命名 plt.______(&#x27;Systemic Crisis/Banking Crisis&#x27;) # 给y轴命名 plt.title(country) count+=1 12345678910111213141516171819202122# 将银行危机banking_crisis列进行特征编码# 将银行危机banking_crisis中未发生危机的数据标为0，发生危机的数据标为1data[&#x27;banking_crisis&#x27;] = data[&#x27;banking_crisis&#x27;].map(&#123;&quot;no_crisis&quot;:0,&quot;crisis&quot;:1&#125;)# 选出所有特征selected_features = [&#x27;systemic_crisis&#x27;, &#x27;exch_usd&#x27;, &#x27;domestic_debt_in_default&#x27;,&#x27;sovereign_external_debt_default&#x27;, &#x27;gdp_weighted_default&#x27;, &#x27;inflation_annual_cpi&#x27;, &#x27;independence&#x27;, &#x27;currency_crises&#x27;,&#x27;inflation_crises&#x27;,&#x27;banking_crisis&#x27;]corr = data[selected_features].______() # 得到各特征间的相关性大小生成相关性矩阵fig = plt.figure(figsize = (12,8))cmap = sns.diverging_palette(220, 10, as_cmap=True) # 生成蓝-白-红的颜色列表mask = np.zeros_like(corr, dtype=np.bool) # 返回与相关性矩阵具有相同形状和类型的零数组作为掩码mask[np.triu_indices_from(mask)] = True # 给相关性矩阵的上三角阵生成掩码# 绘制热力图sns.______(corr, mask=mask, cmap=cmap,vmin=-0.5,vmax=0.7, center=0,annot = True, square=True, linewidths=.5,cbar_kws=&#123;&quot;shrink&quot;: .5&#125;);plt.title(&quot;特征间的相关性&quot;,fontproperties = font)plt.show() 3 构建银行危机预测模型 特征编码 数据集划分与分层采样 建立随机森林预测模型 模型效果的评估 使用SMOTE进行过采样优化模型 特征重要性排序 3.1 特征编码12data.drop([&#x27;case&#x27;,&#x27;cc3&#x27;],axis = 1,inplace = True) # 在原数据集上删掉case列和cc3列data.head() 12345# 对国家country进行labelencoderfrom sklearn.preprocessing import LabelEncoderle = LabelEncoder()le.______(data[&#x27;country&#x27;].values) # 将country的值塞入空字典data[&#x27;country&#x27;]=le.____________(data[&#x27;country&#x27;].values) # 将字典中的country的值转变为索引值 1print(data[&#x27;country&#x27;]) # 查看特征编码后的country名称 1234567# 绘制未发生银行危机no_crisis与发生银行危机crisis的柱状图fig = plt.figure(figsize = (8,6))data[&#x27;banking_crisis&#x27;].value_counts().plot(kind=&#x27;______&#x27;,rot = 360,color = &#x27;lightseagreen&#x27;)plt.xticks([0,1],[&quot;no_crisis&quot;,&quot;crisis&quot;])plt.show() 3.2 数据集划分与分层采样下面我们开始对数据进行训练集与测试集的划分。在Sklearn中的model_selection模块，存在train_test_split()函数，用作训练集和测试集划分，函数语法为：train_test_split(x,y,test_size = None,random_state = None,stratify = y)，其中： x,y: 分别为预测所需的所有特征，以及需要预测的特征。 test_size: 测试集比例，例如test_size=0.2则表示划分20%的数据作为测试集。 random_state: 随机种子，因为划分过程是随机的，为了进行可重复的训练，需要固定一random_state，结果重现。 stratify: 使用分层采样，保证从发生银行危机样本和未发生银行危机样本中抽取了同样比例的训练集和测试集。 函数最终将返回四个变量，分别为x的训练集和测试集，以及y的训练集和测试集。 1234567# 训练集与测试集的划分from sklearn import model_selectionx = data.drop(&#x27;banking_crisis&#x27;,axis = 1) # 将删除banking_crisis列的数据集作为xy = data[&#x27;banking_crisis&#x27;] # banking_crisis列作为yx_train,x_test,y_train,y_test = model_selection.__________________(x, y,test_size=0.2,random_state = 33,stratify=y) 3.3 建立随机森林预测模型随机森林是一种集成学习方法，通过使用随机的方式从数据中抽取样本和特征，训练多个不同的决策树，形成“森林”。每个树都给出自己的分类意见，称“投票”。在分类问题下，森林选择选票最多的分类；在回归问题下则使用平均值。在Python中使用sklearn.ensemble的RandomForestClassifier构建分类模型，其主要参数包括： n_estimators : 训练分类器的数量(默认为100)； max_depth : 每棵树的最大深度(默认为3)； max_features: 划分的最大特征数(默认为 ‘auto’) random_state : 随机种子。 123456from sklearn.ensemble import RandomForestClassifier# 训练随机森林分类模型rf = RandomForestClassifier(n_estimators = 100, max_depth = 20,max_features = 10, random_state = 20)rf.fit(x_train, y_train) y_pred = rf.______(x_test) # 对y进行预测 3.4 模型评估在评价模型好坏时，我们分别使用函数classification_report()、confusion_matrix()和accuracy_score(),用于输出模型的预测报告、混淆矩阵和分类正确率。 12345678910111213from sklearn.metrics import classification_report,confusion_matrixprint(classification_report(y_test, y_pred)) # 输出模型的预测报告confusion_matrix = __________________(y_test, y_pred) print(confusion_matrix) # 输出混淆矩阵# 绘制混淆矩阵热力图fig,ax = plt.subplots(figsize=(8,6)) sns._________(confusion_matrix,ax=ax,annot=True,annot_kws=&#123;&#x27;size&#x27;:15&#125;, fmt=&#x27;d&#x27;,cmap = &#x27;YlGnBu_r&#x27;)ax.set_ylabel(&#x27;真实值&#x27;,fontproperties = font)ax.set_xlabel(&#x27;预测值&#x27;,fontproperties = font)ax.set_title(&#x27;混淆矩阵热力图&#x27;,fontproperties = font)plt.show() # 输出混淆矩阵热力图 12345678910111213141516171819from sklearn.metrics import roc_auc_scorefrom sklearn.metrics import roc_curveroc_auc = ____________(y_test, rf.predict(x_test)) #计算auc的值fpr, tpr, thresholds = ____________(y_test, rf.predict_proba(x_test)[:,1]) #计算不同阈值下的TPR和FPR# 绘制ROC曲线plt.figure(figsize = (8,6))plt.plot(fpr, tpr, label=&#x27;Random Forest (area = %0.2f)&#x27; % roc_auc)plt.plot([0, 1], [0, 1],&#x27;r--&#x27;)# 绘制随机猜测线plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel(&#x27;False Positive Rate&#x27;)plt.ylabel(&#x27;True Positive Rate&#x27;)plt.title(&#x27;ROC curve&#x27;)plt.legend(loc=&quot;lower right&quot;) # 图例位置位于右下方plt.show() 3.5 使用SMOTE进行采样优化模型在对数据集进行划分，接着对训练集进行过采样，将少数类进行扩充。在Python中使用imblearn.over_sampling的SMOTE类构建SMOTE过采样模型。 12345# 对x_train,y_train进行SMOTE过采样from imblearn.over_sampling import SMOTEx_train_resampled, y_train_resampled = SMOTE(random_state=4).fit_resample(x_train, y_train)print(x_train_resampled.shape, y_train_resampled.shape) #查看采样后的数据集大小 1234567# 通过网格搜索选择最优参数from sklearn.model_selection import GridSearchCVparam_grid = [&#123; &#x27;n_estimators&#x27;:[10,20,30,40,50], &#x27;max_depth&#x27;:[5,8,10,15,20,25]&#125;]grid_search = GridSearchCV(rf, param_grid, scoring = &#x27;recall&#x27;) 123456# 输出最佳参数组合以及分数grid_search.fit(x_train_resampled, y_train_resampled)print(&quot;best params:&quot;, grid_search.best_params_)print(&quot;best score:&quot;, grid_search.best_score_) 3.6 特征重要性排序123456789101112131415fig = plt.figure(figsize=(16,12))# 得到随机森林特征重要性评分rf_importance = rf.__________________index = data.drop([&#x27;banking_crisis&#x27;], axis=1).columns # 删掉银行危机banking_crisis列特征# 对得到的特征重要性评分进行降序排序rf_feature_importance = pd.DataFrame(rf_importance.T, index=index,columns=[&#x27;score&#x27;]).sort_values(by=&#x27;score&#x27;, ascending=True)# 水平条形图绘制rf_feature_importance.plot(kind=&#x27;______&#x27;,legend=False,color = &#x27;deepskyblue&#x27;)plt.title(&#x27;随机森林特征重要性&#x27;,fontproperties = font)plt.show()"},{"title":"商品评论情感判定","path":"/2023/08/31/商品评论情感判定/","content":"准备用户在电商平台上面发表的产品评价中包含着用户的偏好信息，所以通过用户评论，可以得到用户的情感倾向以及对产品属性的偏好。 1234567891011121314# 载入必要库import jiebaimport numpy as npimport pandas as pdimport sklearnimport matplotlibimport matplotlib.pyplot as plt import pyecharts.options as optsfrom pyecharts.charts import WordCloudfrom pyecharts.charts import Barimport re#loggingimport warningswarnings.filterwarnings(&#x27;ignore&#x27;) 1 数据读取1.1 读取数据某款手机的商品评论信息数据集，包含2个字段，共计8186个样本。数据集描述如下： 列名 说明 类型 Comment 对该款手机的评论 String Class 该评论的情感倾向: -1 —— 差评 0 —— 中评 1 —— 好评 Int 使用Pandas库中read_csv()函数读取数据。 123#读入数据集data = pd.read_csv(&#x27;./dataset/data.csv&#x27;)data.head(10) 1.2 查看数据12# 数据集的大小data.shape 12# 数据集的基本信息data.info() 2 数据预处理使用分词库jieba中的cut()函数对文本进行分词。 2.1 去除缺失值12# 移除含有缺失值的行data.dropna(axis=0,inplace=True) 12#查看去除缺失值后的行和列data.shape 2.2 分词去除标点数字字母后再分词。 1234def remove_url(src): # 去除标点符号、数字、字母 vTEXT = re.sub(&#x27;[a-zA-Z0-9’!&quot;#$%&amp;\\&#x27;()*+,-./:;&lt;=&gt;?@，。?★、…【】╮ ￣ ▽ ￣ ╭\\\\～⊙％；①（）：《》？“”‘’！[\\\\]^_`&#123;|&#125;~\\s]+&#x27;, &quot;&quot;, src) return vTEXT 12345678910111213cutted = []for row in data.values: text = remove_url(str(row[0])) #去除文本中的标点符号、数字、字母 raw_words = (&#x27; &#x27;.join(jieba.cut(text)))#分词,并用空格进行分隔 cutted.append(raw_words)cutted_array = np.array(cutted)# 生成新数据文件，Comment字段为分词后的内容data_cutted = pd.DataFrame(&#123; &#x27;Comment&#x27;: cutted_array, &#x27;Class&#x27;: data[&#x27;Class&#x27;]&#125;) 12#查看分词后的数据集data_cutted.head() data_cutted为进行分词之后的数据集。 2.3 查看关键词读取停用词文件，再用jieba.analyse中的set_stop_words函数设置停用词，使用extract_tags函数提取关键词。 12345with open(&#x27;./dataset/stopwords.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:#读停用词表 stopwords = [item.strip() for item in f] #通过列表推导式的方式获取所有停用词 for i in stopwords[:100]:#读前100个停用词 print(i,end=&#x27;&#x27;) 123#设定停用词文件,在统计关键词的时候，过滤停用词import jieba.analysejieba.analyse.set_stop_words(&#x27;./dataset/stopwords.txt&#x27;) 1data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == 1] 1234# 好评关键词keywords_pos = jieba.analyse.extract_tags(&#x27;&#x27;.join(data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == 1]),withWeight = True,topK=30)for item in keywords_pos: print(item[0],end=&#x27; &#x27;) 1234#中评关键词keywords_med = jieba.analyse.extract_tags(&#x27;&#x27;.join(data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == 0]),withWeight = True,topK=30)for item in keywords_med: print(item[0],end=&#x27; &#x27;) 12345#差评关键词keywords_neg = jieba.analyse.extract_tags(&#x27;&#x27;.join(data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == -1]),withWeight = True,topK=30)for item in keywords_neg: print (item[0],end=&#x27; &#x27;) 3 可视化分析使用Pyecharts进行绘图。 3.1 评价柱状图1data_cutted[&#x27;Class&#x27;].value_counts() 123456789101112# 不同类别数据记录的统计x_label = [&#x27;好评&#x27;,&#x27;差评&#x27;,&#x27;中评&#x27;]class_num = ( Bar() #设置x轴的值 .add_xaxis(x_label) #设置y轴数据 .add_yaxis(&quot;&quot;,data_cutted[&#x27;Class&#x27;].value_counts().to_list(),color=[&#x27;#4c8dae&#x27;]) #设置title .set_global_opts(title_opts=opts.TitleOpts(title=&quot;好评、中评、差评数量柱状图&quot;)))class_num.render_notebook() 3.2 好评云图123456789101112wordcloud_pos = ( WordCloud() #data_pair：要绘制词云图的数据 .add(series_name=&quot;&quot;, data_pair=keywords_pos[:], word_size_range=[10, 66]) .set_global_opts( title_opts=opts.TitleOpts( #设置词云图标题和标题字号 title=&quot;好评关键词词云图&quot;, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True)) )wordcloud_pos.render_notebook() 3.3 中评云图123456789101112wordcloud_med = ( WordCloud() #data_pair：要绘制词云图的数据 .add(series_name=&quot;&quot;, data_pair=keywords_med[:], word_size_range=[10, 66]) .set_global_opts( title_opts=opts.TitleOpts( #设置词云图标题和标题字号 title=&quot;中评关键词词云图&quot;, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True)) )wordcloud_med.render_notebook() 3.4 差评云图123456789101112wordcloud_neg = ( WordCloud() #data_pair：要绘制词云图的数据 .add(series_name=&quot;&quot;, data_pair=keywords_neg[:], word_size_range=[10, 66]) .set_global_opts( title_opts=opts.TitleOpts( #设置词云图标题和标题字号 title=&quot;差评关键词词云图&quot;, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True)) )wordcloud_neg.render_notebook() 4 文本向量化经过分词之后的文本数据集要先进行向量化之后才能输入到分类模型中进行运算。TF-IDF算法是常用的文本向量化算法。 TF-IDF是Term Frequency-Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。TF-IDF是一种统计方法，用以评估一个词对于一个文件集或一个语料库中的一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。使用sklearn库中的TfidfVectorizer实现tf-idf文本向量化。 1234567# 实现向量化方法from sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(stop_words = stopwords,max_df=2000,min_df=6)#将文本向量化后的数据赋给data_transformdata_transform = vectorizer.fit_transform(data_cutted[&#x27;Comment&#x27;].values.tolist()) 12#文本的词汇表vectorizer.get_feature_names() 12#调用toarray()方法查看文本向量化后的数据data_transform.toarray() 1data_transform.shape 5 高斯朴素贝叶斯模型5.1 数据集划分使用sklearn.model_selection模块的train_test_split()函数划分训练集和测试集。 123from sklearn.model_selection import train_test_split #数据集划分X_train, X_test, y_train, y_test = train_test_split(data_transform, data_cutted[&#x27;Class&#x27;], random_state=10,test_size=0.2) 5.2 构建高斯朴素贝叶斯模型从sklearn.naive_bayes中导入GaussianNB类，使用GaussianNB类初始化一个模型对象，命名为gnb，对gnb调用fit方法，带入训练集X_train，y_train进行训练。 1from sklearn.naive_bayes import GaussianNB 12gnb = GaussianNB()gnb_model = gnb.fit(X_train.toarray(),y_train) 5.3 高斯朴素贝叶斯模型评估从sklearn.metrics中导入classification_report分类报告用于模型评估. 1from sklearn.metrics import classification_report 123gnb_prelabel = gnb_model.predict(X_test.toarray())print(classification_report(y_true=y_test,y_pred=gnb_prelabel)) 6 SVM模型构建6.1 构建SVM模型从sklearn.svm中导入SVC类，使用SVC类初始化一个模型对象，命名为svc，对svc调用fit方法，带入训练集X_train，y_train进行训练。 12345from sklearn.svm import SVC#设置kernel为‘rbf’高斯核，C=1svc = SVC(kernel=&#x27;rbf&#x27;, C=1)svc_model = svc.fit(X_train,y_train) 训练模型后，可以使用模型在测试集X_test上作出预测。 6.2 模型评估12svc_prelabel = svc_model.predict(X_test)print(classification_report(y_true=y_test,y_pred=svc_prelabel)) 通过将SVM模型与构建的高斯朴素贝叶斯模型分类结果比较，可以看出SVM在分类的精确率、召回率，以及模型的准确率上都优于高斯朴素贝叶斯模型。"},{"title":"汽车产品聚类分析","path":"/2023/08/31/汽车产品聚类分析/","content":"内容根据各种汽车参数利用聚类算法进行聚类，识别出相似的汽车，汽车款式数据集中有392个样本，8个特征。说明如下： 字段名称 字段类型 字段说明 mpg 浮点型 每加仑英里 cylinders 整型 气缸数在4到8之间 displacement 浮点型 发动机排量（立方英寸） horsepower 整型 引擎马力 weight 整型 车重（磅） acceleration 浮点型 从0加速到60 mph（秒）的时间 year 整型 型号年份 origin 整型 汽车来源（1.美国，2.欧洲，3.日本） 1 准备工作导入相关包和模块。 123# 忽略警告信息import warningswarnings.filterwarnings(&quot;ignore&quot;) 1234 #导入相关的包和模块import numpy as npimport pandas as pdfrom sklearn import metrics 12import sklearnsklearn.__version__ 1np.__version__ 123#读入数据集并查看数据集的前五行信息auto = pd.read_csv(&#x27;./dataset/Auto.csv&#x27;) auto.head() 12#查看数据集基本信息auto.info() 2 数据预处理1auto.head() 对mpg,displacement,horsepower,weight,acceleration五个连续型变量用Z-score标准化，对cylinders,year,origin三个类别变量进行哑变量编码。 1234#Z-score标准化from sklearn.preprocessing import StandardScalerscaler = StandardScaler()auto[[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]] =scaler.fit_transform(auto[[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]]) 1StandardScaler.__version__ 1auto.head() 1pd.get_dummies? 12#哑变量编码auto_scaled = pd.get_dummies(data=auto,columns=[&#x27;cylinders&#x27;,&#x27;year&#x27;,&#x27;origin&#x27;]) 12#查看进行标准化处理和哑变量编码后数据集的前两行auto_scaled.head(2) 3 K-Means聚类K-Means聚类基于点与点之间的距离的相似度来计算最佳类别归属。其核心目标是将给定的数据集划分成K个簇，并给出每个样本数据对应的中心点。 123#导入K-Means聚类方法，将初始K值设为3，随机种子数设为0，并训练模型from sklearn.cluster import KMeansmodel = KMeans(n_clusters=3,random_state=0).fit(auto_scaled) 1KMeans? 12345#将样本标签和簇质心保存在auto_lable和auto_cluster中,展示分类结果标签的前十行，以及簇质心的第一行auto_label = model.labels_auto_cluster = model.cluster_centers_print(auto_label[:10])print(auto_cluster[0]) 统计聚类结果，根据我们设置的初始K值，结果将所有样本分为三类，并按照每一类的数量进行降序展示： 1pd.value_counts(auto_label) 1pd.value_counts? 12345678#找出簇质心连续性变量的坐标centroid_cluster = pd.DataFrame(auto_cluster).copy().iloc[:,:5]centroid_cluster.columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]#将数据逆标准化，转换为原始数据,结果保留两位小数：centroid_cluster_inversescale = pd.DataFrame(scaler.inverse_transform(centroid_cluster))centroid_cluster_inversescale.columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]centroid_cluster_inversescale.applymap(lambda x:&#x27;%.2f&#x27;%x) 选择轮廓系数作为聚类性能的评估指标，轮廓系数取值范围为[-1,1]，轮廓系数为-1时表示聚类结果不好，为+1时表示簇内实例之间紧凑，为0时表示有簇重叠。轮廓系数越大，表示簇内实例之间紧凑，簇间距离大，即聚类的效果越好。 1pd.value_counts(auto_label) 1234#轮廓系数结果保留四位小数labels = model.labels_print(&quot;轮廓系数(Silhouette Coefficient): %0.4f&quot; % metrics.silhouette_score(auto_scaled, labels)) 1metrics.silhouette_score? 1234for i in range(2,12,2): model = KMeans(n_clusters=i,random_state=0).fit(auto_scaled) labels = model.labels_ print(&quot;轮廓系数(Silhouette Coefficient): %0.4f&quot;% metrics.silhouette_score(auto_scaled, labels)) 根据结果可以看到当k值取2时，其轮廓系数最大，即聚类效果最佳。 4 DBSCAN聚类DBSCAN是一种典型的基于密度的聚类算法，在DBSCAN算法中将数据点分为以下三类： 核心点：在半径Eps内含有超过MinPts数目的点 边界点：在半径Eps内点的数量小于MinPts，但是落在核心点的邻域内 噪音点：既不是核心点也不是边界点的点（离群点）12#导入DBSCANfrom sklearn.cluster import DBSCAN 1DBSCAN? 12#模型的参数设置：一个是半径eps，这里设置为1.5，另一个是指定的数目MinPts，这里设置为4：model = DBSCAN(eps=1.5,min_samples=4).fit(auto_scaled) 根据我们设置的参数，DBSCAN将所有样本分为四类（包含噪声点），并按照每一类的数量降序展示:123auto_label = model.labels_df=pd.value_counts(auto_label)df 123import matplotlib.pyplot as pltplt.figure(figsize=(5,5))df.plot.bar(rot=0) 将不同簇的数据使用均值聚合，得到近似类中心12345#先将数据逆标准化，转为原始数据，并取出前五列，再根据聚类标签分类使用均值聚合，结果保留两位小数auto_new_continuous = pd.DataFrame(scaler.inverse_transform(auto_scaled.iloc[:, :5]), columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;])auto_new_continuous[&#x27;label&#x27;] = auto_labelauto_new_continuous.groupby(&#x27;label&#x27;).mean().applymap(lambda x:&#x27;%.2f&#x27;%x) 去除噪声点样本后，计算轮廓系数12labels = model.labels_metrics.silhouette_score(auto_scaled[labels&gt;-1],labels[labels&gt;-1]) 调整密度参数，查看聚类结果1234567891011121314# 不同最小样本数下的簇个数## 设置参数取值范围min_samples_grid = [1,2,3,4]## 训练模型并输出簇个数cluster_number = []slt_score = []noise_count = []for item in min_samples_grid: model = DBSCAN(eps=1.5,min_samples=item).fit(auto_scaled) cluster_number.append(len(np.unique(model.labels_))-1) slt_score.append(metrics.silhouette_score(auto_scaled[model.labels_&gt;-1], model.labels_[model.labels_&gt;-1])) noise_count.append((model.labels_==-1).sum()) 1np.unique? 123456## 绘图import matplotlib.pyplot as pltplt.plot(min_samples_grid, cluster_number, &#x27;r-*&#x27;, linewidth=2)plt.xlabel(&#x27;最小样本数&#x27;)plt.ylabel(&#x27;簇个数&#x27;)plt.title(&#x27;不同最小样本数下聚类的簇个数&#x27;) 1234plt.plot(min_samples_grid, slt_score, &#x27;b-*&#x27;, linewidth=2)plt.xlabel(&#x27;最小样本数&#x27;)plt.ylabel(&#x27;轮廓系数&#x27;)plt.title(&#x27;不同最小样本数下聚类的轮廓系数&#x27;) 1234plt.plot(min_samples_grid, noise_count, &#x27;g-*&#x27;, linewidth=2)plt.xlabel(&#x27;最小样本数&#x27;)plt.ylabel(&#x27;噪声点数量&#x27;)plt.title(&#x27;不同最小样本数下聚类的噪声点数量&#x27;) 5 层次聚类层次聚类有两种常用的形式，自顶向下和自底向上,自底向上的主要做法是，在开始时，将每个样本视为一个簇，重复的合并最近的两个簇，直到簇的个数达到给定值。 12345from sklearn.cluster import AgglomerativeClustering# 训练模型model = AgglomerativeClustering(n_clusters=3,linkage=&#x27;average&#x27;).fit(auto_scaled)# 输出模型结果 auto_label = model.labels_ 1AgglomerativeClustering? 123auto_label = model.labels_df=pd.value_counts(auto_label)df 12plt.figure(figsize=(5,5))df.plot.bar() 12345#先将数据逆标准化，转为原始数据，并取出前五列，再根据聚类标签分类使用均值聚合，结果保留两位小数auto_new_continuous = pd.DataFrame(scaler.inverse_transform(auto_scaled.iloc[:, :5]), columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;])auto_new_continuous[&#x27;label&#x27;] = auto_labelauto_new_continuous.groupby(&#x27;label&#x27;).mean().applymap(lambda x:&#x27;%.2f&#x27;%x)"},{"title":"图书数据可视化","path":"/2023/08/30/图书数据可视化/","content":"1 数据集介绍使用上次清洗过的数据进行数据分析，并利用可视化图形对分析。数据集共9个字段，600个样本，如下： 价格 星级 评论数 作者 出版日期 出版社 书名 简介1 简介2 2 数据读取2.1 导入相关的库12345678#import pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltimport jiebafrom wordcloud import WordCloud,STOPWORDS# 设置中文字体plt.rcParams[&#x27;font.sans-serif&#x27;]=&#x27;SimHei&#x27;%config InlineBackend.figure_format = &#x27;svg&#x27; 2.2 数据读取调用Pandas对象的read_csv()函数。 1234#读数据集data = pd.read_csv(&#x27;./dataset/当当网机器学习图书数据（已清洗）.csv&#x27;,sep=&#x27;,&#x27;,encoding=&#x27;utf-8&#x27;)#查看前5行data.head(5) 2.3 字段基本统计信息使用DataFrame对象的describe()方法可以查看各列基本统计信息，统计并生成数据集哥哥字段的样本数、均值、标准差、最小值、四分位数等。describe()方法主要参数： percentiles：自定义分位数，默认是25%,50%,75% include：指定统计的数据类型，默认只统计数值型，当为all时数值和离散型都统计 exclude：意排除哪些字段，默认统计所有列 12#查看数据基本统计信息 data.describe(include=&#x27;all&#x27;) 3 可视化使用python中的Matplotilb、Seaborn等库进行可视化。 3.1 各出版社的图书数量统计各出版社关于机器学习相关的图书数量，对出版数量有直观了解。Series对象的value_counts()方法可以对某列的取值数量分布进行统计，其主要参数： normallize：默认为False，若为True，则以百分比的形式显示 sort：是否对结果进行排序，默认为True ascending: 默认对值降序排列(False) dropna:是否删除空值，默认删除(True) 12# 各出版社出版的图书数量data[&#x27;出版社&#x27;].value_counts(ascending=True) 从统计结果可以看到出版机器学习相关图书最多的机械工业出版社。 3.2 各星级图书数量统计柱状图使用Seaborn中的barplot函数绘制柱状图，其主要参数有： x：x坐标传入的值 y：y坐标传入的值 data：传入的数据集 12# 查看各星级的数量data[&#x27;星级&#x27;].value_counts() 123456789101112# # 绘制画布，即画布的大小和分辨率plt.figure(figsize=(8,5), dpi=100)x = data[&#x27;星级&#x27;].value_counts().indexy = data[&#x27;星级&#x27;].value_counts().valuessns.barplot(x,y)# 设置标题plt.title(&#x27;各星级图书数量统计柱状图&#x27;,fontsize=13)# 设置x轴标签plt.xlabel(&#x27;星级&#x27;,fontsize=10)# 设置y轴标签plt.ylabel(&#x27;数量&#x27;,fontsize=10)plt.show() 3.3 图书价格直方图使用Seaborn中的histplot函数来绘制直方图，主要参数有： data：传入的数据 x：做直方图所用的数据，必须是一维数组 bins：分组数量 12345678910111213# 绘制画布，即大小和分辨率plt.figure(figsize=(8,5),dpi=100)# 绘图，分别统计不同图书价格的数量sns.histplot(data,x=&#x27;价格&#x27;,bins=20)# 绘制价格均值直线plt.plot([data[&#x27;价格&#x27;].mean(),data[&#x27;价格&#x27;].mean()],[0,200],&#x27;g--&#x27;)# 设置标题plt.title(&#x27;图书价格直方图&#x27;,fontsize=13)# 设置x轴标签plt.xlabel(&#x27;图书价格&#x27;,fontsize=10)# 设置y轴标签plt.ylabel(&#x27;数量&#x27;,fontsize=10)plt.show() 3.4 高价图书分析设置价格大于100元为高价图书，筛选高价图书如下： 1234# 利用DataFrame直接筛选输出高价图书data_price_high = data[data[&#x27;价格&#x27;]&gt;100]# 查看前3行高价图书信息data_price_high.head(3) 3.4.1高价书出版社统计使用Matplotlib中的pie函数绘制饼状图，主要参数如下： x：每一块的数值比例 labels：每一块外侧显示的说明文字 autopct：控制图内显示的百分比 startangle：起始角度，默认图是从x轴正方向画起，逆时针方向 pctdistance：数值标签距离圆心的距离 radius：控制饼图半径 1data_price_high[&#x27;出版社&#x27;].value_counts().index 123456789## 按照出版社进行分段统计x = data_price_high[&#x27;出版社&#x27;].value_counts().values# 设置饼图的标签labels = data_price_high[&#x27;出版社&#x27;].value_counts().index# 绘制画布plt.figure(figsize=(8,5),dpi=100)# 绘制饼状图plt.pie(x,labels=labels,autopct=&#x27;%1.1f%%&#x27;,startangle=30,pctdistance=0.8,radius=1)plt.show() 3.4.2 高价书星级评定使用Matplotlib中的bach()函数绘制柱状图，主要参数如下： y：y坐标 width：柱子的宽度，即统计的数值大小 height：柱子的高度，默认为0.8 1data_price_high.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean() 1234567891011# 取值各个出版社星级的平均值width = data_price_high.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean().values# 按照出版社求各个出版社星级评分的平均值，取出索引，即出版社y = data_price_high.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean().index# 设置画布plt.figure(figsize=(8,5),dpi=100)# 绘图plt.barh(y,width,height=0.8,color=&#x27;orange&#x27;)plt.title(&#x27;高价图书星级评定&#x27;,fontsize=13)plt.show() 3.5 各出版社图书价格均值对各出版社出版图书价格均值进行统计。 12# 按照出版社进行分组聚合，计算每组平均的图书价格，计算结果如下data.groupby(&#x27;出版社&#x27;)[&#x27;价格&#x27;].mean().sort_values(ascending=False) 3.6 各出版社出版图书口碑分析将出版社一列分组，计算各出版社图书平均星级。 1234# 按照出版社分段统计，求各个出版社星级的均值data_star_mean = data.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean()# 将每个出版社的平均星级进行降序排序 默认是升序排序data_star_mean.sort_values(ascending = False) 3.7 图书简介文本分词对图书的简介进行分析，首先数据中简介1这一列是文本类型，因此我们要先进行分词，分词的目的是将文本按一定的规则进行分词处理。在这里我们使用jieba库里面的cut函数进行分词，jieba库是专门使用Python语言开发的分词库,占用资源较少，常识类文档的分词精度较高。cut函数的主要参数如下： sentence:要进行的分词的句子样本 cut_all:分词的模式，有全模式和精准模式，默认false，精准模式 HMM:隐马尔科夫链，即HMM模型，默认开启，这个是在分词的理论模型中用到的 1234567# 对数据集的每个样本的文本进行中文分词#记录分词后的结果cutted = [] for item in data[&#x27;简介1&#x27;].values: raw_words = (&quot; &quot;.join(jieba.cut(str(item)))) cutted.append(raw_words) 1234567# 创建一个新的DataFrame，将没分词和分词后的句子添加到里面data_cutted = pd.DataFrame(&#123; &#x27;简介1&#x27;: data[&#x27;简介1&#x27;], &#x27;简介1_cut&#x27;: cutted&#125;)data_cutted.head() 3.8 词云图分词处理完毕后，再处理停用词，最后形成词云图。利用wordcloud中的WordColoud()函数绘制词云图，其中主要参数为： font_path:字体路径 stopword:将被忽略或者是删除的单词表 width:词云图的宽度，默认400 height：词云图的高度，默认200 max_font_size:最大字体的大小 12345# 读取停用词stopwd=pd.read_csv(&#x27;./dataset/中文停用词表数据集.csv&#x27;)stopwords=set([i for i in stopwd[&#x27;cn_stopwords&#x27;]])print(len(stopwords))# stopwords 1234567# 定义词云图wc = WordCloud(font_path = &quot;./dataset/simsun.ttc&quot;,#设置字体 stopwords = stopwords, #设置停用词 background_color = &#x27;white&#x27;, width = 1000, height = 618, max_font_size = 400) 12345678# 运行统计词频wc.generate(data_cutted[&#x27;简介1_cut&#x27;].sum())# 4、显示图片plt.figure(&quot;词云图&quot;) #指定所绘图名称plt.imshow(wc) # 以图片的形式显示词云plt.axis(&quot;off&quot;) #关闭图像坐标系plt.show() 1# data_cutted[&#x27;简介1_cut&#x27;].sum() 词云图突出显示了简介中出现频率较高的词，出现词频越高的词在词云图中显示越大。"},{"title":"网站数据清洗","path":"/2023/08/30/网站数据清洗/","content":"1 数据集介绍来自某图书网站爬取的机器学习相关图书信息，数据集共600条数据，5个字段，如下 书名 出版信息 当前价格 星级 评论数 2 数据读取2.1 读取数据数据集保存在csv文件中，使用Pandas中的read_csv()读取csv文件，结果保存为DataFrame或Series对象，使用DataFrame或Series对象的head()方法可以查看前n行数据。 1234567#导入相关的库import pandas as pdimport numpy as np#读取数据data = pd.read_csv(&#x27;./dataset/data.csv&#x27;)#查看数据data.head() 2.2 查看数据集的基本信息调用DataFrame对象的info()方法，获取数据的列名，非空值个数，列数据类型，内存占用信息。 1data.info() 数据集索引为0～599，共600条数据。各字段数据类型均为字符型。 3 数据清洗3.1 提取价格数值由于当前价格一列中含有¥符号，想对图书价格进行统计分析，需要从当前价格中取出价格的数值。借助正则表达书来完成上述操作。re库是python中正则表达式的支持库，使用findall()函数将当前价格中的数值提取出来，保存为新一列当前价格_match，findall()函数返回字符串中所有与正则表达式匹配的全部字符串，返回形式为数组。 +将前面的模式匹配一次或多次 ？匹配前一个字符零次或一次 *将前面的模式匹配零次或多次 .匹配除换行符之外的任意字符 \\转译字符 \\d匹配数字0～9 \\d{n}匹配正好n位数的数字 \\d{n,}匹配至少为n位的数字 \\d{n,m}匹配m～n位数的数字 [A-Za-z]+匹配英文字母组成的字符串 [A-Za-z0-9]+匹配由数字和英文字母组成的字符串DataFrame对象中apply方法可以将某个函数应用到由列或行形成的Series对象上，定义一个函数num_func，用于提取价格数值，然后使用apply方法将num_func应用到当前价格一列上。 1data[&#x27;当前价格&#x27;] 12345678import re#使用正则表达式将当前价格一列只读取数字部分并创建新的一列def func(data): result = re.findall(r&#x27;\\d+\\.?\\d*&#x27;,data) return float(result[0])data[&#x27;当前价格_match&#x27;] = data[&#x27;当前价格&#x27;].apply(func)data.head(3) 3.2 提取评论数值由于评论数一列中不仅包含数值，对评论数进行统计分析，需要从评论数一列中提取出评论的数值，保存为新的一列评论数_match。 1data[&#x27;评论数&#x27;] 12345678import re# 定义读取评论数的函数def func_1(data): esult = re.findall(r&#x27;\\d+&#x27;,data) return int(result[0])# 利用apply方法，将每一条数据进行处理data[&#x27;评论数_match&#x27;] = data[&#x27;评论数&#x27;].apply(func_1) 12# 查看是否处理成功data.head(3) 3.3 转换图书星级数值星级一列中同样包含一些其他字符，从星级一列中提取出星级的数值，将星级数值转换到[0,5]区间内，保存新的一列为星级_match。 123#提取星级数data[&#x27;星级_match&#x27;] = data[&#x27;星级&#x27;].apply(func_1)data.head(3) 12#将星级除以20，取值范围转换到[0,5]的区间内data[&#x27;星级_match_cal&#x27;] = data[&#x27;星级_match&#x27;].apply(lambda x:x/20) 1data.head(3) 3.4 提取作者、出版时间和出版社出版信息中包含作者、出版日期、出版社，用/分割为三部分，分三列存放，字符串对象的split方法通过制定分隔符对字符串进行切片。 1data[&#x27;出版信息&#x27;][:5] 1234# 将出版信息分割成三列，分别提取出作者、出版日期和出版社# 提取出作者data[&#x27;作者&#x27;] = data[&#x27;出版信息&#x27;].apply(lambda x:x.split(&#x27;/&#x27;)[0])data.head(3) 提取出的出版时间为字符串对象，我们可以通过datetime库中的strptime函数将字符串转换为datetime时间类型对象。 12345678910111213# 用正则表达式提取日期，并将日期字符串转换成日期格式from datetime import datetimedef func_2(data): result = re.findall(r&#x27;\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;&#x27;,data) if len(result)&lt;1: return None else: return datetime.strptime(result[0],&#x27;%Y-%m-%d&#x27;) #返回日期类型# 提取日期，并添加为新的一列data[&#x27;出版日期&#x27;] = data[&#x27;出版信息&#x27;].apply(func_2)data.head(3) 12345# 提取出版社一列，并添加为新的一列data[&#x27;出版社&#x27;] = data[&#x27;出版信息&#x27;].apply(lambda x:x.split(&#x27;\\&#x27;)[-1])# 查看结果data.head(3) 3.5 提取书名和简介书名字段包含书名和书籍的相关介绍。以空格为分隔符对字段进行分割，提取图书的书名和简介部分。 1data[&#x27;书名&#x27;][:5] 123456#将&#x27;【】&#x27;和&#x27;[]&#x27;以及之间的内容，用空格来代替def func_3(data): data = data.strip()#先去除头和尾的空格 data = re.sub(&quot;【.*?】&quot;,&quot; &quot;,data) data = re.sub(&quot;\\[.*?\\]&quot;,&quot; &quot;,data) return data.split(&quot; &quot;) 12data[&#x27;书名_split&#x27;] = data[&#x27;书名&#x27;].apply(func_3)data[&#x27;书名_split&#x27;][:5] 123# 提取书名data[&#x27;书名_split_1&#x27;] = data[&#x27;书名_split&#x27;].apply(lambda x:x[0])data[&#x27;书名_split_1&#x27;][:5] 1234#因为分割后的字段长度不唯一，所以从第三个开始我们要先判定每个字段分割后的长度，之后再进行提取# 提取简介1data[&#x27;书名_split_2&#x27;] = data[&#x27;书名_split&#x27;].apply(lambda x: None if len(x)&lt;=1 else x[1])data[&#x27;书名_split_2&#x27;][:5] 123# 提取简介2data[&#x27;书名_split_3&#x27;] = data[&#x27;书名_split&#x27;].apply(lambda x: None if len(x)&lt;=2 else x[2])data[&#x27;书名_split_3&#x27;][:5] 1data.head(3) 3.6 删除不需要的列使用DataFrame对象的drop方法删除不需要的列。 123# 删除不需要的列data.drop([&#x27;书名&#x27;,&#x27;出版信息&#x27;,&#x27;当前价格&#x27;,&#x27;星级&#x27;,&#x27;评论数&#x27;,&#x27;星级_match&#x27;,&#x27;书名_split&#x27;],axis=1,implace=True)data.head(3) 3.7 修改列名使用DataFrame对象的rname对列进行重命名。 123# 修改列名data.rename(columns=&#123;&#x27;当前价格_match&#x27;:&#x27;当前价格&#x27;,&#x27;评论数_match&#x27;:&#x27;评论数&#x27;,&#x27;星级_match_cal&#x27;:&#x27;星级&#x27;,&#x27;书名_split_1&#x27;:&#x27;书名&#x27;,&#x27;书名_split_2&#x27;:&#x27;简介1&#x27;,&#x27;书名_split_3&#x27;:&#x27;简介2&#x27;&#125;,implace=True)data.head(3) 4 保存数据使用DataFrame对象的to_csv方法将处理好的数据保存为CSV文件。 1data.to_csv(&#x27;当当网机器学习图书数据(已清洗).csv&#x27;, sep=&#x27;,&#x27;,encoding=&#x27;utf8&#x27;,index=False)"},{"title":"Hello World","path":"/2023/08/06/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"}]