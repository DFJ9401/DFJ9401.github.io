[{"title":"集成学习尝试","path":"/2023/09/02/集成学习/","content":"1 数据说明与导入本案例使用的数据集为台湾地区信用卡客户数据，具体数据字段的表格信息记录如下: 列名 含义说明 数据类型 ID 客户ID。 int64 LIMIT_BAL 银行给予客户的信用额度，包括个人信用额度和客户的家庭信用额度。 float64 SEX 客户的性别。男性记为1，女性记为2。 int64 EDUCATION 客户的教育水平。研究生及以上记为1，大学记为2，高中记为3，其它记为0。 int64 MARRIAGE 客户的婚姻状况。未婚记为2，已婚记为1，其它记为0。 int64 AGE 客户的年龄。 int64 PAY_1～PAY_6 这六个变量是2005年4月到9月每月的还款记录，取值为-1～9，代表累计逾期月数。 int64 BILL_AMT1～BILL_AMT6 这六个变量是2005年4月到9月每月的账单记录，即每月用信用卡消费记录。 float64 PAY_AMT1～PAY_AMT6 这六个变量是2005年4月到9月每月的支付记录，包括还账单金额和存入信用卡的金额。 float64 default.payment.next.month 代表客户下个月是否违约，违约记为1，未违约则记为0。 int64 123456789101112131415import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport warnings%matplotlib inlineimport sklearnfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier from sklearn import metrics from sklearn.metrics import precision_recall_curvefrom sklearn.metrics import confusion_matrixfrom sklearn import preprocessingwarnings.filterwarnings(&quot;ignore&quot;) 1df = pd.read_csv(&#x27;./dataset/data.csv&#x27;) 2 数据初步探索2.1 数据信息检查1df.____ 1df.shape() 1df.info() 1df.describe() 2.2 数据清洗首先对教育程度的字段用value_counts进行查看：它可以查看字段中有多少个不同值，并计算每个不同值有在该列中有多少重复值。 1df.describe() 1df[&#x27;EDUCATION&#x27;].________ 我们使用replace完成上述替换，inplace参数设置为True表示替换原数据。 12df[&#x27;EDUCATION&#x27;].______([1,3,4,5,6],[3,1,0,0,0],inplace=True)df[&#x27;EDUCATION&#x27;].______ 12df[&#x27;SEX&#x27;].replace([1,2],______,inplace=True)df[&#x27;SEX&#x27;].value_counts() 1df[&#x27;MARRIAGE&#x27;].value_counts() 12df[&#x27;MARRIAGE&#x27;].replace(3,0,inplace=True)df[&#x27;MARRIAGE&#x27;].value_counts() 1df[&#x27;PAY_1&#x27;].value_counts() 12df.iloc[:,[6,7,8,9,10,11]] = df.iloc[:,[6,7,8,9,10,11]].replace(______)df.iloc[:,[6,7,8,9,10,11]].describe() 1df[&#x27;default.payment.next.month&#x27;].value_counts()___/df[&#x27;default.payment.next.month&#x27;].value_counts()____ 2.3 数据可视化与分析2.3.1 数据分布情况首先对于对于连续型变量LIMIT_BAL，使用直方图查看数据分布状况。 12345678910sns.set(rc=&#123;&#x27;figure.figsize&#x27;:(10,5),&quot;font.size&quot;:15,&#x27;font.sans-serif&#x27;:[&#x27;SimHei&#x27;],&quot;axes.titlesize&quot;:15,&quot;axes.labelsize&quot;:15&#125;)plt.title(&#x27;信用额度分布图&#x27;)# 改变 matplotlib 颜色缩写词的解释方式sns.set_color_codes(&quot;pastel&quot;)# kde：是否绘制高斯核密度估计图 bins：直方图bins(柱)的数目sns._____(df[&#x27;LIMIT_BAL&#x27;],_____,bins=200, color=&quot;blue&quot;)plt.xlabel(&quot;信用额度&quot;)plt.rcParams[&#x27;figure.dpi&#x27;] = 1200 #为了使图片更加清晰plt.show() 1234567sns.set(rc=&#123;&#x27;figure.figsize&#x27;:(10,5),&quot;font.size&quot;:15,&#x27;font.sans-serif&#x27;:[&#x27;SimHei&#x27;],&quot;axes.titlesize&quot;:15,&quot;axes.labelsize&quot;:15&#125;)plt.title(&#x27;客户年龄分布图&#x27;)sns.set_color_codes(&quot;pastel&quot;)sns._____(_______,kde=True,bins=59, color=&quot;green&quot;)plt.xlabel(&quot;年龄&quot;)plt.show() 12345678910111213141516171819202122232425# 绘制多个子图，1行3列fig = plt._______(______,figsize=(16,12))sns.set(rc=&#123;&#x27;figure.figsize&#x27;:(10,5),&quot;font.size&quot;:15,&#x27;font.sans-serif&#x27;:[&#x27;SimHei&#x27;],&quot;axes.titlesize&quot;:15,&quot;axes.labelsize&quot;:15&#125;)# 绘制第一个子图plt._______sex_count = df[&#x27;SEX&#x27;].value_counts()# 画饼图，设置扇形标签，设置百分比显示格式，一位小数百分比sex_count.plot(kind=_____, labels=________, autopct=&#x27;%1.1f%%&#x27;)plt.ylabel(&quot;&quot;)plt.title(&#x27;性别分布图&#x27;)plt.subplot(1,3,2)edu_count = df[&#x27;EDUCATION&#x27;].value_counts()edu_count.plot(kind=&#x27;pie&#x27;, labels=[&#x27;本科&#x27;, &#x27;研究生或以上&#x27;,&#x27;高中&#x27;,&#x27;其他&#x27;], autopct=&#x27;%1.1f%%&#x27;)plt.title(&#x27;教育情况分布图&#x27;)plt.ylabel(&quot;&quot;)plt.subplot(1,3,3)mar_count = df[&#x27;MARRIAGE&#x27;].value_counts()mar_count.plot(kind=&#x27;pie&#x27;, labels=[&#x27;未婚&#x27;, &#x27;已婚&#x27;,&#x27;其他&#x27;], autopct=&#x27;%1.1f%%&#x27;)plt.title(&#x27;婚姻状况分布图&#x27;)plt.ylabel(&quot;&quot;)plt.show() 2.3.2 结合违约情况查看数据分布1234567891011121314151617181920212223242526272829303132sns.set(rc=&#123;&#x27;figure.figsize&#x27;:(13,5),&quot;font.size&quot;:15,&#x27;font.sans-serif&#x27;:[&#x27;SimHei&#x27;],&quot;axes.titlesize&quot;:15,&quot;axes.labelsize&quot;:15&#125;)plt.figure()# fig代表绘图窗口(Figure)；ax代表这个绘图窗口上的坐标系(axis)fig, ax = plt.subplots(1,3,figsize=(12,5))plt.subplot(1,3,1)# 条形图，以x轴标签划分统计个数，再以hue标签统计违约与未违约的个数ax=sns._______(x=_______, ______=&#x27;default.payment.next.month&#x27;,data = df)# 设置x轴不同标签名称ax._________([&#x27;男&#x27;, &#x27;女&#x27;],fontsize=12)# 设置x轴名称plt.xlabel(&quot;性别&quot;)plt.ylabel(&quot;&quot;)# 设置图的标题plt.title(&#x27;不同性别违约情况&#x27;)plt.subplot(1,3,2)ax=sns.countplot(x=&#x27;EDUCATION&#x27;,hue=&#x27;default.payment.next.month&#x27;,data = df)ax.set_xticklabels([&#x27;其他&#x27;, &#x27;高中&#x27;,&#x27;本科&#x27;,&#x27;研究生及以上&#x27;],fontsize=12)plt.xlabel(&quot;教育程度&quot;)plt.ylabel(&quot;&quot;)plt.title(&#x27;不同教育程度违约情况&#x27;)plt.subplot(1,3,3)ax=sns.countplot(x=&#x27;MARRIAGE&#x27;,hue=&#x27;default.payment.next.month&#x27;,data = df)ax.set_xticklabels([&#x27;其他&#x27;, &#x27;已婚&#x27;,&#x27;未婚&#x27;], fontsize=12)plt.xlabel(&quot;婚姻状况&quot;)plt.ylabel(&quot;&quot;)plt.title(&#x27;不同婚姻状况违约情况&#x27;)plt.show() 1234567891011121314151617181920212223242526t0 = df[df[&#x27;default.payment.next.month&#x27;] == 0] # 未违约的样本数据t1 = df[df[&#x27;default.payment.next.month&#x27;] == 1] # 违约的样本数据plt.figure()fig, ax = plt.subplots(1,2,figsize=(12,6))# 画第一个子图plt.subplot(1,2,1)# 画直方图，设置高斯核密度估计图参数，以字典传入核密度图的属性信息sns._______(______[&#x27;AGE&#x27;],_______=&#123;&quot;color&quot;:&quot;blue&quot;,&quot;label&quot;:&quot;未违约&quot;&#125;)sns._______(______[&#x27;AGE&#x27;],_______=&#123;&quot;color&quot;:&quot;orange&quot;,&quot;label&quot;:&quot;违约&quot;&#125;)# 设置x轴名称、字符大小plt.xlabel(&#x27;年龄&#x27;, fontsize=12)locs, labels = plt.xticks()plt.title(&#x27;不同年龄违约情况&#x27;)# 刻度线参数设置plt.tick_params(axis=&#x27;both&#x27;, which=&#x27;major&#x27;, labelsize=12)plt.subplot(1,2,2)sns.distplot(t0[&quot;LIMIT_BAL&quot;],kde_kws=&#123;&quot;color&quot;:&quot;blue&quot;,&quot;label&quot;:&quot;未违约&quot;&#125;)sns.distplot(t1[&quot;LIMIT_BAL&quot;],kde_kws=&#123;&quot;color&quot;:&quot;orange&quot;,&quot;label&quot;:&quot;违约&quot;&#125;)plt.xlabel(&#x27;信用卡额度&#x27;, fontsize=12)locs, labels = plt.xticks()plt.title(&#x27;不同信用卡额度违约情况&#x27;)plt.tick_params(axis=&#x27;both&#x27;, which=&#x27;major&#x27;, labelsize=12)plt.show() 3 分类建模3.1 KNN模型在KNN模型中，由于要计算欧氏距离，因此应该先对离散变量做哑变量编码处理，使其变为数值型特征我们对无序离散变量SEX,MARRIAGE进行哑变量编码。 123str_columns = [&#x27;SEX&#x27;,&#x27;MARRIAGE&#x27;]df_new = pd._______(df, columns=str_columns)df_new 12y = df_new[&#x27;default.payment.next.month&#x27;] #因变量X = df_new.______(columns=[&#x27;ID&#x27;,&#x27;default.payment.next.month&#x27;]) #去除因变量与ID，剩余为特征 对特征列进行标准化处理，使得处理过后的每一列数据都是均值为0，标准差为1的数据。 12X.iloc[:,0:20] = ___________(X.iloc[:,0:20])X.sample(5) 12print (&#x27;均值为：&#123;a&#125;&#x27;.format(a = X.LIMIT_BAL.mean()))print (&#x27;标准差为：&#123;a&#125;&#x27;.format(a = X.LIMIT_BAL.std())) 对数据进行训练集和测试集的划分。选取测试集大小为0.3，random_state随机种子。 123X_train, X_test,\\y_train, y_test \\= _________(X, y, test_size=______, random_state=0) 使用sklearn中的KNeighborsClassifier对训练集进行分类训练。当不输入参数时，默认KNN所选取的n_neighbors参数取值为5。 1234# 创建knn类knn_model = ___________# 对数据进行拟合学习knn_model._____(X_train, y_train) 得到模型knn_model后，我们在测试集上验证其分类效果用predict方法获取模型对每一个测试集样本标签的预测结果knn_pred用predict_proba方法获取模型把每一个样本划分为正类的概率knn_score 1234# 预测结果knn_pred = knn_model.______(X_test)# 属于正类的概率knn_score = knn_model._______(X_test)[:,1] 为了展示分类模型的效果，我们需要获取模型的准确率得分，分类报告以及混淆矩阵。sklearn中的accuracy_score用于计算准确率sklearn中的classification_report函数用于显示主要分类指标的文本报告．在报告中显示每个类的精确度，召回率，F1值等信息sklearn中的confusion_matrix总结模型真实值与预测值的情形分析表，以矩阵形式显示 为了方便后续使用，我们定义一个函数Get_report来获取这些信息。 12345678910111213def Get_report(testers , predictors): print (&#x27;模型的准确率为：&#123;a&#125;&#x27;.format(a=metrics._________(testers, predictors))) print (&#x27;模型的分类报告展示如下:&#x27;) print (metrics._________(testers, predictors)) print (&#x27;模型的混淆矩阵展示如下:&#x27;) plt.figure(figsize=(8,4)) ConfMatrix = __________(testers, predictors) sns.heatmap(ConfMatrix,annot=True, cmap=&quot;Blues&quot;, fmt=&quot;d&quot;, xticklabels = [&#x27;未违约&#x27;, &#x27;违约&#x27;], yticklabels = [&#x27;未违约&#x27;, &#x27;违约&#x27;]) plt.ylabel(&#x27;真实标签&#x27;) plt.xlabel(&#x27;预测标签&#x27;) plt.title(&quot;混淆矩阵&quot;) 1Get_report(y_test , knn_pred) 总体预测的精准度尚可，违约样本的实际召回率只有0.27，能够综合展示正类样本的准确率和召回率的f1-score只有0.36。我们再尝试其他模型进行对比。 3.2 决策树1234from sklearn import treefrom sklearn.tree import DecisionTreeClassifiertree_model = __________(random_state=1)tree_model.______(X_train, y_train) 123tree_score = tree_model.______(X_test)[:,1]tree_pred = tree_model._______(X_test)Get_report(_____,_______) 决策树相比KNN，在召回率和f1-score上有所提升。 4 数据过采样1df[&#x27;default.payment.next.month&#x27;].value_counts()[0]/df[&#x27;default.payment.next.month&#x27;].value_counts()[1] 负类和正类的比例在3.5左右，样本数据不平衡问题会导致二分类问题的难度较高。为了解决这个问题，引入SMOTE过采样算法，改变训练集中正负类别样本的分布。 123456789from imblearn.over_sampling import SMOTE# 划分数据集X_train, X_test,\\y_train, y_test \\= train_test_split(X, y, test_size=0.3, random_state=0)# 定义一个SMOTE模型smo = ________(random_state=42)# fit_resample拟合数据X_smo, y_smo = smo.__________(X_train, y_train) 5 引入新评价指标同时，针对不平衡样本的评价指标也需要进行相应的更新，我们可以引入Precision_Recall_curve、AP值、ROC_curve、G-mean，我们定一个Get_curve函数，展示这些曲线，并给出一系列对应的数值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def Get_curve (tester , scorer , predictionvalues): # PR曲线 precision, recall, thresholds = ___________(tester , scorer) # roc曲线 fpr,tpr,thresholds1 = metrics._________(tester , scorer) print(precision) print(recall) f1 = [] gmean = [] # 计算F1：2PR/P+R for i in range(len(precision)): f1.append(2*(precision[i]*recall[i])/(precision[i]+recall[i])) # 计算gmean:(tpr*(1-fpr))^1/2 for i in range(len(fpr)): gmean.append(np._____(tpr[i]*(1-fpr[i]))) plt.figure() fig, ax = plt.subplots(2,2,figsize=(12,3)) # 子图1 PR曲线 plt.subplot(1,3,1) # 横轴召回率，纵轴准确率 plt.plot(_______,_______) plt.title(&#x27;精准率-召回率曲线&#x27;) plt.xlabel(&#x27;召回率&#x27;) plt.ylabel(&#x27;精准率&#x27;) # 设置x轴作图范围 plt.xlim(0,1) # 子图2，roc曲线 plt.subplot(1,3,2) # 横轴假正率，纵轴真正率 plt.plot(_______,_______) plt.title(&#x27;ROC曲线&#x27;) plt.xlabel(&#x27;FPR&#x27;) plt.ylabel(&#x27;TPR&#x27;) # 子图3，gmean曲线 plt.subplot(1,3,3) # gmean根据阈值变化的曲线 plt.plot(________,_______) plt.title(&#x27;Gmean曲线&#x27;) plt.xlabel(&#x27;阈值&#x27;) plt.ylabel(&#x27;gmean值&#x27;) plt.xlim(0,1) plt.show() # 计算AP值 apscore = sklearn.metrics.__________(tester, scorer, average=&#x27;macro&#x27;, sample_weight=None) print(&quot;AP值:&#123;name&#125;&quot;.format(name=apscore)) # Gmean最大时的阈值 best_threshold = thresholds1[_______] # F1最大时的阈值 best_thresholds = thresholds[_______] print(&#x27;F1最大对应阈值 = &#123;a&#125;&#x27;.format(a = best_thresholds)) print(&#x27;Gmean最大对应阈值 = &#123;a&#125;&#x27;.format(a = best_threshold)) print(&#x27;准确率得分= &#123;a&#125;&#x27;.format(a = metrics.accuracy_score(tester, predictionvalues))) print(&#x27;最大Gmean = &#123;a&#125;&#x27;.format(a = max(gmean))) print(&#x27;最大F1 = &#123;a&#125;&#x27;.format(a = max(f1))) 我们传入上文中利用决策树模型得到的参数y_test,tree_score，tree_pred来测试这个函数的效果。 1Get_curve(y_test,tree_score,tree_pred) 6 集成学习6.1 XGBoost12345678# 划分数据集X_train, X_test,\\y_train, y_test \\= train_test_split(X, y, test_size=0.3, random_state=0)# 过采样处理smo = SMOTE(random_state=42)X_smo, y_smo = smo.fit_resample(X_train, y_train) 12345from xgboost import XGBClassifiermodel = _________(booster=&#x27;gbtree&#x27;,learning_rate=0.1,gamma=1,scale_pos_weight=1,n_estimators=1000,max_depth=6,alpha=5,reg_lambda=1)# 设置训练数据eval_set = [(X_test, y_test)]model._____(X_smo, y_smo, early_stopping_rounds=10, eval_metric=&quot;auc&quot;, eval_set=eval_set, verbose=False) learning_rate参数代表学习率，决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。alpha和reg_lambda分别对应L1，L2正则项系数。 12345from xgboost import XGBClassifiermodel = _________(booster=&#x27;gbtree&#x27;,learning_rate=0.1,gamma=1,scale_pos_weight=1,n_estimators=1000,max_depth=6,alpha=5,reg_lambda=1)# 设置训练数据eval_set = [(X_test, y_test)]model._____(X_smo, y_smo, early_stopping_rounds=10, eval_metric=&quot;auc&quot;, eval_set=eval_set, verbose=False) 我们调用Get_curve函数考察最终的结果。 1Get_curve(y_test,y_socre,y_pred) 6.2 CATBoost12345y = df[&#x27;default.payment.next.month&#x27;] # 因变量X = df.drop(columns=[&#x27;ID&#x27;,&#x27;default.payment.next.month&#x27;]) # 去除ID与因变量，剩余为特征X_train, X_test,\\y_train, y_test \\= train_test_split(X, y, test_size=0.3, random_state=0) 123456from sklearn.model_selection import GridSearchCVfrom catboost import CatBoostClassifiercategorical_features_indices = [&#x27;SEX&#x27;,&#x27;MARRIAGE&#x27;]model = ___________(iterations=100, depth=6, cat_features = categorical_features_indices, loss_function=&#x27;Logloss&#x27;, logging_level=&#x27;Silent&#x27;)model.fit(X_train,y_train,eval_set=(X_test, y_test)) 获取CatBoost模型的预测值以及预测为正类的概率。 123cb_pred = model.predict(X_test)cb_socre = model.predict_proba(X_test)[:,1]# predictions = [round(value) for value in cb_pred] 我们再获取使用CatBoost得到的评价指标，与之前的情况做对比。 1Get_curve(y_test, cb_socre, cb_pred) 1234THRESHOLD = __________# 大于阈值输出1，小于则输出0y_pred = _______(model.predict_proba(X_test)[:,1] &gt;= THRESHOLD, 1, 0)Get_report(y_test,y_pred) 123THRESHOLD = _________y_pred = np.where(model.predict_proba(X_test)[:,1] &gt;= THRESHOLD, 1, 0)Get_report(y_test,y_pred)"},{"title":"基于健康学习的机器诊断","path":"/2023/09/02/基于健康学习的机器诊断/","content":"1 数据读取与预处理1.1 数据集简介肝病患数据集包含416名肝病患者记录和167非肝病患者记录，数据集信息如下： 列名 数据类型 含义说明 Age Integer 患者的年龄 Gender String 患者的性别 TB Float 总胆红素 DB Float 直接胆红素 Alkphos Integer 碱性磷酸酶 Sgpt Integer 谷丙转氨酶 Sgot Integer 天冬氨酸氨基转移酶 TP Float 总蛋白 ALB Float 白蛋白 A&#x2F;G Ratio Float 白蛋白和球蛋白比率 label Integer 是否患病(1为患病，2为健康) 1.2 导入数据123456import pandas as pdimport numpy as np# 不显示警告import warningswarnings.filterwarnings(&#x27;ignore&#x27;) 查看数据集 12data = pd.read_csv(&quot;./dataset/Indian Liver Patient Dataset (ILPD).csv&quot;)data.head() 12# 查看数据信息data.info() Gender为唯一的object特征，我们需要提前处理为数值型，方便建立模型。使用.describe()对总体数据进行统计，查看常见的数据的统计学特征。 1data.describe(include=&#x27;all&#x27;) 1.3 数字编码对Gender进行数字编码。 12345678from sklearn.preprocessing import LabelEncoder# 数字编码le = LabelEncoder()# 数字编码data[&quot;Gender&quot;] = le.fit_transform(data[&quot;Gender&quot;][:])data 女性(Female)被编码为0，男性(Male)被编码为1。 1.4 缺失值填补A&#x2F;G Ratio存在缺失值，我们需要对其进行填补。在这里我们采用均值填补的方法进行填充。 1234567from sklearn.impute import SimpleImputer# 采用均值填补法imp = SimpleImputer(strategy=&quot;mean&quot;)data[&quot;A/G Ratio&quot;] = imp.fit_transform(data[&quot;A/G Ratio&quot;].to_frame())data.describe() 1.5 数据列数值转化本数据集的标签列label的值为 {1,2}，将其转化为{−1,1}或者{0,1}。 1234567891011# 定义label列calculate_col = &quot;label&quot;calculate_value = 2# 将label值从&#123;1，2&#125;，转化为&#123;1，0&#125;并更名为label_caldata[calculate_col + &#x27;_cal&#x27;] = calculate_value - data[calculate_col]# 删除label列data = data.drop(labels=&#x27;label&#x27;, axis=1)#查看转化后的数据集data 2 数据相关性探索2.1 年龄分布的直方图、性别数量的柱状图12345678910111213141516171819import matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline%config InlineBackend.figure_format = &#x27;svg&#x27;plt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;] # 用来正常显示中文标签plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False # 用来正常显示负号# 定义genders列genders = [&#x27;女性&#x27;, &#x27;男性&#x27;]fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 5))# 画出年龄分布直方图sns.histplot(data[&#x27;Age&#x27;], kde=False, palette=&quot;Set2&quot;, ax=ax1)# 画出性别分布柱状图sns.countplot(x=&#x27;Gender&#x27;, data=data, palette=&quot;Set2&quot;, ax=ax2)ax1.set(xlabel=&#x27;年龄&#x27;, ylabel=&#x27;数量&#x27;)ax2.set(xlabel=&#x27;性别&#x27;, ylabel=&#x27;数量&#x27;)ax2.set_xticklabels(genders) 2.2 年龄、性别与患病的分布情况12345678910111213141516171819202122# 定义两个图像fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 5))# 绘制年龄箱线图sns.boxplot(x=&#x27;label_cal&#x27;, y=&#x27;Age&#x27;, hue=&#x27;label_cal&#x27;, data=data, palette=&quot;Set2&quot;, ax=ax1)# 绘制性别柱状图sns.countplot(x=&#x27;Gender&#x27;, hue=&#x27;label_cal&#x27;, data=data, palette=&quot;Set2&quot;, ax=ax2)# 给图像加标题ax1.set_title(&#x27;年龄与患病情况的分布&#x27;, fontsize=13)ax2.set_title(&#x27;性别与患病情况的分布&#x27;, fontsize=13)# 给图像添加标签labels = [&#x27;不患病&#x27;, &#x27;患病&#x27;]ax1.set(xlabel=&#x27;是否患病&#x27;, ylabel=&#x27;年龄&#x27;)ax1.set_xticklabels(labels)ax1.legend_.remove()ax2.set(xlabel=&#x27;性别&#x27;, ylabel=&#x27;数量&#x27;)ax2.set_xticklabels(genders)ax2.legend([&#x27;不患病&#x27;, &#x27;患病&#x27;])plt.show() 2.3 各指标间相关性1234567891011121314151617181920def correlation_heatmap(df): hm, ax = plt.subplots(figsize=(12, 8))# 绘制热力图 hm = sns.heatmap( df.corr(), cmap=&#x27;Blues&#x27;, square=True, cbar_kws=&#123;&#x27;shrink&#x27;: .9&#125;, ax=ax, annot=True, linewidths=0.1, vmax=1.0, linecolor=&#x27;white&#x27;, annot_kws=&#123;&#x27;fontsize&#x27;: 12&#125; ) plt.title(&#x27;连续型对象间的皮尔森相关系数&#x27;, y=1.05, size=15)# 删掉Gender与label_cal列df = data.drop([&#x27;Gender&#x27;, &#x27;label_cal&#x27;], axis=1)correlation_heatmap(df) 针对两个离散型变量Gender和label_cal，我们使用卡方检验来观测其相关性，使用的方法是sklearn.feature_selection中的chi2方法： 1234567891011121314from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2# 定义两个新的数据框gender = pd.DataFrame(data[&#x27;Gender&#x27;])label_cal = pd.DataFrame(data[&#x27;label_cal&#x27;])# 使用卡方检验model1 = SelectKBest(chi2, k=1) # 选择k个最佳特征model1.fit_transform(gender, label_cal)# 打印得分print(&#x27;性别变量与是否得病之间的得分为：%.4f&#x27; % model1.scores_)print(&#x27;性别变量与是否得病之间的pvalue为：%.4f&#x27; % model1.pvalues_) 计算label_cal与其他连续型变量之间的关系，使用的方法为sklearn.feature_selection中的f_classif方法： 123456789101112131415from sklearn.feature_selection import f_classiffdata = pd.DataFrame(data.drop([&#x27;Gender&#x27;, &#x27;label_cal&#x27;], axis=1))# 删除gender列与label_cal列label_cal = pd.DataFrame(data[&#x27;label_cal&#x27;])# 使用f_classif计算label_cal与其他连续型变量间的关系F, p_val = f_classif(fdata, label_cal)# f分布的0.05分位数print(&#x27;各连续型变量的名称：&#x27;)print(fdata.columns.tolist())print(&#x27;各连续型变量与是否得病之间的F值为：&#x27;)print(F)print(&#x27;各连续型变量与是否得病之间的pvalue为：&#x27;)print(p_val) 从计算得出的P值能看出，除了TP的值为0.398，大于0.05以外，各个连续型变量与label_cal之间具有很高的的相关性。 3 构建分类模型3.1 数据集划分我们根据计算出的label_cal来对数据集进行划分。划分比例设置为测试集:训练集 = 20%:80%。 123456from sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_reportfrom sklearn.metrics import confusion_matrixfrom sklearn.metrics import accuracy_scorex = data.drop([&#x27;label_cal&#x27;], axis=1) # x为删除label_cal的数据集y = data[&#x27;label_cal&#x27;] # y为label_cal列 12# 划分数据集x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2, stratify=data[&#x27;label_cal&#x27;]) 3.2 逻辑回归逻辑回归是一种广义线性回归模型，在回归的基础上，使用Logistic函数将连续型的输出映射到（0，1）之间，用来解决分类问题。 在Python中，使用sklearn_model的LogisticRegression进行分类建模，使用的主要参数有： penalty ——可设为l1或者l2，代表L1和L2正则化，默认为l2。 class_weight ——class-weight用于指定样本各个类别的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。 random_state ——随机种子，设定为一个常数，保证每次运行的结果都是一样的。 我们使用这几个参数进行模型的构建，其中，class_weight为我们自己赋予的权重： 123456789101112from sklearn.linear_model import LogisticRegression# 为权重赋值weights = &#123;0: 1, 1: 1.3&#125;# 进行logistic回归lr = LogisticRegression(penalty=&#x27;l2&#x27;, random_state=8, class_weight=weights)lr.fit(x_train, y_train)# 对y进行预测y_predprb = lr.predict_proba(x_test)[:, 1]y_pred = lr.predict(x_test) 1234567891011121314from sklearn import metricsfrom sklearn.metrics import auc# 计算fpr，tpr及thresholds的值fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predprb)# 计算gmean的值gmean = np.sqrt(tpr*(1-fpr))# 计算最大的gmean值对应的thresholds值dictionary = dict(zip(thresholds,gmean))max_thresholds = max(dictionary, key=dictionary.get)print(&quot;最大的GMean值为：%.4f&quot;%(max(gmean)))print(&quot;最大的GMean对应的thresholds为：%.4f&quot;%(max_thresholds)) 1234from sklearn.metrics import roc_auc_score# 计算AUC值test_roc_auc = roc_auc_score(y_test, y_predprb)print(test_roc_auc) 12# 打印模型分类预测报告print(classification_report(y_test, y_pred)) 12345678# 画出混淆矩阵热力图cm1 = confusion_matrix(y_test, y_pred)plt.figure(figsize=(8, 6))sns.heatmap(cm1, annot=True, linewidths=.5, square=True, cmap=&#x27;Blues&#x27;)plt.ylabel(&#x27;Actual label&#x27;)plt.xlabel(&#x27;Predicted label&#x27;)all_sample_title = &#x27;ROC AUC Score: &#123;0&#125;&#x27;.format(round(test_roc_auc,2))plt.title(all_sample_title, size=15) 患病类别(label_cal=1)的召回率(Recall)达到0.93，且精确度(Precision)达到0.71，总体的平均F1_score达到0.45，是一个分类水平一般的模型。 3.3 决策树使用sklearn中的DecisionTreeClassifier算法来训练决策树模型。使用的主要参数有： max_depth：设置决策树的最大深度。为多次试验后设置的较好值。 class_weight：用于指定样本各个类别的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。设置为balanced时，能够保证样本量少的类别权重较高。 random_state ——随机种子，设定为一个常数，保证每次运行的结果都是一样的。 12345678910from sklearn.tree import DecisionTreeClassifierfrom sklearn import tree# 建立决策树模型model = DecisionTreeClassifier(random_state=5, class_weight=weights)model = model.fit(x_train, y_train)# 对y进行预测y_predict = model.predict(x_test)y_predprb = model.predict_proba(x_test)[:, 1] 123# 计算AUC值test_roc_auc = roc_auc_score(y_test, y_predprb)print(test_roc_auc) 12# 打印模型分类预测报告print(classification_report(y_test, y_predict)) 12345678# 绘制混淆矩阵热力图cm2 = confusion_matrix(y_test, y_predict)plt.figure(figsize=(9, 9))sns.heatmap(cm2, annot=True, linewidths=.5, square=True, cmap=&#x27;Blues&#x27;)plt.ylabel(&#x27;Actual label&#x27;)plt.xlabel(&#x27;Predicted label&#x27;)all_sample_title = &#x27;ROC AUC score: &#123;0&#125;&#x27;.format(round(test_roc_auc,2))plt.title(all_sample_title, size=15) 患病类别(label_cal=1)的召回率(Recall)达到0.71，且精确度(Precision)达到0.77，总体的平均F1_score达到0.58，分类水平一般。 3.4 随机森林在Python中，使用sklearn.ensemble的RandomForestClassifier进行分类建模，使用的主要参数有： n_estimator：训练分类器的数量，默认值为100。 max_depth：每棵树的最大深度，默认为3。 random_state：随机种子，设定为一个常数，保证每次运行的结果都是一样的。 class_weight：用于指定样本各个类别的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。 12345678910from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import cross_val_score# 建立随机森林模型ran_for = RandomForestClassifier(n_estimators=80, random_state=0, class_weight=weights)ran_for.fit(x_train, y_train)# 对y进行预测y_pred_ran = ran_for.predict(x_test)y_predprb = ran_for.predict_proba(x_test)[:, 1] 123# 计算AUC值test_roc_auc = roc_auc_score(y_test, y_predprb)print(test_roc_auc) 12# 打印模型分类预测报告print(classification_report(y_test, y_pred_ran, digits=2)) 12345678# 绘制混淆矩阵热力图cm3 = confusion_matrix(y_test, y_pred_ran)plt.figure(figsize=(9, 9))sns.heatmap(cm3, annot=True, linewidths=.5, square=True, cmap=&#x27;Blues&#x27;)plt.ylabel(&#x27;Actual label&#x27;)plt.xlabel(&#x27;Predicted label&#x27;)all_sample_title = &#x27;ROC AUC score: &#123;0&#125;&#x27;.format(round(test_roc_auc,2))plt.title(all_sample_title, size=15) 患病类别(label_cal=1)的召回率(Recall)达到0.80，且精确度(Precision)达到0.77，总体的平均F1_score达到0.61。 可以得到随机森林模型的分类效果高于逻辑回归与决策树。 3.5 主成分分析PCA降维是一种常见的数据降维方法，其目的是在“信息”损失较小的前提下，将高维的数据转换到低维，从而减小计算量。PCA通常用于高维数据集的探索与可视化，还可以用于数据压缩，数据预处理等。主成分分析必须从相同量纲的变量表格开始。由于需要将变量总方差分配给特征根，因此变量必须有相同的物理单位，方差才有意义（方差的单位是变量单位的平方）。主成分分析的变量也可以是无量纲的数据，例如标准化或对数转化后的数据。因此在构建模型之前，我们需要进行数据标准化。常用的标准化方法有 min-max 标准化和 z-score 标准化等。在本例中，我们直接采用 z-score 标准化方法。 对数据进行标准化处理： 123456789from sklearn import preprocessingX = data.iloc[:, :-1] # 除了label_cal列y = data[&#x27;label_cal&#x27;]np.random.seed(123)perm = np.random.permutation(len(X)) # 将数组随机生成一个新的序列X = X.loc[perm]y = y[perm]X = preprocessing.scale(X)# 进行标准化处理 1234567891011from sklearn.decomposition import PCA# 使用PCA进行降维pca = PCA(copy=True, n_components=6, whiten=False, random_state=1)X_new = pca.fit_transform(X)print(u&#x27;所保留的6个主成分的方差贡献率为：&#x27;)print(pca.explained_variance_ratio_)print(u&#x27;排名前2的主成分特征向量为：&#x27;)print(pca.components_[0:1])print(u&#x27;累计方差贡献率为：&#x27;)print(sum(pca.explained_variance_ratio_)) 12# 对数据集进行划分x_train, x_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=2, stratify=data[&#x27;label_cal&#x27;]) 12345678# 建立随机森林模型ran_for = RandomForestClassifier(n_estimators=80, random_state=0, class_weight=weights)# 训练模型ran_for.fit(x_train, y_train)# 对y进行预测y_pred_ran = ran_for.predict(x_test)y_predprb = ran_for.predict_proba(x_test)[:, 1] 123# 计算AUC值test_roc_auc = roc_auc_score(y_test, y_predprb)print(test_roc_auc) 12# 打印模型分类预测报告print(classification_report(y_test, y_pred_ran, digits=2)) 12345678# 绘制混淆矩阵热力图cm4 = confusion_matrix(y_test, y_pred_ran)plt.figure(figsize=(9, 9))sns.heatmap(cm4, annot=True, linewidths=.5, square=True, cmap=&#x27;Blues&#x27;)plt.ylabel(&#x27;Actual label&#x27;)plt.xlabel(&#x27;Predicted label&#x27;)all_sample_title = &#x27;ROC AUC score: &#123;0&#125;&#x27;.format(round(test_roc_auc,2))plt.title(all_sample_title, size=15) 降维之后的随机森林模型实现了较为显著的提升，能够实现测试集上患病类别(label_cal=1)的召回率(Recall)达到0.85，且精确度(Precision)达到0.83，总体的平均F1_score达到0.65。"},{"title":"随机森林分析","path":"/2023/08/31/随机森林分析/","content":"准备数据共计1059条，各数据字段含义如下表所示： 字段 含义 case 国家编号，代表特定国家的数字 cc3 国家代码，三个字母的国家&#x2F;地区代码 country 国家名称 year 观测年份 systemic_crisis 系统性危机，“ 0”表示当年未发生系统性危机，“ 1”表示当年有发生系统性危机 exch_usd 该国货币兑美元的汇率 domestic_debt_in_default 国内债务违约，“0”表示当年未发生国内债务违约，“1”表示当年有发生国内债务违约 sovereign_external_debt_default 主权外债违约，“0”表示当年未发生主权外债违约，“1”表示当年有发生主权外债违约 gdp_weighted_default 违约债务总额与GDP之比 inflation_annual_cpi 年度CPI通货膨胀率 independence 独立性，“ 0”表示“无独立性”，“ 1”表示“独立性” currency_crises 货币危机，“ 0”表示当年未发生“货币危机”，“ 1”表示当年有发生“货币危机” inflation_crises 通胀危机，“ 0”表示当年未发生“通胀危机”，“ 1”表示当年有发生“通胀危机” banking_crisis 银行业危机，“ no_crisis”表示当年没有发生银行业危机，而“ crisis”表示当年有发生银行业危机 1 数据读取与预处理1.1 读取数据12345678910111213141516171819202122# 导入相应模块import numpy as np import pandas as pdimport matplotlib.pyplot as pltfrom matplotlib.font_manager import FontProperties# 设置字体font = FontProperties(fname = &quot;./dataset/SimHei.ttf&quot;, size=14)import seaborn as snsimport random# 设置绘图风格%matplotlib inlinesns.set(style=&#x27;whitegrid&#x27;)# 忽略所有警告import warningswarnings.filterwarnings(&#x27;ignore&#x27;)# 读取数据data = pd.read_csv(&#x27;./dataset/african_crises.csv&#x27;)data.sample(5) 1.2 查看数据12unique_countries = data.country.unique()unique_countries 12# 数据集的基本信息data.______() 12# 查看数据统计性指标data.____________(include = &#x27;all&#x27;) 1.3 数据预处理12#查看货币危机currency_crises的取值为2的数据data[data[&#x27;currency_crises&#x27;] == 2] 12data = data[data[&#x27;currency_crises&#x27;] != 2]# 得到生成删除货币危机currency_crises的取值为2的数据集data.______ # 查看新生成的数据集大小 2 经济指标探索性分析2.1 汇率变化情况1234567891011121314151617181920212223plt.figure(figsize=(12,20))for i in range(13): plt.subplot(7,2,i+1) country = unique_countries[i] # 随机生成一种颜色 random.choice():从一个序列中随机的抽取一个元素，抽取6次组成6位代表随机颜色 col=&quot;#&quot;+&#x27;&#x27;.join([random.choice(&#x27;0123456789ABCDEF&#x27;) for j in range(6)]) # 绘制折线图 sns.____________(data[data.country == country][&#x27;year&#x27;],data[data.country == country][&#x27;exch_usd&#x27;],label = country,color = col) # np.logical_and()逻辑与 两个条件均成立时返回True plt.plot([np.min(data[np.logical_and(data.country == country,data.independence == 1)][&#x27;year&#x27;]), np.min(data[np.logical_and(data.country == country,data.independence == 1)][&#x27;year&#x27;])], [0,np.max(data[data.country == country][&#x27;exch_usd&#x27;])],color = &#x27;black&#x27;,linestyle = &#x27;dotted&#x27;,alpha = 0.8) plt.______(country) # 添加图像标题 plt.tight_layout() # 自动调整子图参数以提供指定的填充plt.show() # 输出13个国家的货币兑美元的汇率变化情况的折线图 2.2 通货膨胀率变化情况12345678910111213141516171819202122232425plt.figure(figsize=(12,20))for i in range(13): plt.subplot(7,2,i+1) country = unique_countries[i] # 随机生成一种颜色 col=&quot;#&quot;+&#x27;&#x27;.join([random.choice(&#x27;0123456789ABCDEF&#x27;) for j in range(6)]) # 绘制折线图 sns.lineplot(data[data.country == country][&#x27;year&#x27;],data[data.country == country][&#x27;inflation_annual_cpi&#x27;],label = country,color = col) # 加入散点图 plt.______(data[data.country == country][&#x27;year&#x27;],data[data.country == country][&#x27;inflation_annual_cpi&#x27;],color = col,s = 28) # s指散点的面积 plt.plot([np.min(data[np.logical_and(data.country == country,data.independence==1)][&#x27;year&#x27;]), np.min(data[np.logical_and(data.country == country,data.independence==1)][&#x27;year&#x27;])], [np.min(data[data.country == country][&#x27;inflation_annual_cpi&#x27;]),np.max(data[data.country == country][&#x27;inflation_annual_cpi&#x27;])], color = &#x27;black&#x27;,linestyle = &#x27;dotted&#x27;,alpha = 0.8) # alpha指颜色透明度 plt.title(country) plt.tight_layout() # 自动调整子图参数以提供指定的填充plt.show() # 输出13个国家的通货膨胀率变化情况 2.3 其他危机分布情况123456789101112131415sns.set(style=&#x27;darkgrid&#x27;)columns = [&#x27;systemic_crisis&#x27;,&#x27;domestic_debt_in_default&#x27;,&#x27;sovereign_external_debt_default&#x27;,&#x27;currency_crises&#x27;,&#x27;inflation_crises&#x27;,&#x27;banking_crisis&#x27;]# 绘制其他特征的分布规律图plt.figure(figsize=(16,16))for i in range(6): plt.subplot(&#x27;32&#x27;+str(i+1)) sns.countplot(y = data.country,hue = data[columns[i]],palette = &#x27;rocket&#x27;) # palette为调色板 plt.______(loc = 0) # 选择最优的图例位置 plt.title(columns[i]) plt.tight_layout()plt.show() 2.4 特征间的相关性123456789101112131415161718# 创建包含年份，国家，系统性危机，银行危机的数据集systemic = data[[&#x27;year&#x27;,&#x27;country&#x27;, &#x27;systemic_crisis&#x27;, &#x27;banking_crisis&#x27;]]# 绘制观察系统性危机与银行危机发生的重叠性systemic = systemic[(systemic[&#x27;country&#x27;] == &#x27;Central African Republic&#x27;) | (systemic[&#x27;country&#x27;]==&#x27;Kenya&#x27;) | (systemic[&#x27;country&#x27;]==&#x27;Zimbabwe&#x27;) ]plt.figure(figsize=(12,12))count = 1for country in systemic.country.unique(): plt.subplot(len(systemic.country.unique()),1,count) subset = systemic[(systemic[&#x27;country&#x27;] == country)] sns.lineplot(subset[&#x27;year&#x27;],subset[&#x27;systemic_crisis&#x27;],ci=None) # ci参数可用于指定线段区间的大小 plt.scatter(subset[&#x27;year&#x27;],subset[&quot;banking_crisis&quot;], color=&#x27;coral&#x27;, label=&#x27;Banking Crisis&#x27;) plt.subplots_adjust(hspace=0.6) # hspace用来设置子图上下间的距离 plt.______(&#x27;Years&#x27;) # 给x轴命名 plt.______(&#x27;Systemic Crisis/Banking Crisis&#x27;) # 给y轴命名 plt.title(country) count+=1 12345678910111213141516171819202122# 将银行危机banking_crisis列进行特征编码# 将银行危机banking_crisis中未发生危机的数据标为0，发生危机的数据标为1data[&#x27;banking_crisis&#x27;] = data[&#x27;banking_crisis&#x27;].map(&#123;&quot;no_crisis&quot;:0,&quot;crisis&quot;:1&#125;)# 选出所有特征selected_features = [&#x27;systemic_crisis&#x27;, &#x27;exch_usd&#x27;, &#x27;domestic_debt_in_default&#x27;,&#x27;sovereign_external_debt_default&#x27;, &#x27;gdp_weighted_default&#x27;, &#x27;inflation_annual_cpi&#x27;, &#x27;independence&#x27;, &#x27;currency_crises&#x27;,&#x27;inflation_crises&#x27;,&#x27;banking_crisis&#x27;]corr = data[selected_features].______() # 得到各特征间的相关性大小生成相关性矩阵fig = plt.figure(figsize = (12,8))cmap = sns.diverging_palette(220, 10, as_cmap=True) # 生成蓝-白-红的颜色列表mask = np.zeros_like(corr, dtype=np.bool) # 返回与相关性矩阵具有相同形状和类型的零数组作为掩码mask[np.triu_indices_from(mask)] = True # 给相关性矩阵的上三角阵生成掩码# 绘制热力图sns.______(corr, mask=mask, cmap=cmap,vmin=-0.5,vmax=0.7, center=0,annot = True, square=True, linewidths=.5,cbar_kws=&#123;&quot;shrink&quot;: .5&#125;);plt.title(&quot;特征间的相关性&quot;,fontproperties = font)plt.show() 3 构建银行危机预测模型 特征编码 数据集划分与分层采样 建立随机森林预测模型 模型效果的评估 使用SMOTE进行过采样优化模型 特征重要性排序 3.1 特征编码12data.drop([&#x27;case&#x27;,&#x27;cc3&#x27;],axis = 1,inplace = True) # 在原数据集上删掉case列和cc3列data.head() 12345# 对国家country进行labelencoderfrom sklearn.preprocessing import LabelEncoderle = LabelEncoder()le.______(data[&#x27;country&#x27;].values) # 将country的值塞入空字典data[&#x27;country&#x27;]=le.____________(data[&#x27;country&#x27;].values) # 将字典中的country的值转变为索引值 1print(data[&#x27;country&#x27;]) # 查看特征编码后的country名称 1234567# 绘制未发生银行危机no_crisis与发生银行危机crisis的柱状图fig = plt.figure(figsize = (8,6))data[&#x27;banking_crisis&#x27;].value_counts().plot(kind=&#x27;______&#x27;,rot = 360,color = &#x27;lightseagreen&#x27;)plt.xticks([0,1],[&quot;no_crisis&quot;,&quot;crisis&quot;])plt.show() 3.2 数据集划分与分层采样下面我们开始对数据进行训练集与测试集的划分。在Sklearn中的model_selection模块，存在train_test_split()函数，用作训练集和测试集划分，函数语法为：train_test_split(x,y,test_size = None,random_state = None,stratify = y)，其中： x,y: 分别为预测所需的所有特征，以及需要预测的特征。 test_size: 测试集比例，例如test_size=0.2则表示划分20%的数据作为测试集。 random_state: 随机种子，因为划分过程是随机的，为了进行可重复的训练，需要固定一random_state，结果重现。 stratify: 使用分层采样，保证从发生银行危机样本和未发生银行危机样本中抽取了同样比例的训练集和测试集。 函数最终将返回四个变量，分别为x的训练集和测试集，以及y的训练集和测试集。 1234567# 训练集与测试集的划分from sklearn import model_selectionx = data.drop(&#x27;banking_crisis&#x27;,axis = 1) # 将删除banking_crisis列的数据集作为xy = data[&#x27;banking_crisis&#x27;] # banking_crisis列作为yx_train,x_test,y_train,y_test = model_selection.__________________(x, y,test_size=0.2,random_state = 33,stratify=y) 3.3 建立随机森林预测模型随机森林是一种集成学习方法，通过使用随机的方式从数据中抽取样本和特征，训练多个不同的决策树，形成“森林”。每个树都给出自己的分类意见，称“投票”。在分类问题下，森林选择选票最多的分类；在回归问题下则使用平均值。在Python中使用sklearn.ensemble的RandomForestClassifier构建分类模型，其主要参数包括： n_estimators : 训练分类器的数量(默认为100)； max_depth : 每棵树的最大深度(默认为3)； max_features: 划分的最大特征数(默认为 ‘auto’) random_state : 随机种子。 123456from sklearn.ensemble import RandomForestClassifier# 训练随机森林分类模型rf = RandomForestClassifier(n_estimators = 100, max_depth = 20,max_features = 10, random_state = 20)rf.fit(x_train, y_train) y_pred = rf.______(x_test) # 对y进行预测 3.4 模型评估在评价模型好坏时，我们分别使用函数classification_report()、confusion_matrix()和accuracy_score(),用于输出模型的预测报告、混淆矩阵和分类正确率。 12345678910111213from sklearn.metrics import classification_report,confusion_matrixprint(classification_report(y_test, y_pred)) # 输出模型的预测报告confusion_matrix = __________________(y_test, y_pred) print(confusion_matrix) # 输出混淆矩阵# 绘制混淆矩阵热力图fig,ax = plt.subplots(figsize=(8,6)) sns._________(confusion_matrix,ax=ax,annot=True,annot_kws=&#123;&#x27;size&#x27;:15&#125;, fmt=&#x27;d&#x27;,cmap = &#x27;YlGnBu_r&#x27;)ax.set_ylabel(&#x27;真实值&#x27;,fontproperties = font)ax.set_xlabel(&#x27;预测值&#x27;,fontproperties = font)ax.set_title(&#x27;混淆矩阵热力图&#x27;,fontproperties = font)plt.show() # 输出混淆矩阵热力图 12345678910111213141516171819from sklearn.metrics import roc_auc_scorefrom sklearn.metrics import roc_curveroc_auc = ____________(y_test, rf.predict(x_test)) #计算auc的值fpr, tpr, thresholds = ____________(y_test, rf.predict_proba(x_test)[:,1]) #计算不同阈值下的TPR和FPR# 绘制ROC曲线plt.figure(figsize = (8,6))plt.plot(fpr, tpr, label=&#x27;Random Forest (area = %0.2f)&#x27; % roc_auc)plt.plot([0, 1], [0, 1],&#x27;r--&#x27;)# 绘制随机猜测线plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel(&#x27;False Positive Rate&#x27;)plt.ylabel(&#x27;True Positive Rate&#x27;)plt.title(&#x27;ROC curve&#x27;)plt.legend(loc=&quot;lower right&quot;) # 图例位置位于右下方plt.show() 3.5 使用SMOTE进行采样优化模型在对数据集进行划分，接着对训练集进行过采样，将少数类进行扩充。在Python中使用imblearn.over_sampling的SMOTE类构建SMOTE过采样模型。 12345# 对x_train,y_train进行SMOTE过采样from imblearn.over_sampling import SMOTEx_train_resampled, y_train_resampled = SMOTE(random_state=4).fit_resample(x_train, y_train)print(x_train_resampled.shape, y_train_resampled.shape) #查看采样后的数据集大小 1234567# 通过网格搜索选择最优参数from sklearn.model_selection import GridSearchCVparam_grid = [&#123; &#x27;n_estimators&#x27;:[10,20,30,40,50], &#x27;max_depth&#x27;:[5,8,10,15,20,25]&#125;]grid_search = GridSearchCV(rf, param_grid, scoring = &#x27;recall&#x27;) 123456# 输出最佳参数组合以及分数grid_search.fit(x_train_resampled, y_train_resampled)print(&quot;best params:&quot;, grid_search.best_params_)print(&quot;best score:&quot;, grid_search.best_score_) 3.6 特征重要性排序123456789101112131415fig = plt.figure(figsize=(16,12))# 得到随机森林特征重要性评分rf_importance = rf.__________________index = data.drop([&#x27;banking_crisis&#x27;], axis=1).columns # 删掉银行危机banking_crisis列特征# 对得到的特征重要性评分进行降序排序rf_feature_importance = pd.DataFrame(rf_importance.T, index=index,columns=[&#x27;score&#x27;]).sort_values(by=&#x27;score&#x27;, ascending=True)# 水平条形图绘制rf_feature_importance.plot(kind=&#x27;______&#x27;,legend=False,color = &#x27;deepskyblue&#x27;)plt.title(&#x27;随机森林特征重要性&#x27;,fontproperties = font)plt.show()"},{"title":"商品评论情感判定","path":"/2023/08/31/商品评论情感判定/","content":"准备用户在电商平台上面发表的产品评价中包含着用户的偏好信息，所以通过用户评论，可以得到用户的情感倾向以及对产品属性的偏好。 1234567891011121314# 载入必要库import jiebaimport numpy as npimport pandas as pdimport sklearnimport matplotlibimport matplotlib.pyplot as plt import pyecharts.options as optsfrom pyecharts.charts import WordCloudfrom pyecharts.charts import Barimport re#loggingimport warningswarnings.filterwarnings(&#x27;ignore&#x27;) 1 数据读取1.1 读取数据某款手机的商品评论信息数据集，包含2个字段，共计8186个样本。数据集描述如下： 列名 说明 类型 Comment 对该款手机的评论 String Class 该评论的情感倾向: -1 —— 差评 0 —— 中评 1 —— 好评 Int 使用Pandas库中read_csv()函数读取数据。 123#读入数据集data = pd.read_csv(&#x27;./dataset/data.csv&#x27;)data.head(10) 1.2 查看数据12# 数据集的大小data.shape 12# 数据集的基本信息data.info() 2 数据预处理使用分词库jieba中的cut()函数对文本进行分词。 2.1 去除缺失值12# 移除含有缺失值的行data.dropna(axis=0,inplace=True) 12#查看去除缺失值后的行和列data.shape 2.2 分词去除标点数字字母后再分词。 1234def remove_url(src): # 去除标点符号、数字、字母 vTEXT = re.sub(&#x27;[a-zA-Z0-9’!&quot;#$%&amp;\\&#x27;()*+,-./:;&lt;=&gt;?@，。?★、…【】╮ ￣ ▽ ￣ ╭\\\\～⊙％；①（）：《》？“”‘’！[\\\\]^_`&#123;|&#125;~\\s]+&#x27;, &quot;&quot;, src) return vTEXT 12345678910111213cutted = []for row in data.values: text = remove_url(str(row[0])) #去除文本中的标点符号、数字、字母 raw_words = (&#x27; &#x27;.join(jieba.cut(text)))#分词,并用空格进行分隔 cutted.append(raw_words)cutted_array = np.array(cutted)# 生成新数据文件，Comment字段为分词后的内容data_cutted = pd.DataFrame(&#123; &#x27;Comment&#x27;: cutted_array, &#x27;Class&#x27;: data[&#x27;Class&#x27;]&#125;) 12#查看分词后的数据集data_cutted.head() data_cutted为进行分词之后的数据集。 2.3 查看关键词读取停用词文件，再用jieba.analyse中的set_stop_words函数设置停用词，使用extract_tags函数提取关键词。 12345with open(&#x27;./dataset/stopwords.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:#读停用词表 stopwords = [item.strip() for item in f] #通过列表推导式的方式获取所有停用词 for i in stopwords[:100]:#读前100个停用词 print(i,end=&#x27;&#x27;) 123#设定停用词文件,在统计关键词的时候，过滤停用词import jieba.analysejieba.analyse.set_stop_words(&#x27;./dataset/stopwords.txt&#x27;) 1data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == 1] 1234# 好评关键词keywords_pos = jieba.analyse.extract_tags(&#x27;&#x27;.join(data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == 1]),withWeight = True,topK=30)for item in keywords_pos: print(item[0],end=&#x27; &#x27;) 1234#中评关键词keywords_med = jieba.analyse.extract_tags(&#x27;&#x27;.join(data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == 0]),withWeight = True,topK=30)for item in keywords_med: print(item[0],end=&#x27; &#x27;) 12345#差评关键词keywords_neg = jieba.analyse.extract_tags(&#x27;&#x27;.join(data_cutted[&#x27;Comment&#x27;][data_cutted[&#x27;Class&#x27;] == -1]),withWeight = True,topK=30)for item in keywords_neg: print (item[0],end=&#x27; &#x27;) 3 可视化分析使用Pyecharts进行绘图。 3.1 评价柱状图1data_cutted[&#x27;Class&#x27;].value_counts() 123456789101112# 不同类别数据记录的统计x_label = [&#x27;好评&#x27;,&#x27;差评&#x27;,&#x27;中评&#x27;]class_num = ( Bar() #设置x轴的值 .add_xaxis(x_label) #设置y轴数据 .add_yaxis(&quot;&quot;,data_cutted[&#x27;Class&#x27;].value_counts().to_list(),color=[&#x27;#4c8dae&#x27;]) #设置title .set_global_opts(title_opts=opts.TitleOpts(title=&quot;好评、中评、差评数量柱状图&quot;)))class_num.render_notebook() 3.2 好评云图123456789101112wordcloud_pos = ( WordCloud() #data_pair：要绘制词云图的数据 .add(series_name=&quot;&quot;, data_pair=keywords_pos[:], word_size_range=[10, 66]) .set_global_opts( title_opts=opts.TitleOpts( #设置词云图标题和标题字号 title=&quot;好评关键词词云图&quot;, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True)) )wordcloud_pos.render_notebook() 3.3 中评云图123456789101112wordcloud_med = ( WordCloud() #data_pair：要绘制词云图的数据 .add(series_name=&quot;&quot;, data_pair=keywords_med[:], word_size_range=[10, 66]) .set_global_opts( title_opts=opts.TitleOpts( #设置词云图标题和标题字号 title=&quot;中评关键词词云图&quot;, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True)) )wordcloud_med.render_notebook() 3.4 差评云图123456789101112wordcloud_neg = ( WordCloud() #data_pair：要绘制词云图的数据 .add(series_name=&quot;&quot;, data_pair=keywords_neg[:], word_size_range=[10, 66]) .set_global_opts( title_opts=opts.TitleOpts( #设置词云图标题和标题字号 title=&quot;差评关键词词云图&quot;, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True)) )wordcloud_neg.render_notebook() 4 文本向量化经过分词之后的文本数据集要先进行向量化之后才能输入到分类模型中进行运算。TF-IDF算法是常用的文本向量化算法。 TF-IDF是Term Frequency-Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。TF-IDF是一种统计方法，用以评估一个词对于一个文件集或一个语料库中的一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。使用sklearn库中的TfidfVectorizer实现tf-idf文本向量化。 1234567# 实现向量化方法from sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(stop_words = stopwords,max_df=2000,min_df=6)#将文本向量化后的数据赋给data_transformdata_transform = vectorizer.fit_transform(data_cutted[&#x27;Comment&#x27;].values.tolist()) 12#文本的词汇表vectorizer.get_feature_names() 12#调用toarray()方法查看文本向量化后的数据data_transform.toarray() 1data_transform.shape 5 高斯朴素贝叶斯模型5.1 数据集划分使用sklearn.model_selection模块的train_test_split()函数划分训练集和测试集。 123from sklearn.model_selection import train_test_split #数据集划分X_train, X_test, y_train, y_test = train_test_split(data_transform, data_cutted[&#x27;Class&#x27;], random_state=10,test_size=0.2) 5.2 构建高斯朴素贝叶斯模型从sklearn.naive_bayes中导入GaussianNB类，使用GaussianNB类初始化一个模型对象，命名为gnb，对gnb调用fit方法，带入训练集X_train，y_train进行训练。 1from sklearn.naive_bayes import GaussianNB 12gnb = GaussianNB()gnb_model = gnb.fit(X_train.toarray(),y_train) 5.3 高斯朴素贝叶斯模型评估从sklearn.metrics中导入classification_report分类报告用于模型评估. 1from sklearn.metrics import classification_report 123gnb_prelabel = gnb_model.predict(X_test.toarray())print(classification_report(y_true=y_test,y_pred=gnb_prelabel)) 6 SVM模型构建6.1 构建SVM模型从sklearn.svm中导入SVC类，使用SVC类初始化一个模型对象，命名为svc，对svc调用fit方法，带入训练集X_train，y_train进行训练。 12345from sklearn.svm import SVC#设置kernel为‘rbf’高斯核，C=1svc = SVC(kernel=&#x27;rbf&#x27;, C=1)svc_model = svc.fit(X_train,y_train) 训练模型后，可以使用模型在测试集X_test上作出预测。 6.2 模型评估12svc_prelabel = svc_model.predict(X_test)print(classification_report(y_true=y_test,y_pred=svc_prelabel)) 通过将SVM模型与构建的高斯朴素贝叶斯模型分类结果比较，可以看出SVM在分类的精确率、召回率，以及模型的准确率上都优于高斯朴素贝叶斯模型。"},{"title":"汽车产品聚类分析","path":"/2023/08/31/汽车产品聚类分析/","content":"内容根据各种汽车参数利用聚类算法进行聚类，识别出相似的汽车，汽车款式数据集中有392个样本，8个特征。说明如下： 字段名称 字段类型 字段说明 mpg 浮点型 每加仑英里 cylinders 整型 气缸数在4到8之间 displacement 浮点型 发动机排量（立方英寸） horsepower 整型 引擎马力 weight 整型 车重（磅） acceleration 浮点型 从0加速到60 mph（秒）的时间 year 整型 型号年份 origin 整型 汽车来源（1.美国，2.欧洲，3.日本） 1 准备工作导入相关包和模块。 123# 忽略警告信息import warningswarnings.filterwarnings(&quot;ignore&quot;) 1234 #导入相关的包和模块import numpy as npimport pandas as pdfrom sklearn import metrics 12import sklearnsklearn.__version__ 1np.__version__ 123#读入数据集并查看数据集的前五行信息auto = pd.read_csv(&#x27;./dataset/Auto.csv&#x27;) auto.head() 12#查看数据集基本信息auto.info() 2 数据预处理1auto.head() 对mpg,displacement,horsepower,weight,acceleration五个连续型变量用Z-score标准化，对cylinders,year,origin三个类别变量进行哑变量编码。 1234#Z-score标准化from sklearn.preprocessing import StandardScalerscaler = StandardScaler()auto[[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]] =scaler.fit_transform(auto[[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]]) 1StandardScaler.__version__ 1auto.head() 1pd.get_dummies? 12#哑变量编码auto_scaled = pd.get_dummies(data=auto,columns=[&#x27;cylinders&#x27;,&#x27;year&#x27;,&#x27;origin&#x27;]) 12#查看进行标准化处理和哑变量编码后数据集的前两行auto_scaled.head(2) 3 K-Means聚类K-Means聚类基于点与点之间的距离的相似度来计算最佳类别归属。其核心目标是将给定的数据集划分成K个簇，并给出每个样本数据对应的中心点。 123#导入K-Means聚类方法，将初始K值设为3，随机种子数设为0，并训练模型from sklearn.cluster import KMeansmodel = KMeans(n_clusters=3,random_state=0).fit(auto_scaled) 1KMeans? 12345#将样本标签和簇质心保存在auto_lable和auto_cluster中,展示分类结果标签的前十行，以及簇质心的第一行auto_label = model.labels_auto_cluster = model.cluster_centers_print(auto_label[:10])print(auto_cluster[0]) 统计聚类结果，根据我们设置的初始K值，结果将所有样本分为三类，并按照每一类的数量进行降序展示： 1pd.value_counts(auto_label) 1pd.value_counts? 12345678#找出簇质心连续性变量的坐标centroid_cluster = pd.DataFrame(auto_cluster).copy().iloc[:,:5]centroid_cluster.columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]#将数据逆标准化，转换为原始数据,结果保留两位小数：centroid_cluster_inversescale = pd.DataFrame(scaler.inverse_transform(centroid_cluster))centroid_cluster_inversescale.columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;]centroid_cluster_inversescale.applymap(lambda x:&#x27;%.2f&#x27;%x) 选择轮廓系数作为聚类性能的评估指标，轮廓系数取值范围为[-1,1]，轮廓系数为-1时表示聚类结果不好，为+1时表示簇内实例之间紧凑，为0时表示有簇重叠。轮廓系数越大，表示簇内实例之间紧凑，簇间距离大，即聚类的效果越好。 1pd.value_counts(auto_label) 1234#轮廓系数结果保留四位小数labels = model.labels_print(&quot;轮廓系数(Silhouette Coefficient): %0.4f&quot; % metrics.silhouette_score(auto_scaled, labels)) 1metrics.silhouette_score? 1234for i in range(2,12,2): model = KMeans(n_clusters=i,random_state=0).fit(auto_scaled) labels = model.labels_ print(&quot;轮廓系数(Silhouette Coefficient): %0.4f&quot;% metrics.silhouette_score(auto_scaled, labels)) 根据结果可以看到当k值取2时，其轮廓系数最大，即聚类效果最佳。 4 DBSCAN聚类DBSCAN是一种典型的基于密度的聚类算法，在DBSCAN算法中将数据点分为以下三类： 核心点：在半径Eps内含有超过MinPts数目的点 边界点：在半径Eps内点的数量小于MinPts，但是落在核心点的邻域内 噪音点：既不是核心点也不是边界点的点（离群点）12#导入DBSCANfrom sklearn.cluster import DBSCAN 1DBSCAN? 12#模型的参数设置：一个是半径eps，这里设置为1.5，另一个是指定的数目MinPts，这里设置为4：model = DBSCAN(eps=1.5,min_samples=4).fit(auto_scaled) 根据我们设置的参数，DBSCAN将所有样本分为四类（包含噪声点），并按照每一类的数量降序展示:123auto_label = model.labels_df=pd.value_counts(auto_label)df 123import matplotlib.pyplot as pltplt.figure(figsize=(5,5))df.plot.bar(rot=0) 将不同簇的数据使用均值聚合，得到近似类中心12345#先将数据逆标准化，转为原始数据，并取出前五列，再根据聚类标签分类使用均值聚合，结果保留两位小数auto_new_continuous = pd.DataFrame(scaler.inverse_transform(auto_scaled.iloc[:, :5]), columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;])auto_new_continuous[&#x27;label&#x27;] = auto_labelauto_new_continuous.groupby(&#x27;label&#x27;).mean().applymap(lambda x:&#x27;%.2f&#x27;%x) 去除噪声点样本后，计算轮廓系数12labels = model.labels_metrics.silhouette_score(auto_scaled[labels&gt;-1],labels[labels&gt;-1]) 调整密度参数，查看聚类结果1234567891011121314# 不同最小样本数下的簇个数## 设置参数取值范围min_samples_grid = [1,2,3,4]## 训练模型并输出簇个数cluster_number = []slt_score = []noise_count = []for item in min_samples_grid: model = DBSCAN(eps=1.5,min_samples=item).fit(auto_scaled) cluster_number.append(len(np.unique(model.labels_))-1) slt_score.append(metrics.silhouette_score(auto_scaled[model.labels_&gt;-1], model.labels_[model.labels_&gt;-1])) noise_count.append((model.labels_==-1).sum()) 1np.unique? 123456## 绘图import matplotlib.pyplot as pltplt.plot(min_samples_grid, cluster_number, &#x27;r-*&#x27;, linewidth=2)plt.xlabel(&#x27;最小样本数&#x27;)plt.ylabel(&#x27;簇个数&#x27;)plt.title(&#x27;不同最小样本数下聚类的簇个数&#x27;) 1234plt.plot(min_samples_grid, slt_score, &#x27;b-*&#x27;, linewidth=2)plt.xlabel(&#x27;最小样本数&#x27;)plt.ylabel(&#x27;轮廓系数&#x27;)plt.title(&#x27;不同最小样本数下聚类的轮廓系数&#x27;) 1234plt.plot(min_samples_grid, noise_count, &#x27;g-*&#x27;, linewidth=2)plt.xlabel(&#x27;最小样本数&#x27;)plt.ylabel(&#x27;噪声点数量&#x27;)plt.title(&#x27;不同最小样本数下聚类的噪声点数量&#x27;) 5 层次聚类层次聚类有两种常用的形式，自顶向下和自底向上,自底向上的主要做法是，在开始时，将每个样本视为一个簇，重复的合并最近的两个簇，直到簇的个数达到给定值。 12345from sklearn.cluster import AgglomerativeClustering# 训练模型model = AgglomerativeClustering(n_clusters=3,linkage=&#x27;average&#x27;).fit(auto_scaled)# 输出模型结果 auto_label = model.labels_ 1AgglomerativeClustering? 123auto_label = model.labels_df=pd.value_counts(auto_label)df 12plt.figure(figsize=(5,5))df.plot.bar() 12345#先将数据逆标准化，转为原始数据，并取出前五列，再根据聚类标签分类使用均值聚合，结果保留两位小数auto_new_continuous = pd.DataFrame(scaler.inverse_transform(auto_scaled.iloc[:, :5]), columns=[&#x27;mpg&#x27;,&#x27;displacement&#x27;,&#x27;horsepower&#x27;,&#x27;weight&#x27;,&#x27;acceleration&#x27;])auto_new_continuous[&#x27;label&#x27;] = auto_labelauto_new_continuous.groupby(&#x27;label&#x27;).mean().applymap(lambda x:&#x27;%.2f&#x27;%x)"},{"title":"图书数据可视化","path":"/2023/08/30/图书数据可视化/","content":"1 数据集介绍使用上次清洗过的数据进行数据分析，并利用可视化图形对分析。数据集共9个字段，600个样本，如下： 价格 星级 评论数 作者 出版日期 出版社 书名 简介1 简介2 2 数据读取2.1 导入相关的库12345678#import pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltimport jiebafrom wordcloud import WordCloud,STOPWORDS# 设置中文字体plt.rcParams[&#x27;font.sans-serif&#x27;]=&#x27;SimHei&#x27;%config InlineBackend.figure_format = &#x27;svg&#x27; 2.2 数据读取调用Pandas对象的read_csv()函数。 1234#读数据集data = pd.read_csv(&#x27;./dataset/当当网机器学习图书数据（已清洗）.csv&#x27;,sep=&#x27;,&#x27;,encoding=&#x27;utf-8&#x27;)#查看前5行data.head(5) 2.3 字段基本统计信息使用DataFrame对象的describe()方法可以查看各列基本统计信息，统计并生成数据集哥哥字段的样本数、均值、标准差、最小值、四分位数等。describe()方法主要参数： percentiles：自定义分位数，默认是25%,50%,75% include：指定统计的数据类型，默认只统计数值型，当为all时数值和离散型都统计 exclude：意排除哪些字段，默认统计所有列 12#查看数据基本统计信息 data.describe(include=&#x27;all&#x27;) 3 可视化使用python中的Matplotilb、Seaborn等库进行可视化。 3.1 各出版社的图书数量统计各出版社关于机器学习相关的图书数量，对出版数量有直观了解。Series对象的value_counts()方法可以对某列的取值数量分布进行统计，其主要参数： normallize：默认为False，若为True，则以百分比的形式显示 sort：是否对结果进行排序，默认为True ascending: 默认对值降序排列(False) dropna:是否删除空值，默认删除(True) 12# 各出版社出版的图书数量data[&#x27;出版社&#x27;].value_counts(ascending=True) 从统计结果可以看到出版机器学习相关图书最多的机械工业出版社。 3.2 各星级图书数量统计柱状图使用Seaborn中的barplot函数绘制柱状图，其主要参数有： x：x坐标传入的值 y：y坐标传入的值 data：传入的数据集 12# 查看各星级的数量data[&#x27;星级&#x27;].value_counts() 123456789101112# # 绘制画布，即画布的大小和分辨率plt.figure(figsize=(8,5), dpi=100)x = data[&#x27;星级&#x27;].value_counts().indexy = data[&#x27;星级&#x27;].value_counts().valuessns.barplot(x,y)# 设置标题plt.title(&#x27;各星级图书数量统计柱状图&#x27;,fontsize=13)# 设置x轴标签plt.xlabel(&#x27;星级&#x27;,fontsize=10)# 设置y轴标签plt.ylabel(&#x27;数量&#x27;,fontsize=10)plt.show() 3.3 图书价格直方图使用Seaborn中的histplot函数来绘制直方图，主要参数有： data：传入的数据 x：做直方图所用的数据，必须是一维数组 bins：分组数量 12345678910111213# 绘制画布，即大小和分辨率plt.figure(figsize=(8,5),dpi=100)# 绘图，分别统计不同图书价格的数量sns.histplot(data,x=&#x27;价格&#x27;,bins=20)# 绘制价格均值直线plt.plot([data[&#x27;价格&#x27;].mean(),data[&#x27;价格&#x27;].mean()],[0,200],&#x27;g--&#x27;)# 设置标题plt.title(&#x27;图书价格直方图&#x27;,fontsize=13)# 设置x轴标签plt.xlabel(&#x27;图书价格&#x27;,fontsize=10)# 设置y轴标签plt.ylabel(&#x27;数量&#x27;,fontsize=10)plt.show() 3.4 高价图书分析设置价格大于100元为高价图书，筛选高价图书如下： 1234# 利用DataFrame直接筛选输出高价图书data_price_high = data[data[&#x27;价格&#x27;]&gt;100]# 查看前3行高价图书信息data_price_high.head(3) 3.4.1高价书出版社统计使用Matplotlib中的pie函数绘制饼状图，主要参数如下： x：每一块的数值比例 labels：每一块外侧显示的说明文字 autopct：控制图内显示的百分比 startangle：起始角度，默认图是从x轴正方向画起，逆时针方向 pctdistance：数值标签距离圆心的距离 radius：控制饼图半径 1data_price_high[&#x27;出版社&#x27;].value_counts().index 123456789## 按照出版社进行分段统计x = data_price_high[&#x27;出版社&#x27;].value_counts().values# 设置饼图的标签labels = data_price_high[&#x27;出版社&#x27;].value_counts().index# 绘制画布plt.figure(figsize=(8,5),dpi=100)# 绘制饼状图plt.pie(x,labels=labels,autopct=&#x27;%1.1f%%&#x27;,startangle=30,pctdistance=0.8,radius=1)plt.show() 3.4.2 高价书星级评定使用Matplotlib中的bach()函数绘制柱状图，主要参数如下： y：y坐标 width：柱子的宽度，即统计的数值大小 height：柱子的高度，默认为0.8 1data_price_high.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean() 1234567891011# 取值各个出版社星级的平均值width = data_price_high.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean().values# 按照出版社求各个出版社星级评分的平均值，取出索引，即出版社y = data_price_high.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean().index# 设置画布plt.figure(figsize=(8,5),dpi=100)# 绘图plt.barh(y,width,height=0.8,color=&#x27;orange&#x27;)plt.title(&#x27;高价图书星级评定&#x27;,fontsize=13)plt.show() 3.5 各出版社图书价格均值对各出版社出版图书价格均值进行统计。 12# 按照出版社进行分组聚合，计算每组平均的图书价格，计算结果如下data.groupby(&#x27;出版社&#x27;)[&#x27;价格&#x27;].mean().sort_values(ascending=False) 3.6 各出版社出版图书口碑分析将出版社一列分组，计算各出版社图书平均星级。 1234# 按照出版社分段统计，求各个出版社星级的均值data_star_mean = data.groupby(&#x27;出版社&#x27;)[&#x27;星级&#x27;].mean()# 将每个出版社的平均星级进行降序排序 默认是升序排序data_star_mean.sort_values(ascending = False) 3.7 图书简介文本分词对图书的简介进行分析，首先数据中简介1这一列是文本类型，因此我们要先进行分词，分词的目的是将文本按一定的规则进行分词处理。在这里我们使用jieba库里面的cut函数进行分词，jieba库是专门使用Python语言开发的分词库,占用资源较少，常识类文档的分词精度较高。cut函数的主要参数如下： sentence:要进行的分词的句子样本 cut_all:分词的模式，有全模式和精准模式，默认false，精准模式 HMM:隐马尔科夫链，即HMM模型，默认开启，这个是在分词的理论模型中用到的 1234567# 对数据集的每个样本的文本进行中文分词#记录分词后的结果cutted = [] for item in data[&#x27;简介1&#x27;].values: raw_words = (&quot; &quot;.join(jieba.cut(str(item)))) cutted.append(raw_words) 1234567# 创建一个新的DataFrame，将没分词和分词后的句子添加到里面data_cutted = pd.DataFrame(&#123; &#x27;简介1&#x27;: data[&#x27;简介1&#x27;], &#x27;简介1_cut&#x27;: cutted&#125;)data_cutted.head() 3.8 词云图分词处理完毕后，再处理停用词，最后形成词云图。利用wordcloud中的WordColoud()函数绘制词云图，其中主要参数为： font_path:字体路径 stopword:将被忽略或者是删除的单词表 width:词云图的宽度，默认400 height：词云图的高度，默认200 max_font_size:最大字体的大小 12345# 读取停用词stopwd=pd.read_csv(&#x27;./dataset/中文停用词表数据集.csv&#x27;)stopwords=set([i for i in stopwd[&#x27;cn_stopwords&#x27;]])print(len(stopwords))# stopwords 1234567# 定义词云图wc = WordCloud(font_path = &quot;./dataset/simsun.ttc&quot;,#设置字体 stopwords = stopwords, #设置停用词 background_color = &#x27;white&#x27;, width = 1000, height = 618, max_font_size = 400) 12345678# 运行统计词频wc.generate(data_cutted[&#x27;简介1_cut&#x27;].sum())# 4、显示图片plt.figure(&quot;词云图&quot;) #指定所绘图名称plt.imshow(wc) # 以图片的形式显示词云plt.axis(&quot;off&quot;) #关闭图像坐标系plt.show() 1# data_cutted[&#x27;简介1_cut&#x27;].sum() 词云图突出显示了简介中出现频率较高的词，出现词频越高的词在词云图中显示越大。"},{"title":"网站数据清洗","path":"/2023/08/30/网站数据清洗/","content":"1 数据集介绍来自某图书网站爬取的机器学习相关图书信息，数据集共600条数据，5个字段，如下 书名 出版信息 当前价格 星级 评论数 2 数据读取2.1 读取数据数据集保存在csv文件中，使用Pandas中的read_csv()读取csv文件，结果保存为DataFrame或Series对象，使用DataFrame或Series对象的head()方法可以查看前n行数据。 1234567#导入相关的库import pandas as pdimport numpy as np#读取数据data = pd.read_csv(&#x27;./dataset/data.csv&#x27;)#查看数据data.head() 2.2 查看数据集的基本信息调用DataFrame对象的info()方法，获取数据的列名，非空值个数，列数据类型，内存占用信息。 1data.info() 数据集索引为0～599，共600条数据。各字段数据类型均为字符型。 3 数据清洗3.1 提取价格数值由于当前价格一列中含有¥符号，想对图书价格进行统计分析，需要从当前价格中取出价格的数值。借助正则表达书来完成上述操作。re库是python中正则表达式的支持库，使用findall()函数将当前价格中的数值提取出来，保存为新一列当前价格_match，findall()函数返回字符串中所有与正则表达式匹配的全部字符串，返回形式为数组。 +将前面的模式匹配一次或多次 ？匹配前一个字符零次或一次 *将前面的模式匹配零次或多次 .匹配除换行符之外的任意字符 \\转译字符 \\d匹配数字0～9 \\d{n}匹配正好n位数的数字 \\d{n,}匹配至少为n位的数字 \\d{n,m}匹配m～n位数的数字 [A-Za-z]+匹配英文字母组成的字符串 [A-Za-z0-9]+匹配由数字和英文字母组成的字符串DataFrame对象中apply方法可以将某个函数应用到由列或行形成的Series对象上，定义一个函数num_func，用于提取价格数值，然后使用apply方法将num_func应用到当前价格一列上。 1data[&#x27;当前价格&#x27;] 12345678import re#使用正则表达式将当前价格一列只读取数字部分并创建新的一列def func(data): result = re.findall(r&#x27;\\d+\\.?\\d*&#x27;,data) return float(result[0])data[&#x27;当前价格_match&#x27;] = data[&#x27;当前价格&#x27;].apply(func)data.head(3) 3.2 提取评论数值由于评论数一列中不仅包含数值，对评论数进行统计分析，需要从评论数一列中提取出评论的数值，保存为新的一列评论数_match。 1data[&#x27;评论数&#x27;] 12345678import re# 定义读取评论数的函数def func_1(data): esult = re.findall(r&#x27;\\d+&#x27;,data) return int(result[0])# 利用apply方法，将每一条数据进行处理data[&#x27;评论数_match&#x27;] = data[&#x27;评论数&#x27;].apply(func_1) 12# 查看是否处理成功data.head(3) 3.3 转换图书星级数值星级一列中同样包含一些其他字符，从星级一列中提取出星级的数值，将星级数值转换到[0,5]区间内，保存新的一列为星级_match。 123#提取星级数data[&#x27;星级_match&#x27;] = data[&#x27;星级&#x27;].apply(func_1)data.head(3) 12#将星级除以20，取值范围转换到[0,5]的区间内data[&#x27;星级_match_cal&#x27;] = data[&#x27;星级_match&#x27;].apply(lambda x:x/20) 1data.head(3) 3.4 提取作者、出版时间和出版社出版信息中包含作者、出版日期、出版社，用/分割为三部分，分三列存放，字符串对象的split方法通过制定分隔符对字符串进行切片。 1data[&#x27;出版信息&#x27;][:5] 1234# 将出版信息分割成三列，分别提取出作者、出版日期和出版社# 提取出作者data[&#x27;作者&#x27;] = data[&#x27;出版信息&#x27;].apply(lambda x:x.split(&#x27;/&#x27;)[0])data.head(3) 提取出的出版时间为字符串对象，我们可以通过datetime库中的strptime函数将字符串转换为datetime时间类型对象。 12345678910111213# 用正则表达式提取日期，并将日期字符串转换成日期格式from datetime import datetimedef func_2(data): result = re.findall(r&#x27;\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;&#x27;,data) if len(result)&lt;1: return None else: return datetime.strptime(result[0],&#x27;%Y-%m-%d&#x27;) #返回日期类型# 提取日期，并添加为新的一列data[&#x27;出版日期&#x27;] = data[&#x27;出版信息&#x27;].apply(func_2)data.head(3) 12345# 提取出版社一列，并添加为新的一列data[&#x27;出版社&#x27;] = data[&#x27;出版信息&#x27;].apply(lambda x:x.split(&#x27;\\&#x27;)[-1])# 查看结果data.head(3) 3.5 提取书名和简介书名字段包含书名和书籍的相关介绍。以空格为分隔符对字段进行分割，提取图书的书名和简介部分。 1data[&#x27;书名&#x27;][:5] 123456#将&#x27;【】&#x27;和&#x27;[]&#x27;以及之间的内容，用空格来代替def func_3(data): data = data.strip()#先去除头和尾的空格 data = re.sub(&quot;【.*?】&quot;,&quot; &quot;,data) data = re.sub(&quot;\\[.*?\\]&quot;,&quot; &quot;,data) return data.split(&quot; &quot;) 12data[&#x27;书名_split&#x27;] = data[&#x27;书名&#x27;].apply(func_3)data[&#x27;书名_split&#x27;][:5] 123# 提取书名data[&#x27;书名_split_1&#x27;] = data[&#x27;书名_split&#x27;].apply(lambda x:x[0])data[&#x27;书名_split_1&#x27;][:5] 1234#因为分割后的字段长度不唯一，所以从第三个开始我们要先判定每个字段分割后的长度，之后再进行提取# 提取简介1data[&#x27;书名_split_2&#x27;] = data[&#x27;书名_split&#x27;].apply(lambda x: None if len(x)&lt;=1 else x[1])data[&#x27;书名_split_2&#x27;][:5] 123# 提取简介2data[&#x27;书名_split_3&#x27;] = data[&#x27;书名_split&#x27;].apply(lambda x: None if len(x)&lt;=2 else x[2])data[&#x27;书名_split_3&#x27;][:5] 1data.head(3) 3.6 删除不需要的列使用DataFrame对象的drop方法删除不需要的列。 123# 删除不需要的列data.drop([&#x27;书名&#x27;,&#x27;出版信息&#x27;,&#x27;当前价格&#x27;,&#x27;星级&#x27;,&#x27;评论数&#x27;,&#x27;星级_match&#x27;,&#x27;书名_split&#x27;],axis=1,implace=True)data.head(3) 3.7 修改列名使用DataFrame对象的rname对列进行重命名。 123# 修改列名data.rename(columns=&#123;&#x27;当前价格_match&#x27;:&#x27;当前价格&#x27;,&#x27;评论数_match&#x27;:&#x27;评论数&#x27;,&#x27;星级_match_cal&#x27;:&#x27;星级&#x27;,&#x27;书名_split_1&#x27;:&#x27;书名&#x27;,&#x27;书名_split_2&#x27;:&#x27;简介1&#x27;,&#x27;书名_split_3&#x27;:&#x27;简介2&#x27;&#125;,implace=True)data.head(3) 4 保存数据使用DataFrame对象的to_csv方法将处理好的数据保存为CSV文件。 1data.to_csv(&#x27;当当网机器学习图书数据(已清洗).csv&#x27;, sep=&#x27;,&#x27;,encoding=&#x27;utf8&#x27;,index=False)"},{"title":"Hello World","path":"/2023/08/06/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"}]